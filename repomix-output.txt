This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix. The content has been processed where comments have been removed, empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: .github/, .circleci/*, README.*, ui, **/*test.go, Dockerfile, docker*, LICENSE
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.migrations/
  dev.conf
  V20200123054713__initial_table_create.sql
  V20200123054714__add_spark_extension.sql
  V20200205133700__executable.sql
  V20200206115000__template.sql
  V20200210154600__template_refactor.sql
  V20200211160100__task_col_fix.sql
  V20200211161900__template_indicies.sql
  V20200212101900__template.sql
  V20200213101400__task_indexes.sql
  V20200213125200__rename_default_payload.sql
  V20200225125200__add_limits.sql
  V20200325125200__add_attempts.sql
  V20200325125201__add_spawned.sql
  V20200625125201__add_run_exceptions.sql
  V20210083054714__metrics_uri.sql
  V20210427125201__add_active_deadline_seconds.sql
  V20210807125201__drop_index_container_name.sql
  V20211007125201__add_description.sql
  V20220907125201__add_idempotence.sql
  V20220907125202__add_arch.sql
  V20221215125203__add_labels.sql
  V20230718115000__add_ephemeral_storage.sql
  V20231013191711__add_requires_docker.sql
  V20231122141100__add_target_cluster.sql
  V20240205132100__add_service_account.sql
  V20250122141100__add_cluster_routing.sql
clients/
  cluster/
    cluster.go
    eks_cluster_client.go
  httpclient/
    client.go
  logs/
    eks_cloudwatch_logs_client.go
    eks_s3_logs_client.go
    logs.go
  metrics/
    datadog_metrics_client.go
    metrics.go
  middleware/
    client.go
conf/
  config.yml
config/
  config.go
docs/
  ara.md
exceptions/
  errors.go
execution/
  adapter/
    eks_adapter.go
  engine/
    eks_engine.go
    emr_engine.go
    engine.go
  utils.go
flotilla/
  app.go
  endpoints.go
  router.go
log/
  event.go
  logger.go
queue/
  manager.go
  sqs_manager.go
services/
  definition.go
  execution.go
  logs.go
  template.go
  worker.go
state/
  manager.go
  models.go
  pg_queries.go
  pg_state_manager.go
testutils/
  mocks.go
utils/
  utils.go
worker/
  cloudtrail_worker.go
  events_worker.go
  retry_worker.go
  status_worker.go
  submit_worker.go
  worker_manager.go
  worker.go
.gitignore
go.mod
main.go

================================================================
Files
================================================================

================
File: .migrations/dev.conf
================
flyway.url=jdbc:postgresql://127.0.0.1:5432/flotilla
flyway.user=flotilla
flyway.password=flotilla
flyway.cleanDisabled=true
flyway.group=true

================
File: .migrations/V20200123054713__initial_table_create.sql
================
CREATE TABLE IF NOT EXISTS task_def (
  definition_id character varying PRIMARY KEY,
  alias character varying,
  image character varying NOT NULL,
  group_name character varying NOT NULL,
  memory integer,
  cpu integer,
  gpu integer,
  command text,
  env jsonb,
  "user" character varying,
  arn character varying,
  container_name character varying NOT NULL,
  task_type character varying,
  privileged boolean,
  adaptive_resource_allocation boolean,
  CONSTRAINT task_def_alias UNIQUE(alias)
);
CREATE TABLE IF NOT EXISTS task_def_ports (
  task_def_id character varying NOT NULL REFERENCES task_def(definition_id),
  port integer NOT NULL,
  CONSTRAINT task_def_ports_pkey PRIMARY KEY(task_def_id, port)
);
CREATE INDEX IF NOT EXISTS ix_task_def_alias ON task_def(alias);
CREATE INDEX IF NOT EXISTS ix_task_def_group_name ON task_def(group_name);
CREATE INDEX IF NOT EXISTS ix_task_def_image ON task_def(image);
CREATE INDEX IF NOT EXISTS ix_task_def_env ON task_def USING gin (env jsonb_path_ops);
CREATE TABLE IF NOT EXISTS task (
  run_id character varying NOT NULL PRIMARY KEY,
  definition_id character varying REFERENCES task_def(definition_id),
  alias character varying,
  image character varying,
  cluster_name character varying,
  exit_code integer,
  exit_reason character varying,
  status character varying,
  queued_at timestamp with time zone,
  started_at timestamp with time zone,
  finished_at timestamp with time zone,
  instance_id character varying,
  instance_dns_name character varying,
  group_name character varying,
  env jsonb,
  task_arn character varying,
  docker_id character varying,
  "user" character varying,
  task_type character varying,
  command text,
  command_hash text,
  memory integer,
  cpu integer,
  gpu integer,
  ephemeral_storage integer,
  node_lifecycle text,
  engine character varying DEFAULT 'eks' NOT NULL,
  container_name text,
  pod_name text,
  namespace text,
  max_cpu_used integer,
  max_memory_used integer,
  pod_events jsonb,
  cloudtrail_notifications jsonb
);
CREATE INDEX IF NOT EXISTS ix_task_definition_id ON task(definition_id);
CREATE INDEX IF NOT EXISTS ix_task_cluster_name ON task(cluster_name);
CREATE INDEX IF NOT EXISTS ix_task_status ON task(status);
CREATE INDEX IF NOT EXISTS ix_task_group_name ON task(group_name);
CREATE INDEX IF NOT EXISTS ix_task_env ON task USING gin (env jsonb_path_ops);
CREATE INDEX IF NOT EXISTS ix_task_definition_id ON task(definition_id);
CREATE INDEX IF NOT EXISTS ix_task_task_arn ON task(task_arn);
CREATE INDEX IF NOT EXISTS ix_task_definition_id_started_at_desc ON task(definition_id, started_at DESC NULLS LAST);
CREATE INDEX IF NOT EXISTS ix_task_definition_id_started_at_desc_engine ON task(definition_id, started_at DESC NULLS LAST, engine);
CREATE TABLE IF NOT EXISTS task_status (
  status_id integer NOT NULL PRIMARY KEY,
  task_arn character varying,
  status_version integer NOT NULL,
  status character varying,
  "timestamp" timestamp with time zone DEFAULT now()
);
CREATE INDEX IF NOT EXISTS ix_task_status_task_arn ON task_status(task_arn);
CREATE SEQUENCE IF NOT EXISTS task_status_status_id_seq
  START WITH 1
  INCREMENT BY 1
  NO MINVALUE
  NO MAXVALUE
  CACHE 1;
ALTER TABLE ONLY task_status ALTER COLUMN status_id SET DEFAULT nextval('task_status_status_id_seq'::regclass);
CREATE TABLE IF NOT EXISTS tags (
  text character varying NOT NULL PRIMARY KEY
);
CREATE TABLE IF NOT EXISTS task_def_tags (
  tag_id character varying NOT NULL REFERENCES tags(text),
  task_def_id character varying NOT NULL REFERENCES task_def(definition_id)
);
CREATE TABLE IF NOT EXISTS worker (
  worker_type character varying,
  engine character varying,
  count_per_instance integer
);

================
File: .migrations/V20200123054714__add_spark_extension.sql
================
ALTER TABLE task ADD COLUMN IF NOT EXISTS spark_extension JSONB;

================
File: .migrations/V20200205133700__executable.sql
================
ALTER TABLE task
  ADD COLUMN executable_id VARCHAR,
  ADD COLUMN executable_type VARCHAR DEFAULT 'task_definition';

================
File: .migrations/V20200206115000__template.sql
================
CREATE TABLE template (
  template_id VARCHAR PRIMARY KEY,
  type VARCHAR NOT NULL,
  version INTEGER NOT NULL,
  schema JSONB NOT NULL,
  command_template TEXT NOT NULL,
  image VARCHAR NOT NULL,
  memory INTEGER NOT NULL,
  gpu INTEGER NOT NULL,
  cpu INTEGER NOT NULL,
  env JSONB,
  privileged BOOLEAN,
  adaptive_resource_allocation BOOLEAN,
  container_name VARCHAR NOT NULL,
  CONSTRAINT template_type_version UNIQUE(type, version)
);
ALTER TABLE task ADD COLUMN IF NOT EXISTS executable_request_custom JSONB;

================
File: .migrations/V20200210154600__template_refactor.sql
================
ALTER TABLE template DROP CONSTRAINT template_type_version;
ALTER TABLE template RENAME COLUMN type to template_name;
ALTER TABLE template ADD CONSTRAINT template_name_version UNIQUE(template_name, version);

================
File: .migrations/V20200211160100__task_col_fix.sql
================
ALTER TABLE task RENAME COLUMN executable_request_custom to execution_request_custom;

================
File: .migrations/V20200211161900__template_indicies.sql
================
CREATE INDEX IF NOT EXISTS ix_template_id ON template(template_id);
CREATE INDEX IF NOT EXISTS ix_template_name ON template(template_name);

================
File: .migrations/V20200212101900__template.sql
================
ALTER TABLE template ADD COLUMN default_payload JSONB;
ALTER TABLE template ADD COLUMN avatar_uri VARCHAR;

================
File: .migrations/V20200213101400__task_indexes.sql
================
CREATE INDEX IF NOT EXISTS ix_task_executable_id ON task(executable_id);
CREATE INDEX IF NOT EXISTS ix_task_executable_id_started_at_desc ON task(executable_id, started_at DESC NULLS LAST);
CREATE INDEX IF NOT EXISTS ix_task_executable_id_started_at_desc_engine ON task(executable_id, started_at DESC NULLS LAST, engine);

================
File: .migrations/V20200213125200__rename_default_payload.sql
================
ALTER TABLE template RENAME COLUMN default_payload to defaults;

================
File: .migrations/V20200225125200__add_limits.sql
================
ALTER TABLE task ADD COLUMN memory_limit integer;
ALTER TABLE task ADD COLUMN cpu_limit integer;

================
File: .migrations/V20200325125200__add_attempts.sql
================
ALTER TABLE task ADD COLUMN attempt_count integer;

================
File: .migrations/V20200325125201__add_spawned.sql
================
ALTER TABLE task ADD COLUMN spawned_runs jsonb;

================
File: .migrations/V20200625125201__add_run_exceptions.sql
================
ALTER TABLE task ADD COLUMN run_exceptions jsonb;

================
File: .migrations/V20210083054714__metrics_uri.sql
================
ALTER TABLE task ADD COLUMN IF NOT EXISTS metrics_uri varchar;

================
File: .migrations/V20210427125201__add_active_deadline_seconds.sql
================
ALTER TABLE task ADD COLUMN active_deadline_seconds integer;

================
File: .migrations/V20210807125201__drop_index_container_name.sql
================
alter table task_def alter column container_name drop not null;

================
File: .migrations/V20211007125201__add_description.sql
================
ALTER TABLE task ADD COLUMN IF NOT EXISTS description varchar;

================
File: .migrations/V20220907125201__add_idempotence.sql
================
ALTER TABLE task ADD COLUMN IF NOT EXISTS idempotence_key varchar;

================
File: .migrations/V20220907125202__add_arch.sql
================
ALTER TABLE task ADD COLUMN IF NOT EXISTS arch varchar;

================
File: .migrations/V20221215125203__add_labels.sql
================
ALTER TABLE task ADD COLUMN IF NOT EXISTS labels jsonb;

================
File: .migrations/V20230718115000__add_ephemeral_storage.sql
================
ALTER TABLE task_def ADD COLUMN IF NOT EXISTS ephemeral_storage INTEGER;
ALTER TABLE task ADD COLUMN IF NOT EXISTS ephemeral_storage INTEGER;

================
File: .migrations/V20231013191711__add_requires_docker.sql
================
ALTER TABLE task_def ADD COLUMN IF NOT EXISTS requires_docker BOOLEAN DEFAULT(false);
ALTER TABLE task ADD COLUMN IF NOT EXISTS requires_docker BOOLEAN DEFAULT(false);

================
File: .migrations/V20231122141100__add_target_cluster.sql
================
ALTER TABLE task_def ADD COLUMN IF NOT EXISTS target_cluster VARCHAR;

================
File: .migrations/V20240205132100__add_service_account.sql
================
ALTER TABLE task ADD COLUMN IF NOT EXISTS service_account VARCHAR;

================
File: .migrations/V20250122141100__add_cluster_routing.sql
================
DO $$
BEGIN
    IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'tier') THEN
CREATE TYPE tier AS ENUM ('Tier1', 'Tier2', 'Tier3', 'Tier4');
END IF;
END$$;
DO $$
BEGIN
    IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'cluster_status') THEN
CREATE TYPE cluster_status AS ENUM ('active', 'maintenance', 'offline');
END IF;
END$$;
CREATE TABLE IF NOT EXISTS cluster_state (
    name VARCHAR PRIMARY KEY,
    status cluster_status NOT NULL DEFAULT 'active',
    status_reason VARCHAR,
    status_since TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    capabilities VARCHAR[] NOT NULL DEFAULT '{}',
    allowed_tiers tier[] NOT NULL DEFAULT '{}',
    region VARCHAR NOT NULL,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    namespace VARCHAR NOT NULL DEFAULT '',
    emr_virtual_cluster VARCHAR NOT NULL DEFAULT ''
    );
CREATE INDEX IF NOT EXISTS ix_cluster_state_status ON cluster_state(status);
DO $$
BEGIN
    IF NOT EXISTS (SELECT 1
        FROM information_schema.columns
        WHERE table_name='task' AND column_name='tier')
    THEN
        ALTER TABLE task ADD COLUMN tier tier DEFAULT 'Tier4';
    END IF;
END$$;

================
File: clients/cluster/cluster.go
================
package cluster
import (
	"fmt"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/state"
)
type Client interface {
	Name() string
	Initialize(conf config.Config) error
	CanBeRun(clusterName string, executableResources state.ExecutableResources) (bool, error)
	ListClusters() ([]state.ClusterMetadata, error)
}
func NewClusterClient(conf config.Config, name string) (Client, error) {
	switch name {
	case "eks":
		eksc := &EKSClusterClient{}
		if err := eksc.Initialize(conf); err != nil {
			return nil, errors.Wrap(err, "problem initializing EKSClusterClient")
		}
		return eksc, nil
	default:
		return nil, fmt.Errorf("No Client named [%s] was found", name)
	}
}

================
File: clients/cluster/eks_cluster_client.go
================
package cluster
import (
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/state"
)
type EKSClusterClient struct{}
func (EKSClusterClient) Name() string {
	return ""
}
func (EKSClusterClient) Initialize(conf config.Config) error {
	return nil
}
func (EKSClusterClient) CanBeRun(clusterName string, executableResources state.ExecutableResources) (bool, error) {
	return true, nil
}
func (EKSClusterClient) ListClusters() ([]state.ClusterMetadata, error) {
	return []state.ClusterMetadata{}, nil
}

================
File: clients/httpclient/client.go
================
package httpclient
import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"strings"
	"time"
)
type RetryableError interface {
	Err() string
}
type HttpRetryableError struct {
	e error
}
func (re HttpRetryableError) Error() string {
	return re.e.Error()
}
func (re HttpRetryableError) Err() string {
	return re.e.Error()
}
type RequestExecutor interface {
	Do(req *http.Request, timeout time.Duration, entity interface{}) error
}
type defaultExecutor struct{}
func (de *defaultExecutor) Do(req *http.Request, timeout time.Duration, entity interface{}) error {
	client := http.Client{Timeout: timeout}
	if client.Timeout == 0 {
		client.Timeout = time.Second * 10
	}
	r, err := client.Do(req)
	if r != nil {
		defer r.Body.Close()
	}
	if err != nil {
		return err
	}
	if r.StatusCode >= 200 && r.StatusCode < 400 {
		return json.NewDecoder(r.Body).Decode(entity)
	} else if r.StatusCode >= 500 {
		return HttpRetryableError{fmt.Errorf("Error response: %v", r.Status)}
	} else {
		return fmt.Errorf("Error response: %v", r.Status)
	}
}
type Client struct {
	Host       string
	Timeout    time.Duration
	RetryCount int
	Executor   RequestExecutor
}
func (c *Client) Get(path string, headers map[string]string, entity interface{}) error {
	req, err := c.prepareRequestNoBody("GET", path, headers)
	if err != nil {
		return fmt.Errorf("httpclient GET: %v", err)
	}
	return c.doRequestWithRetry(req, entity)
}
func (c *Client) Delete(path string, headers map[string]string, entity interface{}) error {
	req, err := c.prepareRequestNoBody("DELETE", path, headers)
	if err != nil {
		return fmt.Errorf("httpclient DELETE: %v", err)
	}
	return c.doRequestWithRetry(req, entity)
}
func (c *Client) Put(path string, headers map[string]string, inEntity interface{}, outEntity interface{}) error {
	req, err := c.prepareRequestWithBody("PUT", path, headers, inEntity)
	if err != nil {
		return fmt.Errorf("httpclient PUT: %v", err)
	}
	return c.doRequestWithRetry(req, outEntity)
}
func (c *Client) Post(path string, headers map[string]string, inEntity interface{}, outEntity interface{}) error {
	req, err := c.prepareRequestWithBody("POST", path, headers, inEntity)
	if err != nil {
		return fmt.Errorf("httpclient POST: %v", err)
	}
	return c.doRequestWithRetry(req, outEntity)
}
func (c *Client) prepareRequestNoBody(method string, path string, headers map[string]string) (*http.Request, error) {
	return c.makeRequest(method, path, headers, nil)
}
func (c *Client) prepareRequestWithBody(method string, path string, headers map[string]string, entity interface{}) (*http.Request, error) {
	encoded, err := json.Marshal(entity)
	if err != nil {
		return nil, fmt.Errorf("httpclient get: %v", err)
	}
	return c.makeRequest(method, path, headers, bytes.NewBuffer(encoded))
}
func (c *Client) makeURL(path string) (string, error) {
	host := c.Host
	if !strings.HasPrefix(c.Host, "http") {
		host = strings.Join([]string{"http://", c.Host}, "")
	}
	u, err := url.Parse(host)
	if err != nil {
		return "", fmt.Errorf("Unable to parse hostname (%v): %v", c.Host, err)
	}
	parsedPath, err := url.Parse(path)
	if err != nil {
		return "", fmt.Errorf("Unable to parse path (%v): %v", path, err)
	}
	u.Path = parsedPath.Path
	u.RawQuery = parsedPath.RawQuery
	return u.String(), nil
}
func (c *Client) makeRequest(method, path string, headers map[string]string, body io.Reader) (*http.Request, error) {
	u, err := c.makeURL(path)
	req, err := http.NewRequest(method, u, body)
	if headers != nil {
		for k, v := range headers {
			req.Header.Set(k, v)
		}
	}
	if err != nil {
		return nil, fmt.Errorf("could not create request: %v", err)
	}
	return req, nil
}
func (c *Client) doRequestWithRetry(req *http.Request, entity interface{}) error {
	if c.Executor == nil {
		c.Executor = &defaultExecutor{}
	}
	err := c.retryRequest(3*time.Second, func() error {
		return c.Executor.Do(req, c.Timeout, entity)
	})
	return err
}
type httpreqfunc func() error
func (c *Client) retryRequest(sleepTime time.Duration, fn httpreqfunc) error {
	err := fn()
	if err != nil {
		_, isRetryable := err.(RetryableError)
		if !isRetryable {
			return err
		}
		toSleep := sleepTime
		for retries := 0; retries < c.RetryCount; retries++ {
			time.Sleep(toSleep)
			toSleep = toSleep * 2
			err := fn()
			_, isRetryable := err.(RetryableError)
			if err == nil {
				return nil
			} else if !isRetryable {
				return err
			}
		}
	}
	return err
}

================
File: clients/logs/eks_cloudwatch_logs_client.go
================
package logs
import (
	"encoding/json"
	"fmt"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/aws/aws-sdk-go/aws/awserr"
	"github.com/aws/aws-sdk-go/aws/request"
	"github.com/aws/aws-sdk-go/aws/session"
	"github.com/aws/aws-sdk-go/service/cloudwatchlogs"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/exceptions"
	"github.com/stitchfix/flotilla-os/state"
	"log"
	"net/http"
	"os"
	"sort"
	"strings"
	awstrace "gopkg.in/DataDog/dd-trace-go.v1/contrib/aws/aws-sdk-go/aws"
)
type EKSCloudWatchLogsClient struct {
	logRetentionInDays int64
	logNamespace       string
	logsClient         logsClient
	logger             *log.Logger
}
type EKSCloudWatchLog struct {
	Log string `json:"log"`
}
func (lc *EKSCloudWatchLogsClient) Name() string {
	return "eks-cloudwatch"
}
func (lc *EKSCloudWatchLogsClient) Initialize(conf config.Config) error {
	awsRegion := conf.GetString("eks_log_driver_options_awslogs_region")
	if len(awsRegion) == 0 {
		awsRegion = conf.GetString("aws_default_region")
	}
	if len(awsRegion) == 0 {
		return errors.Errorf(
			"EKSCloudWatchLogsClient needs one of [eks.log.driver.options.awslogs-region] or [aws_default_region] set in config")
	}
	lc.logNamespace = conf.GetString("eks_log_namespace")
	if len(lc.logNamespace) == 0 {
		return errors.Errorf(
			"EKSCloudWatchLogsClient needs one of [eks.log.driver.options.awslogs-group] or [eks.log.namespace] set in config")
	}
	lc.logRetentionInDays = int64(conf.GetInt("eks_log_retention_days"))
	if lc.logRetentionInDays == 0 {
		lc.logRetentionInDays = int64(30)
	}
	flotillaMode := conf.GetString("flotilla_mode")
	if flotillaMode != "test" {
		sess := awstrace.WrapSession(session.Must(session.NewSession(&aws.Config{
			Region: aws.String(awsRegion)})))
		lc.logsClient = cloudwatchlogs.New(sess)
	}
	lc.logger = log.New(os.Stderr, "[cloudwatchlogs] ",
		log.Ldate|log.Ltime|log.Lshortfile)
	return lc.createNamespaceIfNotExists()
}
func (lc *EKSCloudWatchLogsClient) Logs(executable state.Executable, run state.Run, lastSeen *string, role *string, facility *string) (string, *string, error) {
	startFromHead := true
	if run.PodName == nil {
		return "", nil, nil
	}
	handle := lc.toStreamName(run)
	args := &cloudwatchlogs.GetLogEventsInput{
		LogGroupName:  &lc.logNamespace,
		LogStreamName: &handle,
		StartFromHead: &startFromHead,
	}
	if lastSeen != nil && len(*lastSeen) > 0 {
		args.NextToken = lastSeen
	}
	result, err := lc.logsClient.GetLogEvents(args)
	if err != nil {
		if aerr, ok := err.(awserr.Error); ok {
			if aerr.Code() == cloudwatchlogs.ErrCodeResourceNotFoundException {
				return "", nil, exceptions.MissingResource{err.Error()}
			} else if request.IsErrorThrottle(err) {
				lc.logger.Printf(
					"thottled getting logs; executable_id: %v, run_id: %s, error: %+v\n",
					executable.GetExecutableID(), run.RunID, err)
				return "", lastSeen, nil
			}
		}
		return "", nil, errors.Wrap(err, "problem getting logs")
	}
	if len(result.Events) == 0 {
		return "", result.NextForwardToken, nil
	}
	message := lc.logsToMessage(result.Events)
	return message, result.NextForwardToken, nil
}
// This method doesn't return log string, it is a placeholder only.
func (lc *EKSCloudWatchLogsClient) LogsText(executable state.Executable, run state.Run, w http.ResponseWriter) error {
	return errors.Errorf("EKSCloudWatchLogsClient does not support LogsText method.")
}
// Generate stream name
func (lc *EKSCloudWatchLogsClient) toStreamName(run state.Run) string {
	return fmt.Sprintf("%s", *run.PodName)
}
func (lc *EKSCloudWatchLogsClient) logsToMessage(events []*cloudwatchlogs.OutputLogEvent) string {
	sort.Sort(byTimestamp(events))
	messages := make([]string, len(events))
	for i, event := range events {
		var l EKSCloudWatchLog
		err := json.Unmarshal([]byte(*event.Message), &l)
		if err != nil {
			messages[i] = *event.Message
		}
		messages[i] = l.Log
	}
	return strings.Join(messages, "")
}
func (lc *EKSCloudWatchLogsClient) createNamespaceIfNotExists() error {
	exists, err := lc.namespaceExists()
	if err != nil {
		return errors.Wrapf(err, "problem checking if log namespace [%s] exists", lc.logNamespace)
	}
	if !exists {
		return lc.createNamespace()
	}
	return nil
}
func (lc *EKSCloudWatchLogsClient) namespaceExists() (bool, error) {
	result, err := lc.logsClient.DescribeLogGroups(&cloudwatchlogs.DescribeLogGroupsInput{
		LogGroupNamePrefix: &lc.logNamespace,
	})
	if err != nil {
		return false, errors.Wrapf(err, "problem describing log groups with prefix [%s]", lc.logNamespace)
	}
	if len(result.LogGroups) == 0 {
		return false, nil
	}
	for _, group := range result.LogGroups {
		if *group.LogGroupName == lc.logNamespace {
			return true, nil
		}
	}
	return false, nil
}
func (lc *EKSCloudWatchLogsClient) createNamespace() error {
	_, err := lc.logsClient.CreateLogGroup(&cloudwatchlogs.CreateLogGroupInput{
		LogGroupName: &lc.logNamespace,
	})
	if err != nil {
		return errors.Wrapf(err, "problem creating log group with log group name [%s]", lc.logNamespace)
	}
	_, err = lc.logsClient.PutRetentionPolicy(&cloudwatchlogs.PutRetentionPolicyInput{
		LogGroupName:    &lc.logNamespace,
		RetentionInDays: &lc.logRetentionInDays,
	})
	if err != nil {
		return errors.Wrapf(err, "problem setting log group retention policy for log group name [%s]", lc.logNamespace)
	}
	return nil
}

================
File: clients/logs/eks_s3_logs_client.go
================
package logs
import (
	"bufio"
	"bytes"
	"compress/gzip"
	"context"
	"encoding/json"
	"fmt"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/aws/aws-sdk-go/aws/request"
	"github.com/aws/aws-sdk-go/aws/session"
	"github.com/aws/aws-sdk-go/service/s3"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/state"
	awstrace "gopkg.in/DataDog/dd-trace-go.v1/contrib/aws/aws-sdk-go/aws"
	"io"
	"log"
	"net/http"
	"os"
	"strconv"
	"strings"
	"time"
)
type EKSS3LogsClient struct {
	logRetentionInDays int64
	logNamespace       string
	s3Client           *s3.S3
	s3Bucket           string
	s3BucketRootDir    string
	logger             *log.Logger
	emrS3LogsBucket    string
	emrS3LogsBasePath  string
}
type s3Log struct {
	Log    string    `json:"log"`
	Stream string    `json:"stream"`
	Time   time.Time `json:"time"`
}
func (lc *EKSS3LogsClient) Name() string {
	return "eks-s3"
}
func (lc *EKSS3LogsClient) Initialize(conf config.Config) error {
	awsRegion := conf.GetString("eks_log_driver_options_awslogs_region")
	if len(awsRegion) == 0 {
		awsRegion = conf.GetString("aws_default_region")
	}
	if len(awsRegion) == 0 {
		return errors.Errorf(
			"EKSS3LogsClient needs one of [eks.log.driver.options.awslogs-region] or [aws_default_region] set in config")
	}
	flotillaMode := conf.GetString("flotilla_mode")
	if flotillaMode != "test" {
		sess := awstrace.WrapSession(session.Must(session.NewSession(&aws.Config{
			Region: aws.String(awsRegion)})))
		sess = awstrace.WrapSession(sess)
		lc.s3Client = s3.New(sess, aws.NewConfig().WithRegion(awsRegion))
	}
	lc.emrS3LogsBucket = conf.GetString("emr_log_bucket")
	lc.emrS3LogsBasePath = conf.GetString("emr_log_base_path")
	s3BucketName := conf.GetString("eks_log_driver_options_s3_bucket_name")
	if len(s3BucketName) == 0 {
		return errors.Errorf(
			"EKSS3LogsClient needs [eks_log_driver_options_s3_bucket_name] set in config")
	}
	lc.s3Bucket = s3BucketName
	s3BucketRootDir := conf.GetString("eks_log_driver_options_s3_bucket_root_dir")
	if len(s3BucketRootDir) == 0 {
		return errors.Errorf(
			"EKSS3LogsClient needs [eks.log.driver.options.s3_bucket_root_dir] set in config")
	}
	lc.s3BucketRootDir = s3BucketRootDir
	lc.logger = log.New(os.Stderr, "[s3logs] ",
		log.Ldate|log.Ltime|log.Lshortfile)
	return nil
}
func (lc *EKSS3LogsClient) emrLogsToMessageString(run state.Run, lastSeen *string, role *string, facility *string) (string, *string, error) {
	s3DirName, err := lc.emrDriverLogsPath(run)
	if err != nil {
		return "", aws.String(""), errors.Errorf("No logs")
	}
	params := &s3.ListObjectsV2Input{
		Bucket:  aws.String(lc.emrS3LogsBucket),
		Prefix:  aws.String(s3DirName),
		MaxKeys: aws.Int64(1000),
	}
	pageNum := 0
	lastModified := &time.Time{}
	var key *string
	err = lc.s3Client.ListObjectsV2Pages(params,
		func(result *s3.ListObjectsV2Output, lastPage bool) bool {
			pageNum++
			if result != nil {
				for _, content := range result.Contents {
					if strings.Contains(*content.Key, *role) && strings.Contains(*content.Key, *facility) && lastModified.Before(*content.LastModified) {
						if content != nil && *content.Size < int64(10000000) {
							key = content.Key
							lastModified = content.LastModified
						}
					}
				}
			}
			if lastPage {
				return false
			}
			return pageNum <= 10
		})
	if key == nil {
		lc.logger.Println(fmt.Sprintf("run=%s emr logging key not found for role=%s facility=%s", run.RunID, *role, *facility))
		return "", aws.String(""), errors.Errorf("No driver logs found")
	}
	startPosition := int64(0)
	if lastSeen != nil {
		parsed, err := strconv.ParseInt(*lastSeen, 10, 64)
		if err == nil {
			startPosition = parsed
		}
	}
	s3Obj, err := lc.s3Client.GetObjectWithContext(
		context.Background(),
		&s3.GetObjectInput{
			Bucket: aws.String(lc.emrS3LogsBucket),
			Key:    aws.String(*key),
		}, func(r *request.Request) {
			r.HTTPRequest.Header.Add("Accept-Encoding", "gzip")
		})
	if s3Obj != nil && err == nil {
		if s3Obj.ContentLength != nil && *s3Obj.ContentLength > int64(10000000) {
			return "", aws.String(""), errors.Errorf("Logs > 10MB, will not display.")
		}
		defer s3Obj.Body.Close()
		gr, err := gzip.NewReader(s3Obj.Body)
		if err != nil {
			return "", aws.String(""), err
		}
		defer gr.Close()
		reader := bufio.NewReader(gr)
		var b0 bytes.Buffer
		counter := int64(0)
		for {
			line, err := reader.ReadBytes('\n')
			if err != nil {
				if err == io.EOF {
					err = nil
					return b0.String(), aws.String(fmt.Sprintf("%d", counter)), nil
				}
			} else {
				if counter >= startPosition {
					b0.Write(line)
				}
				counter = counter + 1
			}
		}
	}
	return "", aws.String(""), errors.Errorf("No driver logs found")
}
func (lc *EKSS3LogsClient) emrDriverLogsPath(run state.Run) (string, error) {
	if run.SparkExtension.EMRJobId != nil &&
		run.SparkExtension.VirtualClusterId != nil {
		return fmt.Sprintf("%s/%s/jobs/%s/",
			lc.emrS3LogsBasePath,
			*run.SparkExtension.VirtualClusterId,
			*run.SparkExtension.EMRJobId,
		), nil
	}
	return "", errors.New("couldn't construct s3 path.")
}
func (lc *EKSS3LogsClient) Logs(executable state.Executable, run state.Run, lastSeen *string, role *string, facility *string) (string, *string, error) {
	if *run.Engine == state.EKSSparkEngine {
		return lc.emrLogsToMessageString(run, lastSeen, role, facility)
	}
	result, err := lc.getS3Object(run)
	startPosition := int64(0)
	if lastSeen != nil {
		parsed, err := strconv.ParseInt(*lastSeen, 10, 64)
		if err == nil {
			startPosition = parsed
		}
	}
	if result != nil && err == nil {
		acc, position, err := lc.logsToMessageString(result, startPosition)
		newLastSeen := fmt.Sprintf("%d", position)
		return acc, &newLastSeen, err
	}
	return "", aws.String(""), errors.Errorf("No logs.")
}
// Logs returns all logs from the log stream identified by handle since lastSeen
func (lc *EKSS3LogsClient) LogsText(executable state.Executable, run state.Run, w http.ResponseWriter) error {
	if run.Engine == nil || *run.Engine == state.EKSEngine {
		result, err := lc.getS3Object(run)
		if err != nil {
			return err
		} else if result != nil {
			return lc.logsToMessage(result, w)
		}
	}
	if *run.Engine == state.EKSSparkEngine {
		return lc.logsEMR(w)
	}
	return nil
}
// Fetch S3Object associated with the pod's log.
func (lc *EKSS3LogsClient) getS3Object(run state.Run) (*s3.GetObjectOutput, error) {
	//Pod isn't there yet - dont return a 404
	//if run.PodName == nil {
	//	return nil, errors.New("no pod associated with the run.")
	//}
	s3DirName := lc.toS3DirName(run)
	// Get list of S3 objects in the run_id folder.
	result, err := lc.s3Client.ListObjects(&s3.ListObjectsInput{
		Bucket: aws.String(lc.s3Bucket),
		Prefix: aws.String(s3DirName),
	})
	if err != nil {
		return nil, errors.Wrap(err, "problem getting logs")
	}
	if result == nil || result.Contents == nil || len(result.Contents) == 0 {
		return nil, errors.New("no s3 files associated with the run.")
	}
	var key *string
	lastModified := &time.Time{}
	for _, content := range result.Contents {
		if strings.Contains(*content.Key, run.RunID) && lastModified.Before(*content.LastModified) {
			if content != nil && *content.Size < int64(10000000) {
				key = content.Key
				lastModified = content.LastModified
			}
		}
	}
	if key != nil {
		return lc.getS3Key(key)
	} else {
		return nil, errors.New("no s3 files associated with the run.")
	}
}
func (lc *EKSS3LogsClient) getS3Key(s3Key *string) (*s3.GetObjectOutput, error) {
	result, err := lc.s3Client.GetObject(&s3.GetObjectInput{
		Bucket: aws.String(lc.s3Bucket),
		Key:    aws.String(*s3Key),
	})
	if err != nil {
		return nil, err
	}
	return result, nil
}
func (lc *EKSS3LogsClient) toS3DirName(run state.Run) string {
	return fmt.Sprintf("%s/%s", lc.s3BucketRootDir, run.RunID)
}
func (lc *EKSS3LogsClient) logsToMessage(result *s3.GetObjectOutput, w http.ResponseWriter) error {
	reader := bufio.NewReader(result.Body)
	for {
		line, err := reader.ReadBytes('\n')
		if err != nil {
			if err == io.EOF {
				err = nil
			}
			return err
		} else {
			var parsedLine s3Log
			parsedLine, err := parseLines(line)
			if err != nil {
				return err
			}
			_, err = io.WriteString(w, parsedLine.Log)
			if err != nil {
				return err
			}
		}
	}
}
func (lc *EKSS3LogsClient) logsEMR(w http.ResponseWriter) error {
	_, _ = io.WriteString(w, "todo!!!")
	return nil
}
func (lc *EKSS3LogsClient) logsToMessageString(result *s3.GetObjectOutput, startingPosition int64) (string, int64, error) {
	acc := ""
	currentPosition := int64(0)
	// if less than/equal to 0, read entire log.
	if startingPosition <= 0 {
		startingPosition = currentPosition
	}
	// No S3 file or object, return "", 0, err
	if result == nil {
		return acc, startingPosition, errors.New("s3 object not present.")
	}
	reader := bufio.NewReader(result.Body)
	// Reading until startingPosition and discard unneeded lines.
	for currentPosition < startingPosition {
		currentPosition = currentPosition + 1
		_, err := reader.ReadBytes('\n')
		if err != nil {
			if err == io.EOF {
				err = nil
			}
			return acc, startingPosition, err
		}
	}
	// Read upto MaxLogLines
	for currentPosition <= startingPosition+state.MaxLogLines {
		currentPosition = currentPosition + 1
		line, err := reader.ReadBytes('\n')
		if err != nil {
			if err == io.EOF {
				err = nil
			}
			return acc, currentPosition, err
		} else {
			parsedLine, err := parseLines(line)
			if err == nil {
				acc = fmt.Sprintf("%s%s", acc, parsedLine.Log)
			}
		}
	}
	_ = result.Body.Close()
	return acc, currentPosition, nil
}
func parseLines(input []byte) (s3Log, error) {
	var parsedInput s3Log
	err := json.Unmarshal(input, &parsedInput)
	if err != nil {
		splitLines := strings.Split(string(input), " ")
		if len(splitLines) > 0 {
			layout := "2006-01-02T15:04:05.999999999Z"
			timestamp, err := time.Parse(layout, splitLines[0])
			if err != nil {
				return parsedInput, err
			}
			parsedInput.Time = timestamp
			parsedInput.Stream = splitLines[1]
			parsedInput.Log = strings.Join(splitLines[3:], " ")
		}
	}
	return parsedInput, nil
}

================
File: clients/logs/logs.go
================
package logs
import (
	"fmt"
	"github.com/aws/aws-sdk-go/service/cloudwatchlogs"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/state"
	"net/http"
)
type Client interface {
	Name() string
	Initialize(config config.Config) error
	Logs(executable state.Executable, run state.Run, lastSeen *string, role *string, facility *string) (string, *string, error)
	LogsText(executable state.Executable, run state.Run, w http.ResponseWriter) error
}
type logsClient interface {
	DescribeLogGroups(input *cloudwatchlogs.DescribeLogGroupsInput) (*cloudwatchlogs.DescribeLogGroupsOutput, error)
	CreateLogGroup(input *cloudwatchlogs.CreateLogGroupInput) (*cloudwatchlogs.CreateLogGroupOutput, error)
	PutRetentionPolicy(input *cloudwatchlogs.PutRetentionPolicyInput) (*cloudwatchlogs.PutRetentionPolicyOutput, error)
	GetLogEvents(input *cloudwatchlogs.GetLogEventsInput) (*cloudwatchlogs.GetLogEventsOutput, error)
}
type byTimestamp []*cloudwatchlogs.OutputLogEvent
func (events byTimestamp) Len() int           { return len(events) }
func (events byTimestamp) Swap(i, j int)      { events[i], events[j] = events[j], events[i] }
func (events byTimestamp) Less(i, j int) bool { return *(events[i].Timestamp) < *(events[j].Timestamp) }
func NewLogsClient(conf config.Config, logger flotillaLog.Logger, name string) (Client, error) {
	_ = logger.Log("message", "Initializing logs client", "client", name)
	switch name {
	case "eks":
		ekscw := &EKSS3LogsClient{}
		if err := ekscw.Initialize(conf); err != nil {
			return nil, errors.Wrap(err, "problem initializing EKSCloudWatchLogsClient")
		}
		return ekscw, nil
	default:
		return nil, fmt.Errorf("No Client named [%s] was found", name)
	}
}

================
File: clients/metrics/datadog_metrics_client.go
================
package metrics
import (
	"fmt"
	"github.com/DataDog/datadog-go/v5/statsd"
	"github.com/stitchfix/flotilla-os/config"
	"os"
	"strings"
	"time"
)
type DatadogStatsdMetricsClient struct {
	client *statsd.Client
}
func (dd *DatadogStatsdMetricsClient) Init(conf config.Config) error {
	host := os.Getenv("DD_AGENT_HOST")
	var addr string
	if strings.Contains(host, ":") && !strings.Contains(host, "[") {
		addr = fmt.Sprintf("[%s]:8125", host)
	} else {
		addr = fmt.Sprintf("%s:8125", host)
	}
	client, err := statsd.New(addr)
	if err != nil {
		return err
	}
	dd.client = client
	return nil
}
func (dd *DatadogStatsdMetricsClient) Decrement(name Metric, tags []string, rate float64) error {
	return dd.client.Decr(string(name), tags, rate)
}
func (dd *DatadogStatsdMetricsClient) Increment(name Metric, tags []string, rate float64) error {
	return dd.client.Incr(string(name), tags, rate)
}
func (dd *DatadogStatsdMetricsClient) Histogram(name Metric, value float64, tags []string, rate float64) error {
	return dd.client.Histogram(string(name), value, tags, rate)
}
func (dd *DatadogStatsdMetricsClient) Distribution(name Metric, value float64, tags []string, rate float64) error {
	return dd.client.Distribution(string(name), value, tags, rate)
}
func (dd *DatadogStatsdMetricsClient) Timing(name Metric, value time.Duration, tags []string, rate float64) error {
	return dd.client.Timing(string(name), value, tags, rate)
}
func (dd *DatadogStatsdMetricsClient) Set(name Metric, value string, tags []string, rate float64) error {
	return dd.client.Set(string(name), value, tags, rate)
}
func (dd *DatadogStatsdMetricsClient) Event(e event) error {
	se := statsd.NewEvent(e.Title, e.Text)
	se.Tags = e.Tags
	return dd.client.Event(se)
}

================
File: clients/metrics/metrics.go
================
package metrics
import (
	"fmt"
	"sync"
	"time"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
)
type Metric string
const (
	EngineEKSExecute Metric = "engine.eks.execute"
	EngineEKSEnqueue Metric = "engine.eks.enqueue"
	EngineEMRExecute Metric = "engine.emr.execute"
	EngineEMREnqueue Metric = "engine.emr.enqueue"
	EngineEKSTerminate Metric = "engine.eks.terminate"
	EngineEMRTerminate Metric = "engine.emr.terminate"
	EngineEKSRunPodnameChange Metric = "engine.eks.run_podname_changed"
	EngineEKSNodeTriggeredScaledUp Metric = "engine.eks.triggered_scale_up"
	StatusWorkerProcessEKSRun Metric = "status_worker.timing.process_eks_run"
	StatusWorkerAcquireLock Metric = "status_worker.timing.acquire_lock"
	StatusWorkerFetchPodMetrics Metric = "status_worker.timing.fetch_pod_metrics"
	StatusWorkerFetchUpdateStatus Metric = "status_worker.timing.fetch_update_status"
	StatusWorkerLockedRuns Metric = "status_worker.locked_runs"
	StatusWorkerFetchMetrics Metric = "status_worker.fetch_metrics"
	StatusWorkerGetPodList Metric = "status_worker.get_pod_list"
	StatusWorkerGetEvents Metric = "status_worker.get_events"
	StatusWorkerGetJob Metric = "status_worker.get_job"
	EngineUpdateRun Metric = "engine.update_run"
)
type MetricTag string
const (
	StatusSuccess MetricTag = "status:success"
	StatusFailure MetricTag = "status:failure"
)
type Client interface {
	Init(conf config.Config) error
	Decrement(name Metric, tags []string, rate float64) error
	Increment(name Metric, tags []string, rate float64) error
	Histogram(name Metric, value float64, tags []string, rate float64) error
	Distribution(name Metric, value float64, tags []string, rate float64) error
	Set(name Metric, value string, tags []string, rate float64) error
	Event(evt event) error
	Timing(name Metric, value time.Duration, tags []string, rate float64) error
}
type event struct {
	Title string
	Text  string
	Tags  []string
}
var once sync.Once
var instance Client
func InstantiateClient(conf config.Config) error {
	if !conf.IsSet("metrics_client") {
		return fmt.Errorf("`metrics_client` not set in config, unable to instantiate metrics client")
	}
	var err error = nil
	name := conf.GetString("metrics_client")
	once.Do(func() {
		switch name {
		case "dogstatsd":
			instance = &DatadogStatsdMetricsClient{}
			if err = instance.Init(conf); err != nil {
				err = errors.Errorf("Unable to initialize dogstatsd client.")
				instance = nil
				break
			}
		default:
			err = fmt.Errorf("no client named [%s] was found", name)
		}
	})
	return err
}
func Decrement(name Metric, tags []string, rate float64) error {
	if instance != nil {
		return instance.Decrement(name, tags, rate)
	}
	return errors.Errorf("MetricsClient instance is nil, unable to send Decrement metric.")
}
func Increment(name Metric, tags []string, rate float64) error {
	if instance != nil {
		return instance.Increment(name, tags, rate)
	}
	return errors.Errorf("MetricsClient instance is nil, unable to send Increment metric.")
}
func Histogram(name Metric, value float64, tags []string, rate float64) error {
	if instance != nil {
		return instance.Histogram(name, value, tags, rate)
	}
	return errors.Errorf("MetricsClient instance is nil, unable to send Histogram metric.")
}
func Distribution(name Metric, value float64, tags []string, rate float64) error {
	if instance != nil {
		return instance.Distribution(name, value, tags, rate)
	}
	return errors.Errorf("MetricsClient instance is nil, unable to send Distribution metric.")
}
func Set(name Metric, value string, tags []string, rate float64) error {
	if instance != nil {
		return instance.Set(name, value, tags, rate)
	}
	return errors.Errorf("MetricsClient instance is nil, unable to send Set metric.")
}
func Event(title string, text string, tags []string) error {
	if instance != nil {
		return instance.Event(event{
			Title: title,
			Text:  text,
			Tags:  tags,
		})
	}
	return errors.Errorf("MetricsClient instance is nil, unable to send Event metric.")
}
func Timing(name Metric, value time.Duration, tags []string, rate float64) error {
	if instance != nil {
		return instance.Timing(name, value, tags, rate)
	}
	return errors.Errorf("MetricsClient instance is nil, unable to send Event metric.")
}

================
File: clients/middleware/client.go
================
package middleware
import (
	"github.com/stitchfix/flotilla-os/state"
	"net/http"
)
type Client interface {
	AnnotateLaunchRequest(headers *http.Header, lr *state.LaunchRequestV2) error
}
type middlewareClient struct{}
func NewClient() (Client, error) {
	return &middlewareClient{}, nil
}
func (mwC middlewareClient) AnnotateLaunchRequest(headers *http.Header, lr *state.LaunchRequestV2) error {
	return nil
}

================
File: conf/config.yml
================
aws_default_region: us-east-1
cluster_client: eks
create_database_schema: true
database_url: postgresql://flotilla:flotilla@localhost/flotilla?sslmode=disable
eks_clusters: 'clusta, cupcake'
eks_cluster_default: 'clusta'
eks_gpu_cluster_default: 'clusta'
eks_log_driver_name: awslogs
eks_log_driver_options_awslogs-group: flotilla-eks-namespace
eks_log_driver_options_awslogs-region: us-east-1
eks_log_namespace: flotilla-eks-namespace
eks_log_retention_days: 90
enabled_workers:
  - retry
  - submit
execution_engine: eks
flotilla_mode: test
http_server_cors_allowed_origins:
  - http://localhost:3001
http_server_listen_address: :3000
http_server_read_timeout_seconds: 5
http_server_write_timeout_seconds: 10
logs_client: cloudwatch
metrics_client: dogstatsd
metrics_dogstatsd_address: 127.0.0.1:8125
metrics_dogstatsd_namespace: my.flotilla.namespace
metrics_dogstatsd_tags:
  - test
owner_id_var: FLOTILLA_RUN_OWNER_ID
queue_manager: sqs
queue_namespace: dev-flotilla
queue_process_time: 45
queue_retention_seconds: 604800
queue_status: flotilla-status-updates-dev
queue_status_rule: flotilla-task-status
readonly_database_url: postgresql://flotilla:flotilla@localhost/flotilla?sslmode=disable

================
File: config/config.go
================
package config
import (
	"github.com/pkg/errors"
	"github.com/spf13/viper"
	"strings"
)
type Config interface {
	GetString(key string) string
	GetStringSlice(key string) []string
	GetStringMapString(key string) map[string]string
	GetInt(key string) int
	GetBool(key string) bool
	GetFloat64(key string) float64
	IsSet(key string) bool
}
func NewConfig(confDir *string) (Config, error) {
	v := viper.New()
	if v == nil {
		return &conf{}, errors.New("Error initializing internal config")
	}
	if confDir != nil {
		v.SetConfigName("config")
		v.SetConfigType("yaml")
		v.AddConfigPath(*confDir)
		if err := v.ReadInConfig(); err != nil {
			return &conf{}, errors.Wrapf(err, "problem reading config from [%s]", *confDir)
		}
	}
	v.AutomaticEnv()
	v.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
	return &conf{v}, nil
}
type conf struct {
	v *viper.Viper
}
func (c *conf) GetString(key string) string {
	return c.v.GetString(key)
}
func (c *conf) GetFloat64(key string) float64 {
	return c.v.GetFloat64(key)
}
func (c *conf) GetInt(key string) int {
	return c.v.GetInt(key)
}
func (c *conf) GetBool(key string) bool {
	return c.v.GetBool(key)
}
func (c *conf) GetStringMapString(key string) map[string]string {
	return c.v.GetStringMapString(key)
}
func (c *conf) GetStringSlice(key string) []string {
	return c.v.GetStringSlice(key)
}
func (c *conf) IsSet(key string) bool {
	return c.v.IsSet(key)
}

================
File: docs/ara.md
================
*Adaptive Resource Allocation for Kubernetes Pods*

At StitchFix we empower our data scientists to deploy their models and applications end to end without needing engineering skills. To facilitate batch processing we use Flotilla, a task execution service. Flotilla can run jobs on top of Kubernetes or AWS ECS.

One of the problems we faced was how much CPU and memory should we assign to the container pods? The workloads are highly variable on their demands. 

If we give too few resources the jobs may run slower and in the pathological case of running out of memory. If we give too much we are wasting resources and starving other jobs that could potentially be scheduled alongside. 

Solution
The first step was to accurately record the utilization of the resources per pod. We looked at a few different monitoring solutions (kube-state-metrics, Prometheus, and metrics-server). We decided to use the metrics-server since it provided a simple API and tracked the state of the pods in memory. 

```
helm install --name=metrics-server --namespace=kube-system --set args={'--metric-resolution=1s'} stable/metrics-server
```
To instrument fetching the pod metrics, we used the metrics ClientSet. While the job is running, Flotilla fetches the metrics every 2-5 seconds.

If the prior recorded value of memory and CPU are lower than what the Metrics Server is outputting the highest of the two are recorded back with job metadata.

Also, an MD5 checksum of the command and its arguments are stored in the database. This becomes a signature of the job and its resources. 

The core [query for ARA](https://github.com/stitchfix/flotilla-os/blob/master/state/pg_queries.go#L53-L66) and the associated [adapter code](https://github.com/stitchfix/flotilla-os/blob/master/execution/adapter/eks_adapter.go#L269-L301)

================
File: exceptions/errors.go
================
package exceptions
type MalformedInput struct {
	ErrorString string
}
func (e MalformedInput) Error() string {
	return e.ErrorString
}
type ConflictingResource struct {
	ErrorString string
}
func (e ConflictingResource) Error() string {
	return e.ErrorString
}
type MissingResource struct {
	ErrorString string
}
func (e MissingResource) Error() string {
	return e.ErrorString
}

================
File: execution/adapter/eks_adapter.go
================
package adapter
import (
	"fmt"
	utils "github.com/stitchfix/flotilla-os/execution"
	"regexp"
	"strings"
	"time"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/stitchfix/flotilla-os/state"
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
type EKSAdapter interface {
	AdaptJobToFlotillaRun(job *batchv1.Job, run state.Run, pod *corev1.Pod) (state.Run, error)
	AdaptFlotillaDefinitionAndRunToJob(executable state.Executable, run state.Run, schedulerName string, manager state.Manager, araEnabled bool) (batchv1.Job, error)
}
type eksAdapter struct{}
func NewEKSAdapter() (EKSAdapter, error) {
	adapter := eksAdapter{}
	return &adapter, nil
}
func (a *eksAdapter) AdaptJobToFlotillaRun(job *batchv1.Job, run state.Run, pod *corev1.Pod) (state.Run, error) {
	updated := run
	if job.Status.Active == 1 && job.Status.CompletionTime == nil {
		updated.Status = state.StatusRunning
	} else if job.Status.Succeeded == 1 {
		if pod != nil {
			if pod.Status.Phase == corev1.PodSucceeded {
				var exitCode int64 = 0
				var exitReason = fmt.Sprintf("Pod %s Exited Successfully", pod.Name)
				updated.ExitReason = &exitReason
				updated.Status = state.StatusStopped
				updated.ExitCode = &exitCode
			}
		} else {
			var exitCode int64 = 0
			updated.Status = state.StatusStopped
			updated.ExitCode = &exitCode
		}
	} else if job.Status.Failed == 1 {
		var exitCode int64 = 1
		updated.Status = state.StatusStopped
		if pod != nil {
			if pod.Status.ContainerStatuses != nil && len(pod.Status.ContainerStatuses) > 0 {
				containerStatus := pod.Status.ContainerStatuses[len(pod.Status.ContainerStatuses)-1]
				if containerStatus.State.Terminated != nil {
					updated.ExitReason = &containerStatus.State.Terminated.Reason
					exitCode = int64(containerStatus.State.Terminated.ExitCode)
				}
			}
		}
		updated.ExitCode = &exitCode
	}
	if pod != nil && len(pod.Spec.Containers) > 0 {
		container := pod.Spec.Containers[0]
		if len(container.Command) > 3 {
			cmd := strings.Join(container.Command[3:], "\n")
			updated.Command = &cmd
		}
	}
	if job != nil && job.Status.StartTime != nil {
		updated.StartedAt = &job.Status.StartTime.Time
	}
	if updated.Status == state.StatusStopped {
		if job != nil && job.Status.CompletionTime != nil {
			updated.FinishedAt = &job.Status.CompletionTime.Time
		} else {
			finishedAt := time.Now()
			updated.FinishedAt = &finishedAt
		}
	}
	return updated, nil
}
func (a *eksAdapter) AdaptFlotillaDefinitionAndRunToJob(executable state.Executable, run state.Run, schedulerName string, manager state.Manager, araEnabled bool) (batchv1.Job, error) {
	cmd := ""
	if run.Command != nil && len(*run.Command) > 0 {
		cmd = *run.Command
	}
	cmdSlice := a.constructCmdSlice(cmd)
	cmd = strings.Join(cmdSlice[3:], "\n")
	run.Command = &cmd
	resourceRequirements, run := a.constructResourceRequirements(executable, run, manager, araEnabled)
	volumeMounts, volumes := a.constructVolumeMounts(executable, run, manager, araEnabled)
	container := corev1.Container{
		Name:      run.RunID,
		Image:     run.Image,
		Command:   cmdSlice,
		Resources: resourceRequirements,
		Env:       a.envOverrides(executable, run),
		Ports:     a.constructContainerPorts(executable),
	}
	if volumeMounts != nil {
		container.VolumeMounts = volumeMounts
	}
	affinity := a.constructAffinity(executable, run, manager)
	tolerations := a.constructTolerations(executable, run)
	annotations := map[string]string{}
	annotations["prometheus.io/port"] = "9090"
	annotations["prometheus.io/scrape"] = "true"
	labels := utils.GetLabels(run)
	jobSpec := batchv1.JobSpec{
		TTLSecondsAfterFinished: &state.TTLSecondsAfterFinished,
		ActiveDeadlineSeconds:   run.ActiveDeadlineSeconds,
		BackoffLimit:            &state.EKSBackoffLimit,
		Template: corev1.PodTemplateSpec{
			ObjectMeta: v1.ObjectMeta{
				Annotations: annotations,
				Labels:      labels,
			},
			Spec: corev1.PodSpec{
				SchedulerName:      schedulerName,
				Containers:         []corev1.Container{container},
				RestartPolicy:      corev1.RestartPolicyNever,
				ServiceAccountName: *run.ServiceAccount,
				Affinity:           affinity,
				Tolerations:        tolerations,
			},
		},
	}
	if volumes != nil {
		jobSpec.Template.Spec.Volumes = volumes
	}
	eksJob := batchv1.Job{
		Spec: jobSpec,
		ObjectMeta: v1.ObjectMeta{
			Name: run.RunID,
		},
	}
	return eksJob, nil
}
func (a *eksAdapter) constructEviction(run state.Run, manager state.Manager) string {
	if run.Gpu != nil && *run.Gpu > 0 {
		return "false"
	}
	if run.NodeLifecycle != nil && *run.NodeLifecycle == state.OndemandLifecycle {
		return "false"
	}
	if run.CommandHash != nil {
		nodeType, err := manager.GetNodeLifecycle(run.DefinitionID, *run.CommandHash)
		if err == nil && nodeType == state.OndemandLifecycle {
			return "false"
		}
	}
	return "true"
}
func (a *eksAdapter) constructContainerPorts(executable state.Executable) []corev1.ContainerPort {
	var containerPorts []corev1.ContainerPort
	executableResources := executable.GetExecutableResources()
	if executableResources.Ports != nil && len(*executableResources.Ports) > 0 {
		for _, port := range *executableResources.Ports {
			containerPorts = append(containerPorts, corev1.ContainerPort{
				ContainerPort: int32(port),
			})
		}
	}
	return containerPorts
}
func (a *eksAdapter) constructTolerations(executable state.Executable, run state.Run) []corev1.Toleration {
	executableResources := executable.GetExecutableResources()
	tolerations := []corev1.Toleration{}
	if (executableResources.Gpu != nil && *executableResources.Gpu > 0) || (run.Gpu != nil && *run.Gpu > 0) {
		toleration := corev1.Toleration{
			Key:      "nvidia.com/gpu",
			Operator: "Equal",
			Value:    "true",
			Effect:   "NoSchedule",
		}
		tolerations = append(tolerations, toleration)
	}
	return tolerations
}
func (a *eksAdapter) constructAffinity(executable state.Executable, run state.Run, manager state.Manager) *corev1.Affinity {
	affinity := &corev1.Affinity{}
	var requiredMatch []corev1.NodeSelectorRequirement
	var preferredMatches []corev1.PreferredSchedulingTerm
	nodeLifecycleKey := "karpenter.sh/capacity-type"
	nodeArchKey := "kubernetes.io/arch"
	switch run.ClusterName {
	case "flotilla-eks-infra-c":
		nodeLifecycleKey = "node.kubernetes.io/lifecycle"
		nodeArchKey = "kubernetes.io/arch"
	}
	var nodeLifecycle []string
	if run.NodeLifecycle != nil && *run.NodeLifecycle == state.OndemandLifecycle {
		nodeLifecycle = append(nodeLifecycle, "on-demand", "normal")
	} else {
		nodeLifecycle = append(nodeLifecycle, "spot", "on-demand", "normal")
	}
	arch := []string{"amd64"}
	if run.Arch != nil && *run.Arch == "arm64" {
		arch = []string{"arm64"}
	}
	requiredMatch = append(requiredMatch, corev1.NodeSelectorRequirement{
		Key:      nodeLifecycleKey,
		Operator: corev1.NodeSelectorOpIn,
		Values:   nodeLifecycle,
	})
	requiredMatch = append(requiredMatch, corev1.NodeSelectorRequirement{
		Key:      nodeArchKey,
		Operator: corev1.NodeSelectorOpIn,
		Values:   arch,
	})
	affinity = &corev1.Affinity{
		NodeAffinity: &corev1.NodeAffinity{
			RequiredDuringSchedulingIgnoredDuringExecution: &corev1.NodeSelector{
				NodeSelectorTerms: []corev1.NodeSelectorTerm{
					{
						MatchExpressions: requiredMatch,
					},
				},
			},
			PreferredDuringSchedulingIgnoredDuringExecution: preferredMatches,
		},
	}
	return affinity
}
func (a *eksAdapter) constructResourceRequirements(executable state.Executable, run state.Run, manager state.Manager, araEnabled bool) (corev1.ResourceRequirements, state.Run) {
	var ephemeralStorageRequestQuantity resource.Quantity
	maxEphemeralStorage := state.MaxEphemeralStorage
	limits := make(corev1.ResourceList)
	requests := make(corev1.ResourceList)
	cpuLimit, memLimit, cpuRequest, memRequest := a.adaptiveResources(executable, run, manager, araEnabled)
	cpuLimitQuantity := resource.MustParse(fmt.Sprintf("%dm", cpuLimit))
	cpuRequestQuantity := resource.MustParse(fmt.Sprintf("%dm", cpuRequest))
	memLimitQuantity := resource.MustParse(fmt.Sprintf("%dM", memLimit))
	memRequestQuantity := resource.MustParse(fmt.Sprintf("%dM", memRequest))
	limits[corev1.ResourceCPU] = cpuLimitQuantity
	limits[corev1.ResourceMemory] = memLimitQuantity
	requests[corev1.ResourceCPU] = cpuRequestQuantity
	requests[corev1.ResourceMemory] = memRequestQuantity
	executableResources := executable.GetExecutableResources()
	if run.Gpu != nil && *run.Gpu > 0 {
		limits["nvidia.com/gpu"] = resource.MustParse(fmt.Sprintf("%d", *run.Gpu))
		requests["nvidia.com/gpu"] = resource.MustParse(fmt.Sprintf("%d", *run.Gpu))
		run.NodeLifecycle = &state.OndemandLifecycle
	} else if executableResources.Gpu != nil && *executableResources.Gpu > 0 {
		limits["nvidia.com/gpu"] = resource.MustParse(fmt.Sprintf("%d", *executableResources.Gpu))
		requests["nvidia.com/gpu"] = resource.MustParse(fmt.Sprintf("%d", *executableResources.Gpu))
		run.NodeLifecycle = &state.OndemandLifecycle
	}
	run.Memory = aws.Int64(memRequestQuantity.ScaledValue(resource.Mega))
	run.Cpu = aws.Int64(cpuRequestQuantity.ScaledValue(resource.Milli))
	run.MemoryLimit = aws.Int64(memLimitQuantity.ScaledValue(resource.Mega))
	run.CpuLimit = aws.Int64(cpuLimitQuantity.ScaledValue(resource.Milli))
	if run.EphemeralStorage != nil {
		ephemeralStorageRequest := *run.EphemeralStorage
		if ephemeralStorageRequest > maxEphemeralStorage {
			ephemeralStorageRequest = maxEphemeralStorage
		}
		ephemeralStorageRequestQuantity = resource.MustParse(fmt.Sprintf("%dM", ephemeralStorageRequest))
		requests[corev1.ResourceEphemeralStorage] = ephemeralStorageRequestQuantity
		run.EphemeralStorage = aws.Int64(ephemeralStorageRequestQuantity.ScaledValue(resource.Mega))
	}
	resourceRequirements := corev1.ResourceRequirements{
		Limits:   limits,
		Requests: requests,
	}
	return resourceRequirements, run
}
func (a *eksAdapter) constructVolumeMounts(executable state.Executable, run state.Run, manager state.Manager, araEnabled bool) ([]corev1.VolumeMount, []corev1.Volume) {
	var mounts []corev1.VolumeMount = nil
	var volumes []corev1.Volume = nil
	if run.Gpu != nil && *run.Gpu > 0 {
		mounts = make([]corev1.VolumeMount, 1)
		mounts[0] = corev1.VolumeMount{Name: "shared-memory", MountPath: "/dev/shm"}
		volumes = make([]corev1.Volume, 1)
		sharedLimit := resource.MustParse(fmt.Sprintf("%dGi", *run.Gpu*int64(8)))
		emptyDir := corev1.EmptyDirVolumeSource{Medium: "Memory", SizeLimit: &sharedLimit}
		volumes[0] = corev1.Volume{Name: "shared-memory", VolumeSource: corev1.VolumeSource{EmptyDir: &emptyDir}}
	}
	if run.RequiresDocker {
		volumes = append(volumes, corev1.Volume{
			Name: "dockersock",
			VolumeSource: corev1.VolumeSource{
				HostPath: &corev1.HostPathVolumeSource{
					Path: "/var/run/docker.sock",
					Type: nil,
				},
			},
		})
		mounts = append(mounts, corev1.VolumeMount{
			Name:      "dockersock",
			MountPath: "/var/run/docker.sock",
		})
	}
	return mounts, volumes
}
func (a *eksAdapter) adaptiveResources(executable state.Executable, run state.Run, manager state.Manager, araEnabled bool) (int64, int64, int64, int64) {
	isGPUJob := run.Gpu != nil && *run.Gpu > 0
	cpuLimit, memLimit := a.getResourceDefaults(run, executable)
	cpuRequest, memRequest := a.getResourceDefaults(run, executable)
	if !isGPUJob {
		estimatedResources, err := manager.EstimateRunResources(*executable.GetExecutableID(), run.RunID)
		if err == nil {
			cpuRequest = estimatedResources.Cpu
			memRequest = estimatedResources.Memory
		}
		if cpuRequest > cpuLimit {
			cpuLimit = cpuRequest
		}
		if memRequest > memLimit {
			memLimit = memRequest
		}
	}
	cpuRequest, memRequest = a.checkResourceBounds(cpuRequest, memRequest, isGPUJob)
	cpuLimit, memLimit = a.checkResourceBounds(cpuLimit, memLimit, isGPUJob)
	return cpuLimit, memLimit, cpuRequest, memRequest
}
func (a *eksAdapter) checkResourceBounds(cpu int64, mem int64, isGPUJob bool) (int64, int64) {
	maxMem := state.MaxMem
	maxCPU := state.MaxCPU
	if isGPUJob {
		maxMem = state.MaxGPUMem
		maxCPU = state.MaxGPUCPU
	}
	if cpu < state.MinCPU {
		cpu = state.MinCPU
	}
	if cpu > maxCPU {
		cpu = maxCPU
	}
	if mem < state.MinMem {
		mem = state.MinMem
	}
	if mem > maxMem {
		mem = maxMem
	}
	return cpu, mem
}
func (a *eksAdapter) getResourceDefaults(run state.Run, executable state.Executable) (int64, int64) {
	cpu := state.MinCPU
	mem := state.MinMem
	executableResources := executable.GetExecutableResources()
	if run.Cpu != nil && *run.Cpu != 0 {
		cpu = *run.Cpu
	} else {
		if executableResources.Cpu != nil && *executableResources.Cpu != 0 {
			cpu = *executableResources.Cpu
		}
	}
	if run.Memory != nil && *run.Memory != 0 {
		mem = *run.Memory
	} else {
		if executableResources.Memory != nil && *executableResources.Memory != 0 {
			mem = *executableResources.Memory
		}
	}
	if mem >= 36864 && mem < 131072 && (executableResources.Gpu == nil || *executableResources.Gpu == 0) {
		cpuOverride := mem / 8
		if cpuOverride > cpu {
			cpu = cpuOverride
		}
	}
	return cpu, mem
}
func (a *eksAdapter) getLastRun(manager state.Manager, run state.Run) state.Run {
	var lastRun state.Run
	runList, err := manager.ListRuns(1, 0, "started_at", "desc", map[string][]string{
		"queued_at_since": {
			time.Now().AddDate(0, 0, -7).Format(time.RFC3339),
		},
		"status":        {state.StatusStopped},
		"command":       {strings.Replace(*run.Command, "'", "''", -1)},
		"executable_id": {*run.ExecutableID},
	}, nil, []string{state.EKSEngine})
	if err == nil && len(runList.Runs) > 0 {
		lastRun = runList.Runs[0]
	}
	return lastRun
}
func (a *eksAdapter) constructCmdSlice(cmdString string) []string {
	bashCmd := "bash"
	optLogin := "-l"
	optStr := "-cex"
	return []string{bashCmd, optLogin, optStr, cmdString}
}
func (a *eksAdapter) envOverrides(executable state.Executable, run state.Run) []corev1.EnvVar {
	pairs := make(map[string]string)
	resources := executable.GetExecutableResources()
	if resources.Env != nil && len(*resources.Env) > 0 {
		for _, ev := range *resources.Env {
			name := a.sanitizeEnvVar(ev.Name)
			value := ev.Value
			pairs[name] = value
		}
	}
	if run.Env != nil && len(*run.Env) > 0 {
		for _, ev := range *run.Env {
			name := a.sanitizeEnvVar(ev.Name)
			value := ev.Value
			pairs[name] = value
		}
	}
	var res []corev1.EnvVar
	for key := range pairs {
		if len(key) > 0 {
			res = append(res, corev1.EnvVar{
				Name:  key,
				Value: pairs[key],
			})
		}
	}
	return res
}
func (a *eksAdapter) sanitizeEnvVar(key string) string {
	if strings.HasPrefix(key, "$") {
		key = strings.Replace(key, "$", "", 1)
	}
	// Environment variable names can't contain spaces.
	key = strings.Replace(key, " ", "", -1)
	return key
}
func (a *eksAdapter) sanitizeLabel(key string) string {
	key = strings.TrimSpace(key)
	key = regexp.MustCompile(`[^-a-z0-9A-Z_.]+`).ReplaceAllString(key, "_")
	key = strings.TrimPrefix(key, "_")
	key = strings.ToLower(key)
	if len(key) > 63 {
		key = key[:63]
	}
	return key
}

================
File: execution/engine/eks_engine.go
================
package engine
import (
	"bytes"
	"context"
	"fmt"
	"strings"
	"time"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/aws/aws-sdk-go/aws/session"
	"github.com/aws/aws-sdk-go/service/s3"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/clients/metrics"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/execution/adapter"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/queue"
	"github.com/stitchfix/flotilla-os/state"
	awstrace "gopkg.in/DataDog/dd-trace-go.v1/contrib/aws/aws-sdk-go/aws"
	kubernetestrace "gopkg.in/DataDog/dd-trace-go.v1/contrib/k8s.io/client-go/kubernetes"
	v1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	k8sJson "k8s.io/apimachinery/pkg/runtime/serializer/json"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
	metricsv "k8s.io/metrics/pkg/client/clientset/versioned"
)
type EKSExecutionEngine struct {
	kClients        map[string]kubernetes.Clientset
	metricsClients  map[string]metricsv.Clientset
	adapter         adapter.EKSAdapter
	qm              queue.Manager
	log             flotillaLog.Logger
	jobQueue        string
	jobNamespace    string
	jobTtl          int
	jobSA           string
	jobARAEnabled   bool
	schedulerName   string
	serializer      *k8sJson.Serializer
	s3Client        *s3.S3
	s3Bucket        string
	s3BucketRootDir string
	statusQueue     string
	clusters        []string
}
func (ee *EKSExecutionEngine) Initialize(conf config.Config) error {
	ee.clusters = strings.Split(conf.GetString("eks_clusters"), ",")
	ee.kClients = make(map[string]kubernetes.Clientset)
	ee.metricsClients = make(map[string]metricsv.Clientset)
	for _, clusterName := range ee.clusters {
		filename := fmt.Sprintf("%s/%s", conf.GetString("eks_kubeconfig_basepath"), clusterName)
		clientConf, err := clientcmd.BuildConfigFromFlags("", filename)
		if err != nil {
			return err
		}
		clientConf.WrapTransport = kubernetestrace.WrapRoundTripper
		kClient, err := kubernetes.NewForConfig(clientConf)
		_ = ee.log.Log("message", "initializing-eks-clusters", clusterName, "filename", filename, "client", clientConf.ServerName)
		if err != nil {
			return err
		}
		ee.kClients[clusterName] = *kClient
		ee.metricsClients[clusterName] = *metricsv.NewForConfigOrDie(clientConf)
	}
	ee.jobQueue = conf.GetString("eks_job_queue")
	ee.schedulerName = "default-scheduler"
	if conf.IsSet("eks_scheduler_name") {
		ee.schedulerName = conf.GetString("eks_scheduler_name")
	}
	if conf.IsSet("eks_status_queue") {
		ee.statusQueue = conf.GetString("eks_status_queue")
	}
	ee.jobNamespace = conf.GetString("eks_job_namespace")
	ee.jobTtl = conf.GetInt("eks_job_ttl")
	ee.jobSA = conf.GetString("eks_default_service_account")
	ee.jobARAEnabled = true
	adapt, err := adapter.NewEKSAdapter()
	if err != nil {
		return err
	}
	ee.serializer = k8sJson.NewSerializerWithOptions(
		k8sJson.DefaultMetaFactory, nil, nil,
		k8sJson.SerializerOptions{
			Yaml:   true,
			Pretty: true,
			Strict: true,
		},
	)
	awsRegion := conf.GetString("eks_manifest_storage_options_region")
	awsConfig := &aws.Config{Region: aws.String(awsRegion)}
	sess := awstrace.WrapSession(session.Must(session.NewSessionWithOptions(session.Options{Config: *awsConfig})))
	sess = awstrace.WrapSession(sess)
	ee.s3Client = s3.New(sess, aws.NewConfig().WithRegion(awsRegion))
	ee.s3Bucket = conf.GetString("eks_manifest_storage_options_s3_bucket_name")
	ee.s3BucketRootDir = conf.GetString("eks_manifest_storage_options_s3_bucket_root_dir")
	ee.adapter = adapt
	fmt.Printf("EKS Engine initialized\nClusters: %s\n", ee.clusters)
	return nil
}
func (ee *EKSExecutionEngine) GetClusters() []string {
	return ee.clusters
}
func (ee *EKSExecutionEngine) Execute(executable state.Executable, run state.Run, manager state.Manager) (state.Run, bool, error) {
	ctx := context.Background()
	if run.Namespace == nil || *run.Namespace == "" {
		clusters, err := manager.ListClusterStates()
		if err == nil {
			for _, cluster := range clusters {
				if cluster.Name == run.ClusterName && cluster.Namespace != "" {
					run.Namespace = &cluster.Namespace
					break
				}
			}
		}
	}
	if run.ServiceAccount == nil {
		run.ServiceAccount = aws.String(ee.jobSA)
	}
	job, err := ee.adapter.AdaptFlotillaDefinitionAndRunToJob(executable, run, ee.schedulerName, manager, ee.jobARAEnabled)
	kClient, err := ee.getKClient(run)
	if err != nil {
		exitReason := fmt.Sprintf("Invalid cluster name - %s", run.ClusterName)
		run.ExitReason = &exitReason
		return run, false, err
	}
	result, err := kClient.BatchV1().Jobs(ee.jobNamespace).Create(ctx, &job, metav1.CreateOptions{})
	if err != nil {
		if strings.Contains(strings.ToLower(err.Error()), "already exists") {
			return run, false, nil
		}
		if strings.Contains(strings.ToLower(err.Error()), "is invalid") {
			exitReason := err.Error()
			run.ExitReason = &exitReason
			return run, false, err
		}
		_ = metrics.Increment(metrics.EngineEKSExecute, []string{string(metrics.StatusFailure)}, 1)
		return run, true, err
	}
	var b0 bytes.Buffer
	err = ee.serializer.Encode(result, &b0)
	if err == nil {
		putObject := s3.PutObjectInput{
			Bucket:      aws.String(ee.s3Bucket),
			Body:        bytes.NewReader(b0.Bytes()),
			Key:         aws.String(fmt.Sprintf("%s/%s/%s.yaml", ee.s3BucketRootDir, run.RunID, run.RunID)),
			ContentType: aws.String("text/yaml"),
		}
		_, err = ee.s3Client.PutObject(&putObject)
		if err != nil {
			_ = ee.log.Log("s3_upload_error", "error", err.Error())
		}
	}
	_ = metrics.Increment(metrics.EngineEKSExecute, []string{string(metrics.StatusSuccess)}, 1)
	run, _ = ee.getPodName(run)
	adaptedRun, err := ee.adapter.AdaptJobToFlotillaRun(result, run, nil)
	if err != nil {
		return adaptedRun, false, err
	}
	adaptedRun.Status = state.StatusRunning
	return adaptedRun, false, nil
}
func (ee *EKSExecutionEngine) getPodName(run state.Run) (state.Run, error) {
	podList, err := ee.getPodList(run)
	if err != nil {
		return run, err
	}
	if podList != nil && podList.Items != nil && len(podList.Items) > 0 {
		pod := podList.Items[len(podList.Items)-1]
		run.PodName = &pod.Name
		run.Namespace = &pod.Namespace
		if pod.Spec.Containers != nil && len(pod.Spec.Containers) > 0 {
			container := pod.Spec.Containers[len(pod.Spec.Containers)-1]
			cpu := container.Resources.Requests.Cpu().ScaledValue(resource.Milli)
			cpuLimit := container.Resources.Limits.Cpu().ScaledValue(resource.Milli)
			run.Cpu = &cpu
			run.CpuLimit = &cpuLimit
			run = ee.getInstanceDetails(pod, run)
			mem := container.Resources.Requests.Memory().ScaledValue(resource.Mega)
			run.Memory = &mem
			memLimit := container.Resources.Limits.Memory().ScaledValue(resource.Mega)
			run.MemoryLimit = &memLimit
		}
	}
	return run, nil
}
func (ee *EKSExecutionEngine) getInstanceDetails(pod v1.Pod, run state.Run) state.Run {
	if len(pod.Spec.NodeName) > 0 {
		run.InstanceDNSName = pod.Spec.NodeName
	}
	return run
}
func (ee *EKSExecutionEngine) getPodList(run state.Run) (*v1.PodList, error) {
	ctx := context.Background()
	kClient, err := ee.getKClient(run)
	if err != nil {
		return &v1.PodList{}, err
	}
	if run.PodName != nil {
		pod, err := kClient.CoreV1().Pods(ee.jobNamespace).Get(ctx, *run.PodName, metav1.GetOptions{})
		if pod != nil {
			return &v1.PodList{Items: []v1.Pod{*pod}}, err
		}
	} else {
		if run.QueuedAt == nil {
			return &v1.PodList{}, err
		}
		queuedAt := *run.QueuedAt
		if time.Now().After(queuedAt.Add(time.Minute * time.Duration(5))) {
			podList, err := kClient.CoreV1().Pods(ee.jobNamespace).List(ctx, metav1.ListOptions{
				LabelSelector: fmt.Sprintf("job-name=%s", run.RunID),
			})
			return podList, err
		}
	}
	return &v1.PodList{}, err
}
func (ee *EKSExecutionEngine) getKClient(run state.Run) (kubernetes.Clientset, error) {
	kClient, ok := ee.kClients[run.ClusterName]
	if !ok {
		return kubernetes.Clientset{}, errors.New(fmt.Sprintf("Invalid cluster name - %s", run.ClusterName))
	}
	return kClient, nil
}
func (ee *EKSExecutionEngine) Terminate(run state.Run) error {
	ctx := context.Background()
	gracePeriod := int64(300)
	deletionPropagation := metav1.DeletePropagationBackground
	_ = ee.log.Log("terminating run=", run.RunID)
	deleteOptions := &metav1.DeleteOptions{
		GracePeriodSeconds: &gracePeriod,
		PropagationPolicy:  &deletionPropagation,
	}
	kClient, err := ee.getKClient(run)
	if err != nil {
		exitReason := fmt.Sprintf(err.Error())
		run.ExitReason = &exitReason
		return err
	}
	_ = kClient.BatchV1().Jobs(ee.jobNamespace).Delete(ctx, run.RunID, *deleteOptions)
	if run.PodName != nil {
		_ = kClient.CoreV1().Pods(ee.jobNamespace).Delete(ctx, *run.PodName, *deleteOptions)
	}
	_ = metrics.Increment(metrics.EngineEKSTerminate, []string{string(metrics.StatusSuccess)}, 1)
	return nil
}
func (ee *EKSExecutionEngine) Enqueue(run state.Run) error {
	qurl, err := ee.qm.QurlFor(ee.jobQueue, false)
	if err != nil {
		_ = metrics.Increment(metrics.EngineEKSEnqueue, []string{string(metrics.StatusFailure)}, 1)
		return errors.Wrapf(err, "problem getting queue url for [%s]", run.ClusterName)
	}
	if err = ee.qm.Enqueue(qurl, run); err != nil {
		_ = metrics.Increment(metrics.EngineEKSEnqueue, []string{string(metrics.StatusFailure)}, 1)
		return errors.Wrapf(err, "problem enqueing run [%s] to queue [%s]", run.RunID, qurl)
	}
	_ = metrics.Increment(metrics.EngineEKSEnqueue, []string{string(metrics.StatusSuccess)}, 1)
	return nil
}
func (ee *EKSExecutionEngine) PollRuns() ([]RunReceipt, error) {
	qurl, err := ee.qm.QurlFor(ee.jobQueue, false)
	if err != nil {
		return nil, errors.Wrap(err, "problem listing queues to poll")
	}
	queues := []string{qurl}
	var runs []RunReceipt
	for _, qurl := range queues {
		runReceipt, err := ee.qm.ReceiveRun(qurl)
		if err != nil {
			return runs, errors.Wrapf(err, "problem receiving run from queue url [%s]", qurl)
		}
		if runReceipt.Run == nil {
			continue
		}
		runs = append(runs, RunReceipt{runReceipt})
	}
	return runs, nil
}
func (ee *EKSExecutionEngine) PollStatus() (RunReceipt, error) {
	return RunReceipt{}, nil
}
func (ee *EKSExecutionEngine) PollRunStatus() (state.Run, error) {
	return state.Run{}, nil
}
func (ee *EKSExecutionEngine) Define(td state.Definition) (state.Definition, error) {
	return td, errors.New("Definition of tasks are only for ECSs.")
}
func (ee *EKSExecutionEngine) Deregister(definition state.Definition) error {
	return errors.Errorf("EKSExecutionEngine does not allow for deregistering of task definitions.")
}
func (ee *EKSExecutionEngine) Get(run state.Run) (state.Run, error) {
	ctx := context.Background()
	kClient, err := ee.getKClient(run)
	if err != nil {
		return state.Run{}, err
	}
	job, err := kClient.BatchV1().Jobs(ee.jobNamespace).Get(ctx, run.RunID, metav1.GetOptions{})
	if err != nil {
		return state.Run{}, errors.Errorf("error getting kubernetes job %s", err)
	}
	updates, err := ee.adapter.AdaptJobToFlotillaRun(job, run, nil)
	if err != nil {
		return state.Run{}, errors.Errorf("error adapting kubernetes job to flotilla run %s", err)
	}
	return updates, nil
}
func (ee *EKSExecutionEngine) GetEvents(run state.Run) (state.PodEventList, error) {
	ctx := context.Background()
	if run.PodName == nil {
		return state.PodEventList{}, nil
	}
	kClient, err := ee.getKClient(run)
	if err != nil {
		return state.PodEventList{}, err
	}
	eventList, err := kClient.CoreV1().Events(ee.jobNamespace).List(ctx, metav1.ListOptions{FieldSelector: fmt.Sprintf("involvedObject.name==%s", *run.PodName)})
	if err != nil {
		return state.PodEventList{}, errors.Errorf("error getting kubernetes event for flotilla run %s", err)
	}
	var podEvents []state.PodEvent
	for _, e := range eventList.Items {
		eTime := e.FirstTimestamp.Time
		runEvent := state.PodEvent{
			Message:      e.Message,
			Timestamp:    &eTime,
			EventType:    e.Type,
			Reason:       e.Reason,
			SourceObject: e.ObjectMeta.Name,
		}
		if strings.Contains(e.Reason, "TriggeredScaleUp") {
			source := fmt.Sprintf("source:%s", e.ObjectMeta.Name)
			_ = metrics.Increment(metrics.EngineEKSNodeTriggeredScaledUp, []string{source}, 1)
		}
		podEvents = append(podEvents, runEvent)
	}
	podEventList := state.PodEventList{
		Total:     len(podEvents),
		PodEvents: podEvents,
	}
	return podEventList, nil
}
func (ee *EKSExecutionEngine) FetchPodMetrics(run state.Run) (state.Run, error) {
	ctx := context.Background()
	if run.PodName != nil {
		metricsClient, ok := ee.metricsClients[run.ClusterName]
		if !ok {
			return run, errors.New("Metrics client not defined.")
		}
		start := time.Now()
		podMetrics, err := metricsClient.MetricsV1beta1().PodMetricses(ee.jobNamespace).Get(ctx, *run.PodName, metav1.GetOptions{})
		_ = metrics.Timing(metrics.StatusWorkerFetchMetrics, time.Since(start), []string{run.ClusterName}, 1)
		if err != nil {
			return run, err
		}
		if len(podMetrics.Containers) > 0 {
			containerMetrics := podMetrics.Containers[0]
			mem := containerMetrics.Usage.Memory().ScaledValue(resource.Mega)
			if run.MaxMemoryUsed == nil || *run.MaxMemoryUsed == 0 || *run.MaxMemoryUsed < mem {
				run.MaxMemoryUsed = &mem
			}
			cpu := containerMetrics.Usage.Cpu().MilliValue()
			if run.MaxCpuUsed == nil || *run.MaxCpuUsed == 0 || *run.MaxCpuUsed < cpu {
				run.MaxCpuUsed = &cpu
			}
		}
		return run, nil
	}
	return run, errors.New("no pod associated with the run.")
}
func (ee *EKSExecutionEngine) FetchUpdateStatus(run state.Run) (state.Run, error) {
	ctx := context.Background()
	kClient, err := ee.getKClient(run)
	if err != nil {
		return state.Run{}, err
	}
	start := time.Now()
	job, err := kClient.BatchV1().Jobs(ee.jobNamespace).Get(ctx, run.RunID, metav1.GetOptions{})
	_ = metrics.Timing(metrics.StatusWorkerGetJob, time.Since(start), []string{run.ClusterName}, 1)
	if err != nil {
		return run, err
	}
	var mostRecentPod *v1.Pod
	var mostRecentPodCreationTimestamp metav1.Time
	start = time.Now()
	podList, err := ee.getPodList(run)
	_ = metrics.Timing(metrics.StatusWorkerGetPodList, time.Since(start), []string{run.ClusterName}, 1)
	if err == nil && podList != nil && podList.Items != nil && len(podList.Items) > 0 {
		for _, p := range podList.Items {
			if mostRecentPodCreationTimestamp.Before(&p.CreationTimestamp) || len(podList.Items) == 1 {
				mostRecentPod = &p
				mostRecentPodCreationTimestamp = p.CreationTimestamp
			}
		}
		if mostRecentPod != nil && (run.PodName == nil || mostRecentPod.Name != *run.PodName) {
			if run.PodName != nil && mostRecentPod.Name != *run.PodName {
				_ = metrics.Increment(metrics.EngineEKSRunPodnameChange, []string{}, 1)
			}
			run.PodName = &mostRecentPod.Name
			run = ee.getInstanceDetails(*mostRecentPod, run)
		}
		if mostRecentPod != nil && len(run.InstanceDNSName) == 0 {
			run = ee.getInstanceDetails(*mostRecentPod, run)
		}
		if mostRecentPod != nil && mostRecentPod.Spec.Containers != nil && len(mostRecentPod.Spec.Containers) > 0 {
			container := mostRecentPod.Spec.Containers[len(mostRecentPod.Spec.Containers)-1]
			cpu := container.Resources.Requests.Cpu().ScaledValue(resource.Milli)
			run.Cpu = &cpu
			mem := container.Resources.Requests.Memory().ScaledValue(resource.Mega)
			run.Memory = &mem
			cpuLimit := container.Resources.Limits.Cpu().ScaledValue(resource.Milli)
			run.CpuLimit = &cpuLimit
			memLimit := container.Resources.Limits.Memory().ScaledValue(resource.Mega)
			run.MemoryLimit = &memLimit
		}
	}
	hoursBack := time.Now().Add(-24 * time.Hour)
	start = time.Now()
	var events state.PodEventList
	_ = metrics.Timing(metrics.StatusWorkerGetEvents, time.Since(start), []string{run.ClusterName}, 1)
	if err == nil && len(events.PodEvents) > 0 {
		newEvents := events.PodEvents
		if run.PodEvents != nil && len(*run.PodEvents) > 0 {
			priorEvents := *run.PodEvents
			for _, newEvent := range newEvents {
				unseen := true
				for _, priorEvent := range priorEvents {
					if priorEvent.Equal(newEvent) {
						unseen = false
						break
					}
				}
				if unseen {
					priorEvents = append(priorEvents, newEvent)
				}
			}
			run.PodEvents = &priorEvents
		} else {
			run.PodEvents = &newEvents
		}
	}
	if run.PodEvents != nil {
		attemptCount := int64(0)
		for _, podEvent := range *run.PodEvents {
			if strings.Contains(podEvent.Reason, "Scheduled") {
				attemptCount = attemptCount + 1
			}
		}
		run.AttemptCount = &attemptCount
	}
	if err == nil && podList != nil && podList.Items != nil && len(podList.Items) == 0 && run.PodName != nil && run.QueuedAt.Before(hoursBack) {
		err = ee.Terminate(run)
		if err == nil {
			job.Status.Failed = 1
			mostRecentPod = nil
		}
	}
	return ee.adapter.AdaptJobToFlotillaRun(job, run, mostRecentPod)
}

================
File: execution/engine/emr_engine.go
================
package engine
import (
	"bytes"
	"encoding/json"
	"fmt"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/aws/aws-sdk-go/aws/session"
	"github.com/aws/aws-sdk-go/service/emrcontainers"
	"github.com/aws/aws-sdk-go/service/s3"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/clients/metrics"
	"github.com/stitchfix/flotilla-os/config"
	utils "github.com/stitchfix/flotilla-os/execution"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/queue"
	"github.com/stitchfix/flotilla-os/state"
	awstrace "gopkg.in/DataDog/dd-trace-go.v1/contrib/aws/aws-sdk-go/aws"
	v1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	_ "k8s.io/apimachinery/pkg/apis/meta/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	k8sJson "k8s.io/apimachinery/pkg/runtime/serializer/json"
	"k8s.io/client-go/kubernetes"
	_ "k8s.io/client-go/kubernetes/scheme"
	"k8s.io/client-go/tools/clientcmd"
	"regexp"
	"strings"
)
type EMRExecutionEngine struct {
	sqsQueueManager     queue.Manager
	log                 flotillaLog.Logger
	emrJobQueue         string
	emrJobNamespace     string
	emrJobRoleArn       map[string]string
	emrJobSA            string
	emrVirtualClusters  map[string]string
	emrContainersClient *emrcontainers.EMRContainers
	schedulerName       string
	s3Client            *s3.S3
	awsRegion           string
	s3LogsBucket        string
	s3EventLogPath      string
	s3LogsBasePath      string
	s3ManifestBucket    string
	s3ManifestBasePath  string
	serializer          *k8sJson.Serializer
	clusters            []string
	driverInstanceType  string
	kClients            map[string]kubernetes.Clientset
}
func (emr *EMRExecutionEngine) Initialize(conf config.Config) error {
	emr.emrVirtualClusters = make(map[string]string)
	emr.emrVirtualClusters = conf.GetStringMapString("emr_virtual_clusters")
	emr.emrJobQueue = conf.GetString("emr_job_queue")
	emr.emrJobNamespace = conf.GetString("emr_job_namespace")
	emr.emrJobRoleArn = conf.GetStringMapString("emr_job_role_arn")
	emr.awsRegion = conf.GetString("emr_aws_region")
	emr.s3LogsBucket = conf.GetString("emr_log_bucket")
	emr.s3LogsBasePath = conf.GetString("emr_log_base_path")
	emr.s3EventLogPath = conf.GetString("emr_log_event_log_path")
	emr.s3ManifestBucket = conf.GetString("emr_manifest_bucket")
	emr.s3ManifestBasePath = conf.GetString("emr_manifest_base_path")
	emr.emrJobSA = conf.GetString("emr_default_service_account")
	emr.schedulerName = conf.GetString("eks_scheduler_name")
	emr.driverInstanceType = conf.GetString("emr_driver_instance_type")
	awsConfig := &aws.Config{Region: aws.String(emr.awsRegion)}
	sess := session.Must(session.NewSessionWithOptions(session.Options{Config: *awsConfig}))
	sess = awstrace.WrapSession(sess)
	emr.s3Client = s3.New(sess, aws.NewConfig().WithRegion(emr.awsRegion))
	emr.emrContainersClient = emrcontainers.New(sess, aws.NewConfig().WithRegion(emr.awsRegion))
	emr.serializer = k8sJson.NewSerializerWithOptions(
		k8sJson.SimpleMetaFactory{}, nil, nil,
		k8sJson.SerializerOptions{
			Yaml:   true,
			Pretty: true,
			Strict: true,
		},
	)
	emr.clusters = strings.Split(conf.GetString("eks_clusters"), ",")
	emr.kClients = make(map[string]kubernetes.Clientset)
	for _, clusterName := range emr.clusters {
		filename := fmt.Sprintf("%s/%s", conf.GetString("eks_kubeconfig_basepath"), clusterName)
		clientConf, err := clientcmd.BuildConfigFromFlags("", filename)
		if err != nil {
			return fmt.Errorf("error building kubeconfig: %v", err)
		}
		kClient, err := kubernetes.NewForConfig(clientConf)
		_ = emr.log.Log("message", "initializing-eks-clusters", clusterName, "filename", filename, "client", clientConf.ServerName)
		if err != nil {
			return fmt.Errorf("error creating kubernetes client: %v", err)
		}
		emr.kClients[clusterName] = *kClient
	}
	fmt.Printf("EMR engine initialized\nVirtual Clusters: %v\nJobRoles: %v\n", emr.emrVirtualClusters, emr.emrJobRoleArn)
	return nil
}
func (emr *EMRExecutionEngine) GetClusters() []string {
	var clusters []string
	for k, v := range emr.emrVirtualClusters {
		if v != "" {
			clusters = append(clusters, k)
		}
	}
	return clusters
}
func (emr *EMRExecutionEngine) getKClient(run state.Run) (kubernetes.Clientset, error) {
	kClient, ok := emr.kClients[run.ClusterName]
	if !ok {
		return kubernetes.Clientset{}, errors.New(fmt.Sprintf("Invalid cluster name - %s", run.ClusterName))
	}
	return kClient, nil
}
func (emr *EMRExecutionEngine) Execute(executable state.Executable, run state.Run, manager state.Manager) (state.Run, bool, error) {
	run = emr.estimateExecutorCount(run, manager)
	run = emr.estimateMemoryResources(run, manager)
	if run.ServiceAccount == nil || *run.ServiceAccount == "" {
		run.ServiceAccount = aws.String(emr.emrJobSA)
	}
	if run.CommandHash != nil && run.NodeLifecycle != nil && *run.NodeLifecycle == state.SpotLifecycle {
		nodeType, err := manager.GetNodeLifecycle(run.DefinitionID, *run.CommandHash)
		if err == nil && nodeType == state.OndemandLifecycle {
			run.NodeLifecycle = &state.OndemandLifecycle
		}
	}
	startJobRunInput := emr.generateEMRStartJobRunInput(executable, run, manager)
	emrJobManifest := aws.String(fmt.Sprintf("%s/%s/%s.json", emr.s3ManifestBasePath, run.RunID, "start-job-run-input"))
	obj, err := json.MarshalIndent(startJobRunInput, "", "\t")
	if err == nil {
		emrJobManifest = emr.writeStringToS3(emrJobManifest, obj)
	}
	emr.log.Log("message", "Start EMR JobRun", "ExecutionRoleArn", startJobRunInput.ExecutionRoleArn)
	startJobRunOutput, err := emr.emrContainersClient.StartJobRun(&startJobRunInput)
	if err == nil {
		run.SparkExtension.VirtualClusterId = startJobRunOutput.VirtualClusterId
		run.SparkExtension.EMRJobId = startJobRunOutput.Id
		run.SparkExtension.EMRJobManifest = emrJobManifest
		run.Status = state.StatusQueued
		_ = metrics.Increment(metrics.EngineEMRExecute, []string{string(metrics.StatusSuccess)}, 1)
	} else {
		run.ExitReason = aws.String(fmt.Sprintf("%v", err))
		run.ExitCode = aws.Int64(-1)
		run.StartedAt = run.QueuedAt
		run.FinishedAt = run.QueuedAt
		run.Status = state.StatusStopped
		_ = emr.log.Log("EMR job submission error", "error", err.Error())
		_ = metrics.Increment(metrics.EngineEKSExecute, []string{string(metrics.StatusFailure)}, 1)
		return run, false, err
	}
	return run, false, nil
}
func (emr *EMRExecutionEngine) generateApplicationConf(executable state.Executable, run state.Run, manager state.Manager) []*emrcontainers.Configuration {
	sparkDefaults := map[string]*string{
		"spark.kubernetes.driver.podTemplateFile":   emr.driverPodTemplate(executable, run, manager),
		"spark.kubernetes.executor.podTemplateFile": emr.executorPodTemplate(executable, run, manager),
		"spark.kubernetes.container.image":          &run.Image,
		"spark.eventLog.dir":                        aws.String(fmt.Sprintf("s3://%s/%s", emr.s3LogsBucket, emr.s3EventLogPath)),
		"spark.history.fs.logDirectory":             aws.String(fmt.Sprintf("s3://%s/%s", emr.s3LogsBucket, emr.s3EventLogPath)),
		"spark.eventLog.enabled":                    aws.String("true"),
		"spark.default.parallelism":                 aws.String("256"),
		"spark.sql.shuffle.partitions":              aws.String("256"),
		"spark.metrics.conf.*.sink.prometheusServlet.class": aws.String("org.apache.spark.metrics.sink.PrometheusServlet"),
		"spark.metrics.conf.*.sink.prometheusServlet.path":  aws.String("/metrics/driver/prometheus"),
		"master.sink.prometheusServlet.path":                aws.String("/metrics/master/prometheus"),
		"applications.sink.prometheusServlet.path":          aws.String("/metrics/applications/prometheus"),
		"spark.kubernetes.driver.service.annotation.prometheus.io/port":   aws.String("4040"),
		"spark.kubernetes.driver.service.annotation.prometheus.io/path":   aws.String("/metrics/driver/prometheus/"),
		"spark.kubernetes.driver.service.annotation.prometheus.io/scrape": aws.String("true"),
		"spark.kubernetes.driver.annotation.prometheus.io/scrape": aws.String("true"),
		"spark.kubernetes.driver.annotation.prometheus.io/path":   aws.String("/metrics/executors/prometheus/"),
		"spark.kubernetes.driver.annotation.prometheus.io/port":   aws.String("4040"),
		"spark.ui.prometheus.enabled":                             aws.String("true"),
	}
	hiveDefaults := map[string]*string{}
	for _, k := range run.SparkExtension.ApplicationConf {
		sparkDefaults[*k.Name] = k.Value
	}
	if run.SparkExtension.HiveConf != nil {
		for _, k := range run.SparkExtension.HiveConf {
			if k.Name != nil && k.Value != nil {
				hiveDefaults[*k.Name] = k.Value
			}
		}
	}
	return []*emrcontainers.Configuration{
		{
			Classification: aws.String("spark-defaults"),
			Properties:     sparkDefaults,
		},
		{
			Classification: aws.String("spark-hive-site"),
			Properties:     hiveDefaults,
		},
	}
}
func (emr *EMRExecutionEngine) generateEMRStartJobRunInput(executable state.Executable, run state.Run, manager state.Manager) emrcontainers.StartJobRunInput {
	roleArn := emr.emrJobRoleArn[*run.ServiceAccount]
	clusterID := emr.emrVirtualClusters[run.ClusterName]
	startJobRunInput := emrcontainers.StartJobRunInput{
		ClientToken: &run.RunID,
		ConfigurationOverrides: &emrcontainers.ConfigurationOverrides{
			MonitoringConfiguration: &emrcontainers.MonitoringConfiguration{
				PersistentAppUI: aws.String(emrcontainers.PersistentAppUIEnabled),
				S3MonitoringConfiguration: &emrcontainers.S3MonitoringConfiguration{
					LogUri: aws.String(fmt.Sprintf("s3://%s/%s", emr.s3LogsBucket, emr.s3LogsBasePath)),
				},
			},
			ApplicationConfiguration: emr.generateApplicationConf(executable, run, manager),
		},
		ExecutionRoleArn: &roleArn,
		JobDriver: &emrcontainers.JobDriver{
			SparkSubmitJobDriver: &emrcontainers.SparkSubmitJobDriver{
				EntryPoint:            run.SparkExtension.SparkSubmitJobDriver.EntryPoint,
				EntryPointArguments:   run.SparkExtension.SparkSubmitJobDriver.EntryPointArguments,
				SparkSubmitParameters: emr.sparkSubmitParams(run),
			}},
		Name:             &run.RunID,
		ReleaseLabel:     run.SparkExtension.EMRReleaseLabel,
		VirtualClusterId: &clusterID,
	}
	return startJobRunInput
}
func (emr *EMRExecutionEngine) generateTags(run state.Run) map[string]*string {
	tags := make(map[string]*string)
	if run.Env != nil && len(*run.Env) > 0 {
		for _, ev := range *run.Env {
			name := emr.sanitizeEnvVar(ev.Name)
			space := regexp.MustCompile(`\s+`)
			if len(ev.Value) < 256 && len(name) < 128 {
				tags[name] = aws.String(space.ReplaceAllString(ev.Value, ""))
			}
		}
	}
	return tags
}
// generates volumes and volumemounts depending on cluster name.
// TODO cleanup after migration
func generateVolumesForCluster(clusterName string, isEmptyDir bool) ([]v1.Volume, []v1.VolumeMount) {
	var volumes []v1.Volume
	var volumeMounts []v1.VolumeMount
	if isEmptyDir {
		// Use a emptyDir volume
		specificVolume := v1.Volume{
			Name: "shared-lib-volume",
			VolumeSource: v1.VolumeSource{
				EmptyDir: &(v1.EmptyDirVolumeSource{}),
			},
		}
		volumes = append(volumes, specificVolume)
	} else {
		sharedLibVolume := v1.Volume{
			Name: "shared-lib-volume",
			VolumeSource: v1.VolumeSource{
				PersistentVolumeClaim: &v1.PersistentVolumeClaimVolumeSource{
					ClaimName: "s3-claim",
				},
			},
		}
		volumes = append(volumes, sharedLibVolume)
	}
	volumeMount := v1.VolumeMount{
		Name:      "shared-lib-volume",
		MountPath: "/var/lib/app",
	}
	volumeMounts = append(volumeMounts, volumeMount)
	return volumes, volumeMounts
}
func (emr *EMRExecutionEngine) driverPodTemplate(executable state.Executable, run state.Run, manager state.Manager) *string {
	run.NodeLifecycle = &state.OndemandLifecycle
	workingDir := "/var/lib/app"
	if run.SparkExtension != nil && run.SparkExtension.SparkSubmitJobDriver != nil && run.SparkExtension.SparkSubmitJobDriver.WorkingDir != nil {
		workingDir = *run.SparkExtension.SparkSubmitJobDriver.WorkingDir
	}
	volumes, volumeMounts := generateVolumesForCluster(run.ClusterName, true)
	podSpec := v1.PodSpec{
		TerminationGracePeriodSeconds: aws.Int64(90),
		Volumes:                       volumes,
		SchedulerName:                 emr.schedulerName,
		Containers: []v1.Container{
			{
				Name:         "spark-kubernetes-driver",
				Env:          emr.envOverrides(executable, run),
				VolumeMounts: volumeMounts,
				WorkingDir:   workingDir,
			},
		},
		InitContainers: []v1.Container{{
			Name:         fmt.Sprintf("init-driver-%s", run.RunID),
			Image:        run.Image,
			Env:          emr.envOverrides(executable, run),
			VolumeMounts: volumeMounts,
			Command:      emr.constructCmdSlice(run.SparkExtension.DriverInitCommand),
		}},
		RestartPolicy: v1.RestartPolicyNever,
		Affinity:      emr.constructAffinity(executable, run, manager, true),
		Tolerations:   emr.constructTolerations(executable, run),
	}
	if emr.driverInstanceType != "" {
		podSpec.NodeSelector = map[string]string{
			"node.kubernetes.io/instance-type": emr.driverInstanceType,
		}
	}
	labels := utils.GetLabels(run)
	pod := v1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Annotations: map[string]string{
				"karpenter.sh/do-not-evict": "true",
				"flotilla-run-id":           run.RunID,
			},
			Labels: labels,
		},
		Spec: podSpec,
	}
	key := aws.String(fmt.Sprintf("%s/%s/%s.yaml", emr.s3ManifestBasePath, run.RunID, "driver-template"))
	return emr.writeK8ObjToS3(&pod, key)
}
func (emr *EMRExecutionEngine) executorPodTemplate(executable state.Executable, run state.Run, manager state.Manager) *string {
	workingDir := "/var/lib/app"
	if run.SparkExtension != nil && run.SparkExtension.SparkSubmitJobDriver != nil && run.SparkExtension.SparkSubmitJobDriver.WorkingDir != nil {
		workingDir = *run.SparkExtension.SparkSubmitJobDriver.WorkingDir
	}
	labels := utils.GetLabels(run)
	volumes, volumeMounts := generateVolumesForCluster(run.ClusterName, true)
	pod := v1.Pod{
		Status: v1.PodStatus{},
		ObjectMeta: metav1.ObjectMeta{
			Annotations: map[string]string{
				"karpenter.sh/do-not-evict": "true",
				"flotilla-run-id":           run.RunID},
			Labels: labels,
		},
		Spec: v1.PodSpec{
			TerminationGracePeriodSeconds: aws.Int64(90),
			Volumes:                       volumes,
			SchedulerName:                 emr.schedulerName,
			Containers: []v1.Container{
				{
					Name:         "spark-kubernetes-executor",
					Env:          emr.envOverrides(executable, run),
					VolumeMounts: volumeMounts,
					WorkingDir:   workingDir,
				},
			},
			InitContainers: []v1.Container{{
				Name:         fmt.Sprintf("init-executor-%s", run.RunID),
				Image:        run.Image,
				Env:          emr.envOverrides(executable, run),
				VolumeMounts: volumeMounts,
				Command:      emr.constructCmdSlice(run.SparkExtension.ExecutorInitCommand),
			}},
			RestartPolicy: v1.RestartPolicyNever,
			Affinity:      emr.constructAffinity(executable, run, manager, false),
			Tolerations:   emr.constructTolerations(executable, run),
		},
	}
	key := aws.String(fmt.Sprintf("%s/%s/%s.yaml", emr.s3ManifestBasePath, run.RunID, "executor-template"))
	return emr.writeK8ObjToS3(&pod, key)
}
func (emr *EMRExecutionEngine) writeK8ObjToS3(obj runtime.Object, key *string) *string {
	var b0 bytes.Buffer
	err := emr.serializer.Encode(obj, &b0)
	payload := bytes.ReplaceAll(b0.Bytes(), []byte("status: {}"), []byte(""))
	payload = bytes.ReplaceAll(payload, []byte("creationTimestamp: null"), []byte(""))
	payload = bytes.ReplaceAll(payload, []byte("resources: {}"), []byte(""))
	if err == nil {
		putObject := s3.PutObjectInput{
			Bucket:      aws.String(emr.s3ManifestBucket),
			Body:        bytes.NewReader(payload),
			Key:         key,
			ContentType: aws.String("text/yaml"),
		}
		_, err = emr.s3Client.PutObject(&putObject)
		if err != nil {
			_ = emr.log.Log("s3_upload_error", "error", err.Error())
		}
	}
	return aws.String(fmt.Sprintf("s3://%s/%s", emr.s3ManifestBucket, *key))
}
func (emr *EMRExecutionEngine) writeStringToS3(key *string, body []byte) *string {
	if body != nil && key != nil {
		putObject := s3.PutObjectInput{
			Bucket:      aws.String(emr.s3ManifestBucket),
			Body:        bytes.NewReader(body),
			Key:         key,
			ContentType: aws.String("text/yaml"),
		}
		_, err := emr.s3Client.PutObject(&putObject)
		if err != nil {
			_ = emr.log.Log("s3_upload_error", "error", err.Error())
		}
	}
	return aws.String(fmt.Sprintf("s3://%s/%s", emr.s3ManifestBucket, *key))
}
func (emr *EMRExecutionEngine) constructEviction(run state.Run, manager state.Manager) string {
	if run.NodeLifecycle != nil && *run.NodeLifecycle == state.OndemandLifecycle {
		return "false"
	}
	if run.CommandHash != nil {
		nodeType, err := manager.GetNodeLifecycle(run.DefinitionID, *run.CommandHash)
		if err == nil && nodeType == state.OndemandLifecycle {
			return "false"
		}
	}
	return "true"
}
func (emr *EMRExecutionEngine) constructTolerations(executable state.Executable, run state.Run) []v1.Toleration {
	tolerations := []v1.Toleration{}
	tolerations = append(tolerations, v1.Toleration{
		Key:      "emr",
		Operator: "Equal",
		Value:    "true",
		Effect:   "NoSchedule",
	})
	return tolerations
}
func (emr *EMRExecutionEngine) constructAffinity(executable state.Executable, run state.Run, manager state.Manager, driver bool) *v1.Affinity {
	affinity := &v1.Affinity{}
	var requiredMatch []v1.NodeSelectorRequirement
	nodeLifecycleKey := "karpenter.sh/capacity-type"
	nodeArchKey := "kubernetes.io/arch"
	newCluster := true
	arch := []string{"amd64"}
	if run.Arch != nil && *run.Arch == "arm64" {
		arch = []string{"arm64"}
	}
	var nodeLifecycle []string
	nodePreference := "spot"
	if (run.NodeLifecycle != nil && *run.NodeLifecycle == state.OndemandLifecycle) || driver {
		nodeLifecycle = append(nodeLifecycle, "on-demand")
		nodePreference = "on-demand"
	} else {
		nodeLifecycle = append(nodeLifecycle, "spot", "on-demand")
	}
	if run.CommandHash != nil {
		nodeType, err := manager.GetNodeLifecycle(run.DefinitionID, *run.CommandHash)
		if err == nil && nodeType == state.OndemandLifecycle {
			nodeLifecycle = []string{"on-demand"}
		}
	}
	requiredMatch = append(requiredMatch, v1.NodeSelectorRequirement{
		Key:      nodeLifecycleKey,
		Operator: v1.NodeSelectorOpIn,
		Values:   nodeLifecycle,
	})
	requiredMatch = append(requiredMatch, v1.NodeSelectorRequirement{
		Key:      nodeArchKey,
		Operator: v1.NodeSelectorOpIn,
		Values:   arch,
	})
	if newCluster {
		requiredMatch = append(requiredMatch, v1.NodeSelectorRequirement{
			Key:      "emr",
			Operator: v1.NodeSelectorOpIn,
			Values:   []string{"true"},
		})
	}
	affinity = &v1.Affinity{
		NodeAffinity: &v1.NodeAffinity{
			RequiredDuringSchedulingIgnoredDuringExecution: &v1.NodeSelector{
				NodeSelectorTerms: []v1.NodeSelectorTerm{
					{
						MatchExpressions: requiredMatch,
					},
				},
			},
			PreferredDuringSchedulingIgnoredDuringExecution: []v1.PreferredSchedulingTerm{{
				Weight: 50,
				Preference: v1.NodeSelectorTerm{
					MatchExpressions: []v1.NodeSelectorRequirement{{
						Key:      nodeLifecycleKey,
						Operator: v1.NodeSelectorOpIn,
						Values:   []string{nodePreference},
					}},
				},
			}},
		},
		PodAffinity: &v1.PodAffinity{
			PreferredDuringSchedulingIgnoredDuringExecution: []v1.WeightedPodAffinityTerm{
				{
					Weight: 40,
					PodAffinityTerm: v1.PodAffinityTerm{
						LabelSelector: &metav1.LabelSelector{
							MatchLabels: map[string]string{
								"flotilla-run-id": run.RunID},
						},
						TopologyKey: "topology.kubernetes.io/zone",
					},
				},
			},
		},
	}
	return affinity
}
func (emr *EMRExecutionEngine) estimateExecutorCount(run state.Run, manager state.Manager) state.Run {
	return run
}
func setResourceSuffix(value string) string {
	if strings.Contains(value, "g") || strings.Contains(value, "m") {
		return strings.ToUpper(value)
	}
	if strings.Contains(value, "K") {
		return strings.ToLower(value)
	}
	return value
}
func (emr *EMRExecutionEngine) estimateMemoryResources(run state.Run, manager state.Manager) state.Run {
	if run.CommandHash == nil {
		return run
	}
	executorOOM, _ := manager.ExecutorOOM(run.DefinitionID, *run.CommandHash)
	driverOOM, _ := manager.DriverOOM(run.DefinitionID, *run.CommandHash)
	var sparkSubmitConf []state.Conf
	for _, k := range run.SparkExtension.SparkSubmitJobDriver.SparkSubmitConf {
		if *k.Name == "spark.executor.memory" && k.Value != nil {
			if executorOOM {
				quantity := resource.MustParse(setResourceSuffix(*k.Value))
				quantity.Set(int64(float64(quantity.Value()) * 1.25))
				k.Value = aws.String(strings.ToLower(quantity.String()))
			} else {
				quantity := resource.MustParse(setResourceSuffix(*k.Value))
				minVal := resource.MustParse("1G")
				if quantity.MilliValue() > minVal.MilliValue() {
					quantity.Set(int64(float64(quantity.Value()) * 1.0))
					k.Value = aws.String(strings.ToLower(quantity.String()))
				}
			}
		}
		if driverOOM {
			if *k.Name == "spark.driver.memory" && k.Value != nil {
				quantity := resource.MustParse(setResourceSuffix(*k.Value))
				quantity.Set(quantity.Value() * 3)
				k.Value = aws.String(strings.ToLower(quantity.String()))
			}
		}
		sparkSubmitConf = append(sparkSubmitConf, state.Conf{Name: k.Name, Value: k.Value})
	}
	run.SparkExtension.SparkSubmitJobDriver.SparkSubmitConf = sparkSubmitConf
	return run
}
func (emr *EMRExecutionEngine) sparkSubmitParams(run state.Run) *string {
	var buffer bytes.Buffer
	buffer.WriteString(fmt.Sprintf(" --name %s", run.RunID))
	for _, k := range run.SparkExtension.SparkSubmitJobDriver.SparkSubmitConf {
		buffer.WriteString(fmt.Sprintf(" --conf %s=%s", *k.Name, *k.Value))
	}
	buffer.WriteString(fmt.Sprintf(" --conf %s=%s", "spark.kubernetes.executor.podNamePrefix", run.RunID))
	buffer.WriteString(fmt.Sprintf(" --conf spark.log4j.rootLogger=DEBUG"))
	buffer.WriteString(fmt.Sprintf(" --conf spark.log4j.rootCategory=DEBUG"))
	if run.SparkExtension.SparkSubmitJobDriver.Class != nil {
		buffer.WriteString(fmt.Sprintf(" --class %s", *run.SparkExtension.SparkSubmitJobDriver.Class))
	}
	if len(run.SparkExtension.SparkSubmitJobDriver.Files) > 0 {
		files := strings.Join(run.SparkExtension.SparkSubmitJobDriver.Files, ",")
		buffer.WriteString(fmt.Sprintf(" --files %s", files))
	}
	if len(run.SparkExtension.SparkSubmitJobDriver.PyFiles) > 0 {
		files := strings.Join(run.SparkExtension.SparkSubmitJobDriver.PyFiles, ",")
		buffer.WriteString(fmt.Sprintf(" --py-files %s", files))
	}
	if len(run.SparkExtension.SparkSubmitJobDriver.Jars) > 0 {
		jars := strings.Join(run.SparkExtension.SparkSubmitJobDriver.Jars, ",")
		buffer.WriteString(fmt.Sprintf(" --jars %s", jars))
	}
	return aws.String(buffer.String())
}
func (emr *EMRExecutionEngine) Terminate(run state.Run) error {
	if run.Status == state.StatusStopped {
		return errors.New("Run is already in a stopped state.")
	}
	cancelJobRunInput := emrcontainers.CancelJobRunInput{
		Id:               run.SparkExtension.EMRJobId,
		VirtualClusterId: run.SparkExtension.VirtualClusterId,
	}
	key := aws.String(fmt.Sprintf("%s/%s/%s.json", emr.s3ManifestBasePath, run.RunID, "cancel-job-run-input"))
	obj, err := json.Marshal(cancelJobRunInput)
	if err == nil {
		emr.writeStringToS3(key, obj)
	}
	_, err = emr.emrContainersClient.CancelJobRun(&cancelJobRunInput)
	if err != nil {
		_ = metrics.Increment(metrics.EngineEMRTerminate, []string{string(metrics.StatusFailure)}, 1)
		_ = emr.log.Log("EMR job termination error", "error", err.Error())
	}
	_ = metrics.Increment(metrics.EngineEMRTerminate, []string{string(metrics.StatusSuccess)}, 1)
	return err
}
func (emr *EMRExecutionEngine) Enqueue(run state.Run) error {
	qurl, err := emr.sqsQueueManager.QurlFor(emr.emrJobQueue, false)
	if err != nil {
		_ = metrics.Increment(metrics.EngineEMREnqueue, []string{string(metrics.StatusFailure)}, 1)
		_ = emr.log.Log("EMR job enqueue error", "error", err.Error())
		return errors.Wrapf(err, "problem getting queue url for [%s]", run.ClusterName)
	}
	if err = emr.sqsQueueManager.Enqueue(qurl, run); err != nil {
		_ = metrics.Increment(metrics.EngineEMREnqueue, []string{string(metrics.StatusFailure)}, 1)
		_ = emr.log.Log("EMR job enqueue error", "error", err.Error())
		return errors.Wrapf(err, "problem enqueing run [%s] to queue [%s]", run.RunID, qurl)
	}
	_ = metrics.Increment(metrics.EngineEMREnqueue, []string{string(metrics.StatusSuccess)}, 1)
	return nil
}
func (emr *EMRExecutionEngine) PollRuns() ([]RunReceipt, error) {
	qurl, err := emr.sqsQueueManager.QurlFor(emr.emrJobQueue, false)
	if err != nil {
		return nil, errors.Wrap(err, "problem listing queues to poll")
	}
	queues := []string{qurl}
	var runs []RunReceipt
	for _, qurl := range queues {
		runReceipt, err := emr.sqsQueueManager.ReceiveRun(qurl)
		if err != nil {
			return runs, errors.Wrapf(err, "problem receiving run from queue url [%s]", qurl)
		}
		if runReceipt.Run == nil {
			continue
		}
		runs = append(runs, RunReceipt{runReceipt})
	}
	return runs, nil
}
func (emr *EMRExecutionEngine) PollStatus() (RunReceipt, error) {
	return RunReceipt{}, nil
}
func (emr *EMRExecutionEngine) PollRunStatus() (state.Run, error) {
	return state.Run{}, nil
}
func (emr *EMRExecutionEngine) Define(td state.Definition) (state.Definition, error) {
	return td, nil
}
func (emr *EMRExecutionEngine) Deregister(definition state.Definition) error {
	return errors.Errorf("EMRExecutionEngine does not allow for deregistering of task definitions.")
}
func (emr *EMRExecutionEngine) Get(run state.Run) (state.Run, error) {
	return run, nil
}
func (emr *EMRExecutionEngine) GetEvents(run state.Run) (state.PodEventList, error) {
	return state.PodEventList{}, nil
}
func (emr *EMRExecutionEngine) FetchPodMetrics(run state.Run) (state.Run, error) {
	return run, nil
}
func (emr *EMRExecutionEngine) FetchUpdateStatus(run state.Run) (state.Run, error) {
	return run, nil
}
func (emr *EMRExecutionEngine) envOverrides(executable state.Executable, run state.Run) []v1.EnvVar {
	pairs := make(map[string]string)
	resources := executable.GetExecutableResources()
	if resources.Env != nil && len(*resources.Env) > 0 {
		for _, ev := range *resources.Env {
			name := emr.sanitizeEnvVar(ev.Name)
			value := ev.Value
			pairs[name] = value
		}
	}
	if run.Env != nil && len(*run.Env) > 0 {
		for _, ev := range *run.Env {
			name := emr.sanitizeEnvVar(ev.Name)
			value := ev.Value
			pairs[name] = value
		}
	}
	var res []v1.EnvVar
	for key := range pairs {
		if len(key) > 0 {
			res = append(res, v1.EnvVar{
				Name:  key,
				Value: pairs[key],
			})
		}
	}
	res = append(res, v1.EnvVar{
		Name: "SPARK_APPLICATION_ID",
		ValueFrom: &v1.EnvVarSource{
			FieldRef: &v1.ObjectFieldSelector{
				APIVersion: "v1",
				FieldPath:  "metadata.labels['spark-app-selector']",
			},
		},
	})
	return res
}
func (emr *EMRExecutionEngine) sanitizeEnvVar(key string) string {
	if strings.HasPrefix(key, "$") {
		key = strings.Replace(key, "$", "", 1)
	}
	// Environment variable names can't contain spaces.
	key = strings.Replace(key, " ", "", -1)
	return key
}
func (emr *EMRExecutionEngine) constructCmdSlice(command *string) []string {
	cmdString := ""
	if command != nil {
		cmdString = *command
	}
	bashCmd := "bash"
	optLogin := "-l"
	optStr := "-ce"
	return []string{bashCmd, optLogin, optStr, cmdString}
}

================
File: execution/engine/engine.go
================
package engine
import (
	"fmt"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/queue"
	"github.com/stitchfix/flotilla-os/state"
)
type Engine interface {
	Initialize(conf config.Config) error
	Execute(executable state.Executable, run state.Run, manager state.Manager) (state.Run, bool, error)
	Terminate(run state.Run) error
	Enqueue(run state.Run) error
	PollRuns() ([]RunReceipt, error)
	PollRunStatus() (state.Run, error)
	PollStatus() (RunReceipt, error)
	GetEvents(run state.Run) (state.PodEventList, error)
	FetchUpdateStatus(run state.Run) (state.Run, error)
	FetchPodMetrics(run state.Run) (state.Run, error)
	Define(definition state.Definition) (state.Definition, error)
	Deregister(definition state.Definition) error
}
type RunReceipt struct {
	queue.RunReceipt
}
func NewExecutionEngine(conf config.Config, qm queue.Manager, name string, logger log.Logger) (Engine, error) {
	switch name {
	case state.EKSEngine:
		eksEng := &EKSExecutionEngine{qm: qm, log: logger}
		if err := eksEng.Initialize(conf); err != nil {
			return nil, errors.Wrap(err, "problem initializing EKSExecutionEngine")
		}
		return eksEng, nil
	case state.EKSSparkEngine:
		emrEng := &EMRExecutionEngine{sqsQueueManager: qm, log: logger}
		if err := emrEng.Initialize(conf); err != nil {
			return nil, errors.Wrap(err, "problem initializing EMRExecutionEngine")
		}
		return emrEng, nil
	default:
		return nil, fmt.Errorf("no Engine named [%s] was found", name)
	}
}

================
File: execution/utils.go
================
package utils
import (
	"github.com/stitchfix/flotilla-os/state"
	"os"
	"regexp"
	"strings"
)
func GetLabels(run state.Run) map[string]string {
	var labels = make(map[string]string)
	if run.ClusterName != "" {
		labels["cluster-name"] = run.ClusterName
	}
	if run.RunID != "" {
		labels["flotilla-run-id"] = SanitizeLabel(run.RunID)
		labels["flotilla-run-mode"] = SanitizeLabel(os.Getenv("FLOTILLA_MODE"))
	}
	if run.User != "" {
		labels["owner"] = SanitizeLabel(run.User)
	}
	if _, workflowExists := run.Labels["kube_workflow"]; !workflowExists {
		if _, taskNameExists := run.Labels["kube_task_name"]; taskNameExists {
			labels["kube_workflow"] = SanitizeLabel(run.Labels["kube_task_name"])
		}
	}
	for k, v := range run.Labels {
		labels[k] = SanitizeLabel(v)
	}
	return labels
}
func SanitizeLabel(key string) string {
	key = strings.TrimSpace(key)
	key = regexp.MustCompile(`[^-a-z0-9A-Z_.]+`).ReplaceAllString(key, "_")
	key = strings.TrimPrefix(key, "_")
	key = strings.ToLower(key)
	if len(key) > 63 {
		key = key[:63]
	}
	for {
		tempKey := strings.TrimSuffix(key, "_")
		if tempKey == key {
			break
		}
		key = tempKey
	}
	return key
}

================
File: flotilla/app.go
================
package flotilla
import (
	"github.com/stitchfix/flotilla-os/clients/middleware"
	"github.com/stitchfix/flotilla-os/queue"
	"net/http"
	"strings"
	"time"
	"github.com/pkg/errors"
	"github.com/rs/cors"
	"github.com/stitchfix/flotilla-os/clients/cluster"
	"github.com/stitchfix/flotilla-os/clients/logs"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/execution/engine"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/services"
	"github.com/stitchfix/flotilla-os/state"
	"github.com/stitchfix/flotilla-os/worker"
)
type App struct {
	address            string
	mode               string
	corsAllowedOrigins []string
	logger             flotillaLog.Logger
	readTimeout        time.Duration
	writeTimeout       time.Duration
	handler            http.Handler
	workerManager      worker.Worker
}
func (app *App) Run() error {
	srv := &http.Server{
		Addr:         app.address,
		Handler:      app.handler,
		ReadTimeout:  app.readTimeout,
		WriteTimeout: app.writeTimeout,
	}
	app.workerManager.GetTomb().Go(app.workerManager.Run)
	return srv.ListenAndServe()
}
func NewApp(conf config.Config,
	log flotillaLog.Logger,
	eksLogsClient logs.Client,
	eksExecutionEngine engine.Engine,
	stateManager state.Manager,
	eksClusterClient cluster.Client,
	eksQueueManager queue.Manager,
	emrExecutionEngine engine.Engine,
	emrQueueManager queue.Manager,
	middlewareClient middleware.Client,
) (App, error) {
	var app App
	app.logger = log
	app.configure(conf)
	executionService, err := services.NewExecutionService(conf, eksExecutionEngine, stateManager, eksClusterClient, emrExecutionEngine)
	if err != nil {
		return app, errors.Wrap(err, "problem initializing execution service")
	}
	templateService, err := services.NewTemplateService(conf, stateManager)
	if err != nil {
		return app, errors.Wrap(err, "problem initializing template service")
	}
	eksLogService, err := services.NewLogService(stateManager, eksLogsClient)
	if err != nil {
		return app, errors.Wrap(err, "problem initializing eks log service")
	}
	workerService, err := services.NewWorkerService(conf, stateManager)
	if err != nil {
		return app, errors.Wrap(err, "problem initializing worker service")
	}
	definitionService, err := services.NewDefinitionService(stateManager)
	if err != nil {
		return app, errors.Wrap(err, "problem initializing definition service")
	}
	ep := endpoints{
		executionService:  executionService,
		eksLogService:     eksLogService,
		workerService:     workerService,
		templateService:   templateService,
		logger:            log,
		middlewareClient:  middlewareClient,
		definitionService: definitionService,
	}
	app.configureRoutes(ep)
	if err = app.initializeEKSWorkers(conf, log, eksExecutionEngine, emrExecutionEngine, stateManager, eksQueueManager); err != nil {
		return app, errors.Wrap(err, "problem eks initializing workers")
	}
	return app, nil
}
func (app *App) configure(conf config.Config) {
	app.address = conf.GetString("http_server_listen_address")
	if len(app.address) == 0 {
		app.address = ":5000"
	}
	readTimeout := conf.GetInt("http_server_read_timeout_seconds")
	if readTimeout == 0 {
		readTimeout = 5
	}
	writeTimeout := conf.GetInt("http_server_write_timeout_seconds")
	if writeTimeout == 0 {
		writeTimeout = 10
	}
	app.readTimeout = time.Duration(readTimeout) * time.Second
	app.writeTimeout = time.Duration(writeTimeout) * time.Second
	app.mode = conf.GetString("flotilla_mode")
	app.corsAllowedOrigins = strings.Split(conf.GetString("http_server_cors_allowed_origins"), ",")
}
func (app *App) configureRoutes(ep endpoints) {
	router := NewRouter(ep)
	c := cors.New(cors.Options{
		AllowedOrigins: app.corsAllowedOrigins,
		AllowedMethods: []string{"GET", "DELETE", "POST", "PUT"},
	})
	app.handler = c.Handler(router)
}
func (app *App) initializeEKSWorkers(
	conf config.Config,
	log flotillaLog.Logger,
	ee engine.Engine,
	emr engine.Engine,
	sm state.Manager,
	qm queue.Manager) error {
	workerManager, err := worker.NewWorker("worker_manager", log, conf, ee, emr, sm, qm)
	_ = app.logger.Log("message", "Starting worker", "name", "worker_manager")
	if err != nil {
		return errors.Wrapf(err, "problem initializing worker with name [%s]", "worker_manager")
	}
	app.workerManager = workerManager
	return nil
}
func (app *App) initializeEMRWorkers(
	conf config.Config,
	log flotillaLog.Logger,
	ee engine.Engine,
	emr engine.Engine,
	sm state.Manager,
	qm queue.Manager) error {
	workerManager, err := worker.NewWorker("worker_manager", log, conf, ee, emr, sm, qm)
	_ = app.logger.Log("message", "Starting worker", "name", "worker_manager")
	if err != nil {
		return errors.Wrapf(err, "problem initializing worker with name [%s]", "worker_manager")
	}
	app.workerManager = workerManager
	return nil
}

================
File: flotilla/endpoints.go
================
package flotilla
import (
	"crypto/md5"
	"encoding/json"
	"fmt"
	"math/rand"
	"net/http"
	"net/url"
	"strconv"
	"strings"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/gorilla/mux"
	"github.com/stitchfix/flotilla-os/clients/middleware"
	"github.com/stitchfix/flotilla-os/exceptions"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/services"
	"github.com/stitchfix/flotilla-os/state"
	"github.com/stitchfix/flotilla-os/utils"
)
type endpoints struct {
	executionService  services.ExecutionService
	definitionService services.DefinitionService
	templateService   services.TemplateService
	eksLogService     services.LogService
	workerService     services.WorkerService
	middlewareClient  middleware.Client
	logger            flotillaLog.Logger
}
type listRequest struct {
	limit      int
	offset     int
	sortBy     string
	order      string
	filters    map[string][]string
	envFilters map[string]string
}
func (ep *endpoints) getURLParam(v url.Values, key string, defaultValue string) string {
	val, ok := v[key]
	if ok && len(val) > 0 {
		return val[0]
	}
	return defaultValue
}
func (ep *endpoints) getFilters(params url.Values, nonFilters map[string]bool) (map[string][]string, map[string]string) {
	filters := make(map[string][]string)
	envFilters := make(map[string]string)
	for k, v := range params {
		if !nonFilters[k] && len(v) > 0 {
			if k == "env" {
				for _, kv := range v {
					split := strings.Split(kv, "|")
					if len(split) == 2 {
						envFilters[split[0]] = split[1]
					}
				}
			} else {
				filters[k] = v
			}
		}
	}
	return filters, envFilters
}
func (ep *endpoints) decodeListRequest(r *http.Request) listRequest {
	var lr listRequest
	params := r.URL.Query()
	lr.limit, _ = strconv.Atoi(ep.getURLParam(params, "limit", "1024"))
	lr.offset, _ = strconv.Atoi(ep.getURLParam(params, "offset", "0"))
	lr.sortBy = ep.getURLParam(params, "sort_by", "group_name")
	lr.order = ep.getURLParam(params, "order", "asc")
	lr.filters, lr.envFilters = ep.getFilters(params, map[string]bool{
		"limit":   true,
		"offset":  true,
		"sort_by": true,
		"order":   true,
	})
	return lr
}
func (ep *endpoints) decodeOrderableListRequest(r *http.Request, orderable state.IOrderable) listRequest {
	var lr listRequest
	params := r.URL.Query()
	lr.limit, _ = strconv.Atoi(ep.getURLParam(params, "limit", "1024"))
	lr.offset, _ = strconv.Atoi(ep.getURLParam(params, "offset", "0"))
	lr.sortBy = ep.getURLParam(params, "sort_by", orderable.DefaultOrderField())
	lr.order = ep.getURLParam(params, "order", "asc")
	lr.filters, lr.envFilters = ep.getFilters(params, map[string]bool{
		"limit":   true,
		"offset":  true,
		"sort_by": true,
		"order":   true,
	})
	return lr
}
func (ep *endpoints) decodeRequest(r *http.Request, entity interface{}) error {
	return json.NewDecoder(r.Body).Decode(entity)
}
func (ep endpoints) encodeError(w http.ResponseWriter, err error) {
	w.Header().Set("Content-Type", "application/json; charset=utf-8")
	switch err.(type) {
	case exceptions.MalformedInput:
		w.WriteHeader(http.StatusBadRequest)
	case exceptions.ConflictingResource:
		w.WriteHeader(http.StatusConflict)
	case exceptions.MissingResource:
		w.WriteHeader(http.StatusNotFound)
	default:
		w.WriteHeader(http.StatusInternalServerError)
	}
	_ = json.NewEncoder(w).Encode(map[string]interface{}{
		"error": err.Error(),
	})
}
func (ep *endpoints) encodeResponse(w http.ResponseWriter, response interface{}) {
	w.Header().Set("Content-Type", "application/json; charset=utf-8")
	_ = json.NewEncoder(w).Encode(response)
}
func (ep *endpoints) ListDefinitions(w http.ResponseWriter, r *http.Request) {
	lr := ep.decodeListRequest(r)
	definitionList, err := ep.definitionService.List(
		lr.limit, lr.offset, lr.sortBy, lr.order, lr.filters, lr.envFilters)
	if definitionList.Definitions == nil {
		definitionList.Definitions = []state.Definition{}
	}
	if err != nil {
		ep.logger.Log(
			"message", "problem listing definitions",
			"operation", "ListDefinitions",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		response := make(map[string]interface{})
		response["total"] = definitionList.Total
		response["definitions"] = definitionList.Definitions
		response["limit"] = lr.limit
		response["offset"] = lr.offset
		response["sort_by"] = lr.sortBy
		response["order"] = lr.order
		response["env_filters"] = lr.envFilters
		for k, v := range lr.filters {
			response[k] = v
		}
		ep.encodeResponse(w, response)
	}
}
func (ep *endpoints) GetDefinition(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	definition, err := ep.definitionService.Get(vars["definition_id"])
	if err != nil {
		ep.logger.Log(
			"message", "problem getting definitions",
			"operation", "GetDefinition",
			"error", fmt.Sprintf("%+v", err),
			"definition_id", vars["definition_id"])
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, definition)
	}
}
func (ep *endpoints) GetDefinitionByAlias(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	definition, err := ep.definitionService.GetByAlias(vars["alias"])
	if err != nil {
		ep.logger.Log(
			"message", "problem getting definition by alias",
			"operation", "GetDefinitionByAlias",
			"error", fmt.Sprintf("%+v", err),
			"alias", vars["alias"])
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, definition)
	}
}
func (ep *endpoints) CreateDefinition(w http.ResponseWriter, r *http.Request) {
	var definition state.Definition
	err := ep.decodeRequest(r, &definition)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	created, err := ep.definitionService.Create(&definition)
	if err != nil {
		ep.logger.Log(
			"message", "problem creating definition",
			"operation", "CreateDefinition",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, created)
	}
}
func (ep *endpoints) UpdateDefinition(w http.ResponseWriter, r *http.Request) {
	var definition state.Definition
	err := ep.decodeRequest(r, &definition)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	vars := mux.Vars(r)
	updated, err := ep.definitionService.Update(vars["definition_id"], definition)
	if err != nil {
		ep.logger.Log(
			"message", "problem updating definition",
			"operation", "UpdateDefinition",
			"error", fmt.Sprintf("%+v", err),
			"definition_id", vars["definition_id"])
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, updated)
	}
}
func (ep *endpoints) DeleteDefinition(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	err := ep.definitionService.Delete(vars["definition_id"])
	if err != nil {
		ep.logger.Log(
			"message", "problem deleting definition",
			"operation", "DeleteDefinition",
			"error", fmt.Sprintf("%+v", err),
			"definition_id", vars["definition_id"])
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, map[string]bool{"deleted": true})
	}
}
func (ep *endpoints) ListRuns(w http.ResponseWriter, r *http.Request) {
	lr := ep.decodeListRequest(r)
	runList, err := ep.executionService.List(lr.limit, lr.offset, lr.order, lr.sortBy, lr.filters, lr.envFilters)
	if err != nil {
		ep.logger.Log(
			"message", "problem listing runs",
			"operation", "ListRuns",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		response := make(map[string]interface{})
		response["total"] = runList.Total
		response["history"] = runList.Runs
		response["limit"] = lr.limit
		response["offset"] = lr.offset
		response["sort_by"] = lr.sortBy
		response["order"] = lr.order
		response["env_filters"] = lr.envFilters
		for k, v := range lr.filters {
			response[k] = v
		}
		ep.encodeResponse(w, response)
	}
}
func (ep *endpoints) ListDefinitionRuns(w http.ResponseWriter, r *http.Request) {
	lr := ep.decodeListRequest(r)
	vars := mux.Vars(r)
	definitionID, ok := vars["definition_id"]
	if ok {
		lr.filters["definition_id"] = []string{definitionID}
	}
	runList, err := ep.executionService.List(lr.limit, lr.offset, lr.order, lr.sortBy, lr.filters, lr.envFilters)
	if err != nil {
		ep.logger.Log(
			"message", "problem listing definition runs",
			"operation", "ListDefinitionRuns",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		response := ep.createListRunsResponse(runList, lr)
		ep.encodeResponse(w, response)
	}
}
func (ep *endpoints) ListTemplateRuns(w http.ResponseWriter, r *http.Request) {
	lr := ep.decodeListRequest(r)
	vars := mux.Vars(r)
	tplID, ok := vars["template_id"]
	if ok {
		lr.filters["executable_id"] = []string{tplID}
	}
	runList, err := ep.executionService.List(lr.limit, lr.offset, lr.order, lr.sortBy, lr.filters, lr.envFilters)
	if err != nil {
		ep.logger.Log(
			"message", "problem listing runs for template",
			"operation", "ListTemplateRuns",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		response := ep.createListRunsResponse(runList, lr)
		ep.encodeResponse(w, response)
	}
}
func (ep *endpoints) createListRunsResponse(runList state.RunList, req listRequest) map[string]interface{} {
	response := make(map[string]interface{})
	response["total"] = runList.Total
	response["history"] = runList.Runs
	response["limit"] = req.limit
	response["offset"] = req.offset
	response["sort_by"] = req.sortBy
	response["order"] = req.order
	response["env_filters"] = req.envFilters
	for k, v := range req.filters {
		response[k] = v
	}
	return response
}
func (ep *endpoints) GetRun(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	run, err := ep.executionService.Get(vars["run_id"])
	if err != nil {
		ep.logger.Log(
			"message", "problem getting run",
			"operation", "GetRun",
			"error", fmt.Sprintf("%+v", err),
			"run_id", vars["run_id"])
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, run)
	}
}
func (ep *endpoints) GetPayload(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	run, err := ep.executionService.Get(vars["run_id"])
	if err != nil {
		ep.logger.Log(
			"message", "problem getting run",
			"operation", "GetRun",
			"error", fmt.Sprintf("%+v", err),
			"run_id", vars["run_id"])
		ep.encodeError(w, err)
	} else {
		if run.ExecutionRequestCustom != nil {
			ep.encodeResponse(w, run.ExecutionRequestCustom)
		} else {
			ep.encodeResponse(w, map[string]string{})
		}
	}
}
func (ep *endpoints) CreateRun(w http.ResponseWriter, r *http.Request) {
	var lr state.LaunchRequest
	err := ep.decodeRequest(r, &lr)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	vars := mux.Vars(r)
	req := state.DefinitionExecutionRequest{
		ExecutionRequestCommon: &state.ExecutionRequestCommon{
			Env:              lr.Env,
			OwnerID:          "v1-unknown",
			Command:          nil,
			Memory:           nil,
			Cpu:              nil,
			Gpu:              nil,
			Engine:           &state.DefaultEngine,
			EphemeralStorage: nil,
			NodeLifecycle:    nil,
			CommandHash:      nil,
			Tier:             lr.Tier,
		},
	}
	run, err := ep.executionService.CreateDefinitionRunByDefinitionID(vars["definition_id"], &req)
	if err != nil {
		ep.logger.Log(
			"message", "problem creating run",
			"operation", "CreateRun",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, run)
	}
}
func (ep *endpoints) CreateRunV2(w http.ResponseWriter, r *http.Request) {
	var lr state.LaunchRequestV2
	err := ep.decodeRequest(r, &lr)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	err = ep.middlewareClient.AnnotateLaunchRequest(&r.Header, &lr)
	if err != nil {
		ep.encodeError(w, err)
		return
	}
	if len(lr.RunTags.OwnerEmail) == 0 || len(lr.RunTags.TeamName) == 0 {
		ep.encodeError(w, exceptions.MalformedInput{
			ErrorString: fmt.Sprintf("run_tags must exist in body and contain [owner_email] and [team_name]")})
		return
	}
	vars := mux.Vars(r)
	if lr.Engine == nil {
		if lr.SparkExtension != nil {
			lr.Engine = &state.EKSSparkEngine
		} else {
			lr.Engine = &state.EKSEngine
		}
	}
	if lr.CommandHash == nil && lr.Description != nil {
		lr.CommandHash = aws.String(fmt.Sprintf("%x", md5.Sum([]byte(*lr.Description))))
	}
	req := state.DefinitionExecutionRequest{
		ExecutionRequestCommon: &state.ExecutionRequestCommon{
			Env:              lr.Env,
			OwnerID:          lr.RunTags.OwnerEmail,
			Command:          nil,
			Memory:           nil,
			Cpu:              nil,
			Gpu:              nil,
			Engine:           lr.Engine,
			EphemeralStorage: nil,
			NodeLifecycle:    nil,
			SparkExtension:   lr.SparkExtension,
			Description:      lr.Description,
			CommandHash:      lr.CommandHash,
			IdempotenceKey:   lr.IdempotenceKey,
			Arch:             lr.Arch,
			Labels:           lr.Labels,
			ServiceAccount:   lr.ServiceAccount,
			Tier:             lr.Tier,
		},
	}
	run, err := ep.executionService.CreateDefinitionRunByDefinitionID(vars["definition_id"], &req)
	if err != nil {
		ep.logger.Log(
			"message", "problem creating V2 run",
			"operation", "CreateRunV2",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, run)
	}
}
func (ep *endpoints) CreateRunV4(w http.ResponseWriter, r *http.Request) {
	var lr state.LaunchRequestV2
	err := ep.decodeRequest(r, &lr)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	err = ep.middlewareClient.AnnotateLaunchRequest(&r.Header, &lr)
	if err != nil {
		ep.encodeError(w, err)
		return
	}
	if len(lr.RunTags.OwnerID) == 0 {
		ep.encodeError(w, exceptions.MalformedInput{
			ErrorString: fmt.Sprintf("run_tags must exist in body and contain [owner_id]")})
		return
	}
	if lr.Engine == nil {
		if lr.SparkExtension != nil {
			lr.Engine = &state.EKSSparkEngine
		} else {
			lr.Engine = &state.EKSEngine
		}
	}
	clusterMetadata, err := ep.executionService.ListClusters()
	if err != nil {
		ep.logger.Log(
			"message", "problem listing clusters",
			"operation", "CreateRunV4",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
		return
	}
	if len(clusterMetadata) == 0 {
		*lr.ClusterName = ep.executionService.GetDefaultCluster()
	} else {
		var activeClusters []string
		for _, cluster := range clusterMetadata {
			if cluster.Status == state.StatusActive {
				activeClusters = append(activeClusters, cluster.Name)
			}
		}
		if len(activeClusters) > 0 {
			*lr.ClusterName = activeClusters[rand.Intn(len(activeClusters))]
		} else if len(clusterMetadata) > 0 {
			*lr.ClusterName = clusterMetadata[0].Name
		}
	}
	if lr.CommandHash == nil && lr.Description != nil {
		lr.CommandHash = aws.String(fmt.Sprintf("%x", md5.Sum([]byte(*lr.Description))))
	}
	if lr.NodeLifecycle != nil {
		if !utils.StringSliceContains(state.NodeLifeCycles, *lr.NodeLifecycle) {
			ep.encodeError(w, exceptions.MalformedInput{
				ErrorString: fmt.Sprintf("Nodelifecyle must be [normal, spot]")})
			return
		}
	} else {
		lr.NodeLifecycle = &state.DefaultLifecycle
	}
	vars := mux.Vars(r)
	req := state.DefinitionExecutionRequest{
		ExecutionRequestCommon: &state.ExecutionRequestCommon{
			ClusterName:           *lr.ClusterName,
			Env:                   lr.Env,
			OwnerID:               lr.RunTags.OwnerID,
			Command:               lr.Command,
			Memory:                lr.Memory,
			Cpu:                   lr.Cpu,
			Gpu:                   lr.Gpu,
			EphemeralStorage:      lr.EphemeralStorage,
			Engine:                lr.Engine,
			NodeLifecycle:         lr.NodeLifecycle,
			ActiveDeadlineSeconds: lr.ActiveDeadlineSeconds,
			SparkExtension:        lr.SparkExtension,
			Description:           lr.Description,
			CommandHash:           lr.CommandHash,
			IdempotenceKey:        lr.IdempotenceKey,
			Arch:                  lr.Arch,
			Labels:                lr.Labels,
			ServiceAccount:        lr.ServiceAccount,
			Tier:                  lr.Tier,
		},
	}
	run, err := ep.executionService.CreateDefinitionRunByDefinitionID(vars["definition_id"], &req)
	if err != nil {
		ep.logger.Log(
			"message", "problem creating V4 run",
			"operation", "CreateRunV4",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, run)
	}
}
func (ep *endpoints) CreateRunByAlias(w http.ResponseWriter, r *http.Request) {
	var lr state.LaunchRequestV2
	err := ep.decodeRequest(r, &lr)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	err = ep.middlewareClient.AnnotateLaunchRequest(&r.Header, &lr)
	if err != nil {
		ep.encodeError(w, err)
		return
	}
	if len(lr.RunTags.OwnerID) == 0 {
		ep.encodeError(w, exceptions.MalformedInput{
			ErrorString: fmt.Sprintf("run_tags must exist in body and contain [owner_id]")})
		return
	}
	if lr.Engine == nil || *lr.Engine == "ecs" {
		if lr.SparkExtension != nil {
			lr.Engine = &state.EKSSparkEngine
		} else {
			lr.Engine = &state.EKSEngine
		}
	}
	if lr.CommandHash == nil && lr.Description != nil {
		lr.CommandHash = aws.String(fmt.Sprintf("%x", md5.Sum([]byte(*lr.Description))))
	}
	if lr.NodeLifecycle != nil {
		if !utils.StringSliceContains(state.NodeLifeCycles, *lr.NodeLifecycle) {
			ep.encodeError(w, exceptions.MalformedInput{
				ErrorString: fmt.Sprintf("Nodelifecyle must be [normal, spot]")})
			return
		}
	} else {
		lr.NodeLifecycle = &state.DefaultLifecycle
	}
	vars := mux.Vars(r)
	req := state.DefinitionExecutionRequest{
		ExecutionRequestCommon: &state.ExecutionRequestCommon{
			Env:                   lr.Env,
			OwnerID:               lr.RunTags.OwnerID,
			Command:               lr.Command,
			Memory:                lr.Memory,
			Cpu:                   lr.Cpu,
			Gpu:                   lr.Gpu,
			EphemeralStorage:      lr.EphemeralStorage,
			Engine:                lr.Engine,
			NodeLifecycle:         lr.NodeLifecycle,
			ActiveDeadlineSeconds: lr.ActiveDeadlineSeconds,
			SparkExtension:        lr.SparkExtension,
			Description:           lr.Description,
			CommandHash:           lr.CommandHash,
			IdempotenceKey:        lr.IdempotenceKey,
			Arch:                  lr.Arch,
			Labels:                lr.Labels,
			ServiceAccount:        lr.ServiceAccount,
			Tier:                  lr.Tier,
		},
	}
	run, err := ep.executionService.CreateDefinitionRunByAlias(vars["alias"], &req)
	if err != nil {
		ep.logger.Log(
			"message", "problem creating run alias",
			"operation", "CreateRunByAlias",
			"error", fmt.Sprintf("%+v", err),
			"alias", vars["alias"])
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, run)
	}
}
func (ep *endpoints) StopRun(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	userInfo := ep.ExtractUserInfo(r)
	err := ep.executionService.Terminate(vars["run_id"], userInfo)
	if err != nil {
		ep.logger.Log(
			"message", "problem stopping run",
			"operation", "StopRun",
			"error", fmt.Sprintf("%+v", err),
			"run_id", vars["run_id"])
	}
	ep.encodeResponse(w, map[string]bool{"terminated": true})
}
func (ep *endpoints) ExtractUserInfo(r *http.Request) state.UserInfo {
	var userInfo state.UserInfo
	for name, headers := range r.Header {
		name = strings.ToLower(name)
		for _, h := range headers {
			if strings.Contains(name, "-name") {
				userInfo.Name = h
			}
			if strings.Contains(name, "-email") {
				userInfo.Email = h
			}
		}
	}
	return userInfo
}
func (ep *endpoints) UpdateRun(w http.ResponseWriter, r *http.Request) {
	var run state.Run
	err := ep.decodeRequest(r, &run)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	vars := mux.Vars(r)
	err = ep.executionService.UpdateStatus(vars["run_id"], run.Status, run.ExitCode, run.RunExceptions, run.ExitReason)
	if err != nil {
		ep.logger.Log(
			"message", "problem updating run",
			"operation", "UpdateRun",
			"error", fmt.Sprintf("%+v", err),
			"run_id", vars["run_id"])
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, map[string]bool{"updated": true})
	}
}
func (ep *endpoints) GetEvents(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	run, err := ep.executionService.Get(vars["run_id"])
	if err != nil {
		ep.logger.Log(
			"message", "problem getting run",
			"operation", "GetRun",
			"error", fmt.Sprintf("%+v", err),
			"run_id", vars["run_id"])
		ep.encodeError(w, err)
		return
	}
	var podEventList state.PodEventList
	if run.PodEvents != nil {
		podEventList.Total = len(*run.PodEvents)
		podEventList.PodEvents = *run.PodEvents
	}
	ep.encodeResponse(w, podEventList)
}
func (ep *endpoints) GetLogs(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	params := r.URL.Query()
	lastSeen := ep.getURLParam(params, "last_seen", "")
	rawText := ep.getStringBoolVal(ep.getURLParam(params, "raw_text", ""))
	run, err := ep.executionService.Get(vars["run_id"])
	role := ep.getURLParam(params, "role", "driver")
	facility := ep.getURLParam(params, "facility", "stderr")
	if err != nil {
		_ = ep.logger.Log(
			"message", "problem getting run",
			"operation", "GetRun",
			"error", fmt.Sprintf("%+v", err),
			"run_id", vars["run_id"])
		ep.encodeError(w, err)
		return
	}
	if run.Engine == nil {
		run.Engine = &state.DefaultEngine
	}
	if rawText == true {
		_ = ep.eksLogService.LogsText(vars["run_id"], w)
	} else {
		log, newLastSeen, err := ep.eksLogService.Logs(vars["run_id"], &lastSeen, &role, &facility)
		res := map[string]string{
			"log":       "",
			"last_seen": lastSeen,
		}
		if err == nil {
			res = map[string]string{
				"log":       log,
				"last_seen": *newLastSeen,
			}
		}
		ep.encodeResponse(w, res)
	}
}
func (ep *endpoints) GetGroups(w http.ResponseWriter, r *http.Request) {
	response := make(map[string]interface{})
	response["total"] = 0
	response["groups"] = []string{}
	ep.encodeResponse(w, response)
}
func (ep *endpoints) GetTags(w http.ResponseWriter, r *http.Request) {
	response := make(map[string]interface{})
	response["total"] = 0
	response["tags"] = []string{}
	ep.encodeResponse(w, response)
}
func (ep *endpoints) ListClusters(w http.ResponseWriter, r *http.Request) {
	clusters, err := ep.executionService.ListClusters()
	if err != nil {
		ep.encodeError(w, err)
		return
	}
	ep.encodeResponse(w, map[string]interface{}{
		"clusters": clusters,
	})
}
func (ep *endpoints) ListWorkers(w http.ResponseWriter, r *http.Request) {
	wl, err := ep.workerService.List(state.EKSEngine)
	wlEKS, errEKS := ep.workerService.List(state.EKSEngine)
	if wl.Workers == nil {
		wl.Workers = []state.Worker{}
	}
	if wlEKS.Workers == nil {
		wlEKS.Workers = []state.Worker{}
	}
	if err != nil || errEKS != nil {
		ep.encodeError(w, err)
	} else {
		response := make(map[string]interface{})
		response["total"] = wl.Total + wlEKS.Total
		response["workers"] = append(wl.Workers, wlEKS.Workers...)
		ep.encodeResponse(w, response)
	}
}
func (ep *endpoints) GetWorker(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	worker, err := ep.workerService.Get(vars["worker_type"], state.DefaultEngine)
	if err != nil {
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, worker)
	}
}
func (ep *endpoints) UpdateWorker(w http.ResponseWriter, r *http.Request) {
	var worker state.Worker
	err := ep.decodeRequest(r, &worker)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	vars := mux.Vars(r)
	updated, err := ep.workerService.Update(vars["worker_type"], worker)
	if err != nil {
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, updated)
	}
}
func (ep *endpoints) BatchUpdateWorkers(w http.ResponseWriter, r *http.Request) {
	var wks []state.Worker
	err := ep.decodeRequest(r, &wks)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	updated, err := ep.workerService.BatchUpdate(wks)
	if err != nil {
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, updated)
	}
}
func (ep *endpoints) getStringBoolVal(s string) bool {
	l := strings.ToLower(s)
	if l == "true" {
		return true
	}
	return false
}
func (ep *endpoints) CreateTemplateRunByName(w http.ResponseWriter, r *http.Request) {
	var req state.TemplateExecutionRequest
	err := ep.decodeRequest(r, &req)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	if len(req.OwnerID) == 0 {
		ep.encodeError(w, exceptions.MalformedInput{
			ErrorString: fmt.Sprintf("request payload must contain [owner_id]; the run_tags field is deprecated for the v7 endpoint.")})
		return
	}
	req.Engine = &state.DefaultEngine
	if req.NodeLifecycle != nil {
		if !utils.StringSliceContains(state.NodeLifeCycles, *req.NodeLifecycle) {
			ep.encodeError(w, exceptions.MalformedInput{
				ErrorString: fmt.Sprintf("Nodelifecyle must be [normal, spot]")})
			return
		}
	} else {
		req.NodeLifecycle = &state.DefaultLifecycle
	}
	vars := mux.Vars(r)
	run, err := ep.executionService.CreateTemplateRunByTemplateName(vars["template_name"], vars["template_version"], &req)
	if err != nil {
		ep.logger.Log(
			"message", "problem creating template run",
			"operation", "CreateTemplateRun",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, run)
	}
}
func (ep *endpoints) CreateTemplateRun(w http.ResponseWriter, r *http.Request) {
	var req state.TemplateExecutionRequest
	err := ep.decodeRequest(r, &req)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	if len(req.OwnerID) == 0 {
		ep.encodeError(w, exceptions.MalformedInput{
			ErrorString: fmt.Sprintf("request payload must contain [owner_id]; the run_tags field is deprecated for the v7 endpoint.")})
		return
	}
	req.Engine = &state.DefaultEngine
	if req.NodeLifecycle != nil {
		if !utils.StringSliceContains(state.NodeLifeCycles, *req.NodeLifecycle) {
			ep.encodeError(w, exceptions.MalformedInput{
				ErrorString: fmt.Sprintf("Nodelifecyle must be [normal, spot]")})
			return
		}
	} else {
		req.NodeLifecycle = &state.DefaultLifecycle
	}
	vars := mux.Vars(r)
	run, err := ep.executionService.CreateTemplateRunByTemplateID(vars["template_id"], &req)
	if err != nil {
		ep.logger.Log(
			"message", "problem creating template run",
			"operation", "CreateTemplateRun",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, run)
	}
}
func (ep *endpoints) ListTemplates(w http.ResponseWriter, r *http.Request) {
	var (
		tl  state.TemplateList
		err error
	)
	lr := ep.decodeOrderableListRequest(r, &state.Template{})
	params := r.URL.Query()
	latestOnly := ep.getStringBoolVal(ep.getURLParam(params, "latest_only", "true"))
	if latestOnly == true {
		tl, err = ep.templateService.ListLatestOnly(lr.limit, lr.offset, lr.sortBy, lr.order)
	} else {
		tl, err = ep.templateService.List(lr.limit, lr.offset, lr.sortBy, lr.order)
	}
	if tl.Templates == nil {
		tl.Templates = []state.Template{}
	}
	if err != nil {
		ep.logger.Log(
			"message", "problem listing templates",
			"operation", "ListTemplates",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		response := make(map[string]interface{})
		response["total"] = tl.Total
		response["templates"] = tl.Templates
		response["limit"] = lr.limit
		response["offset"] = lr.offset
		response["sort_by"] = lr.sortBy
		response["order"] = lr.order
		ep.encodeResponse(w, response)
	}
}
func (ep *endpoints) GetTemplate(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	tpl, err := ep.templateService.GetByID(vars["template_id"])
	if err != nil {
		ep.logger.Log(
			"message", "problem getting templates",
			"operation", "GetTemplate",
			"error", fmt.Sprintf("%+v", err),
			"template_id", vars["template_id"])
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, tpl)
	}
}
func (ep *endpoints) CreateTemplate(w http.ResponseWriter, r *http.Request) {
	var req state.CreateTemplateRequest
	err := ep.decodeRequest(r, &req)
	if err != nil {
		ep.encodeError(w, exceptions.MalformedInput{ErrorString: err.Error()})
		return
	}
	created, err := ep.templateService.Create(&req)
	if err != nil {
		ep.logger.Log(
			"message", "problem creating template",
			"operation", "CreateTemplate",
			"error", fmt.Sprintf("%+v", err))
		ep.encodeError(w, err)
	} else {
		ep.encodeResponse(w, created)
	}
}
func (ep *endpoints) GetCluster(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	clusters, err := ep.executionService.ListClusters()
	if err != nil {
		ep.encodeError(w, err)
		return
	}
	for _, cluster := range clusters {
		if cluster.Name == vars["cluster_name"] {
			ep.encodeResponse(w, cluster)
			return
		}
	}
	ep.encodeError(w, fmt.Errorf("cluster %s not found", vars["cluster_name"]))
}
func (ep *endpoints) UpdateCluster(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	var clusterMetadata state.ClusterMetadata
	if err := json.NewDecoder(r.Body).Decode(&clusterMetadata); err != nil {
		ep.encodeError(w, err)
		return
	}
	clusterMetadata.Name = vars["cluster_name"]
	var err error
	if len(clusterMetadata.AllowedTiers) > 0 || len(clusterMetadata.Capabilities) > 0 ||
		clusterMetadata.Namespace != "" || clusterMetadata.Region != "" ||
		clusterMetadata.EMRVirtualCluster != "" {
		// Full update
		err = ep.executionService.UpdateClusterMetadata(clusterMetadata)
	} else {
		// Just updating status (backward compatibility)
		err = ep.executionService.UpdateClusterMetadata(clusterMetadata)
	}
	if err != nil {
		ep.encodeError(w, err)
		return
	}
	ep.encodeResponse(w, map[string]bool{"updated": true})
}
func (ep *endpoints) DeleteCluster(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	err := ep.executionService.UpdateClusterMetadata(
		state.ClusterMetadata{
			Name:         vars["cluster_name"],
			Status:       state.StatusOffline,
			StatusReason: "Deleted via API",
		})
	if err != nil {
		ep.encodeError(w, err)
		return
	}
	ep.encodeResponse(w, map[string]bool{"deleted": true})
}
func (ep *endpoints) HealthCheck(w http.ResponseWriter, r *http.Request) {
	healthy := true
	var err error
	_, err = ep.executionService.ListClusters()
	if err != nil {
		healthy = false
		ep.logger.Log(
			"message", "health check failed",
			"error", err.Error())
	}
	if healthy {
		ep.encodeResponse(w, map[string]string{
			"status":  "healthy",
			"message": "Service is up and running",
		})
	} else {
		w.WriteHeader(http.StatusServiceUnavailable)
		ep.encodeResponse(w, map[string]string{
			"status":  "unhealthy",
			"message": "Service is experiencing issues",
			"error":   err.Error(),
		})
	}
}

================
File: flotilla/router.go
================
package flotilla
import (
	muxtrace "gopkg.in/DataDog/dd-trace-go.v1/contrib/gorilla/mux"
)
func NewRouter(ep endpoints) *muxtrace.Router {
	r := muxtrace.NewRouter()
	v1 := r.PathPrefix("/api/v1").Subrouter()
	v1.HandleFunc("/task", ep.ListDefinitions).Methods("GET")
	v1.HandleFunc("/task", ep.CreateDefinition).Methods("POST")
	v1.HandleFunc("/task/{definition_id}", ep.GetDefinition).Methods("GET")
	v1.HandleFunc("/task/{definition_id}", ep.UpdateDefinition).Methods("PUT")
	v1.HandleFunc("/task/{definition_id}", ep.DeleteDefinition).Methods("DELETE")
	v1.HandleFunc("/task/{definition_id}/execute", ep.CreateRun).Methods("PUT")
	v1.HandleFunc("/task/alias/{alias}", ep.GetDefinitionByAlias).Methods("GET")
	v1.HandleFunc("/task/alias/{alias}/execute", ep.CreateRunByAlias).Methods("PUT")
	v1.HandleFunc("/history", ep.ListRuns).Methods("GET")
	v1.HandleFunc("/history/{run_id}", ep.GetRun).Methods("GET")
	v1.HandleFunc("/task/history/{run_id}", ep.GetRun).Methods("GET")
	v1.HandleFunc("/task/{definition_id}/history", ep.ListDefinitionRuns).Methods("GET")
	v1.HandleFunc("/task/{definition_id}/history/{run_id}", ep.GetRun).Methods("GET")
	v1.HandleFunc("/task/{definition_id}/history/{run_id}", ep.StopRun).Methods("DELETE")
	v1.HandleFunc("/{run_id}/status", ep.UpdateRun).Methods("PUT")
	v1.HandleFunc("/{run_id}/logs", ep.GetLogs).Methods("GET")
	v1.HandleFunc("/{run_id}/events", ep.GetEvents).Methods("GET")
	v1.HandleFunc("/groups", ep.GetGroups).Methods("GET")
	v1.HandleFunc("/tags", ep.GetTags).Methods("GET")
	v1.HandleFunc("/clusters", ep.ListClusters).Methods("GET")
	v2 := r.PathPrefix("/api/v2").Subrouter()
	v2.HandleFunc("/task/{definition_id}/execute", ep.CreateRunV2).Methods("PUT")
	v4 := r.PathPrefix("/api/v4").Subrouter()
	v4.HandleFunc("/task/{definition_id}/execute", ep.CreateRunV4).Methods("PUT")
	v5 := r.PathPrefix("/api/v5").Subrouter()
	v5.HandleFunc("/worker", ep.ListWorkers).Methods("GET")
	v5.HandleFunc("/worker", ep.BatchUpdateWorkers).Methods("PUT")
	v5.HandleFunc("/worker/{worker_type}", ep.GetWorker).Methods("GET")
	v5.HandleFunc("/worker/{worker_type}", ep.UpdateWorker).Methods("PUT")
	v6 := r.PathPrefix("/api/v6").Subrouter()
	v6.HandleFunc("/clusters", ep.ListClusters).Methods("GET")
	v6.HandleFunc("/clusters/{cluster_name}", ep.GetCluster).Methods("GET")
	v6.HandleFunc("/clusters/{cluster_name}", ep.UpdateCluster).Methods("PUT")
	v6.HandleFunc("/clusters/{cluster_name}", ep.DeleteCluster).Methods("DELETE")
	v6.HandleFunc("/{run_id}/events", ep.GetEvents).Methods("GET")
	v6.HandleFunc("/groups", ep.GetGroups).Methods("GET")
	v6.HandleFunc("/health", ep.HealthCheck).Methods("GET")
	v6.HandleFunc("/history", ep.ListRuns).Methods("GET")
	v6.HandleFunc("/history/{run_id}", ep.GetRun).Methods("GET")
	v6.HandleFunc("/tags", ep.GetTags).Methods("GET")
	v6.HandleFunc("/task", ep.ListDefinitions).Methods("GET")
	v6.HandleFunc("/task", ep.CreateDefinition).Methods("POST")
	v6.HandleFunc("/task/alias/{alias}", ep.GetDefinitionByAlias).Methods("GET")
	v6.HandleFunc("/task/alias/{alias}/execute", ep.CreateRunByAlias).Methods("PUT")
	v6.HandleFunc("/task/{definition_id}", ep.GetDefinition).Methods("GET")
	v6.HandleFunc("/task/{definition_id}", ep.UpdateDefinition).Methods("PUT")
	v6.HandleFunc("/task/{definition_id}", ep.DeleteDefinition).Methods("DELETE")
	v6.HandleFunc("/task/{definition_id}/execute", ep.CreateRunV4).Methods("PUT")
	v6.HandleFunc("/task/{definition_id}/history", ep.ListDefinitionRuns).Methods("GET")
	v6.HandleFunc("/task/{definition_id}/history/{run_id}", ep.GetRun).Methods("GET")
	v6.HandleFunc("/task/{definition_id}/history/{run_id}", ep.StopRun).Methods("DELETE")
	v6.HandleFunc("/task/history/{run_id}", ep.GetRun).Methods("GET")
	v6.HandleFunc("/{run_id}/status", ep.UpdateRun).Methods("PUT")
	v6.HandleFunc("/{run_id}/logs", ep.GetLogs).Methods("GET")
	v7 := r.PathPrefix("/api/v7").Subrouter()
	v7.HandleFunc("/template/{template_id}/execute", ep.CreateTemplateRun).Methods("PUT")
	v7.HandleFunc("/template/name/{template_name}/version/{template_version}/execute", ep.CreateTemplateRunByName).Methods("PUT")
	v7.HandleFunc("/template", ep.ListTemplates).Methods("GET")
	v7.HandleFunc("/template", ep.CreateTemplate).Methods("POST")
	v7.HandleFunc("/template/{template_id}", ep.GetTemplate).Methods("GET")
	v7.HandleFunc("/template/history/{run_id}", ep.GetRun).Methods("GET")
	v7.HandleFunc("/template/{template_id}/history", ep.ListTemplateRuns).Methods("GET")
	v7.HandleFunc("/template/{template_id}/history/{run_id}", ep.GetRun).Methods("GET")
	v7.HandleFunc("/template/{template_id}/history/{run_id}", ep.StopRun).Methods("DELETE")
	return r
}

================
File: log/event.go
================
package log
import (
	"errors"
	"github.com/stitchfix/flotilla-os/clients/httpclient"
	"log"
	"os"
	"time"
)
type EventSink interface {
	Receive(keyvals ...interface{}) error
}
type LocalEventSink struct {
	logger *log.Logger
}
func NewLocalEventSink() *LocalEventSink {
	logger := log.New(os.Stderr, "[LocalEventSink] ",
		log.Ldate|log.Ltime|log.Lshortfile)
	return &LocalEventSink{logger}
}
func (localSink *LocalEventSink) Receive(keyvals ...interface{}) error {
	log.Printf("\n%v\n", keyvals)
	return nil
}
type HTTPEventSink struct {
	path   string
	method string
	client httpclient.Client
}
type HTTPEvent struct {
	Timestamp time.Time              `json:"timestamp"`
	Message   map[string]interface{} `json:"message"`
}
func NewHTTPSink(host string, path string, method string) HTTPEventSink {
	return HTTPEventSink{
		path, method, httpclient.Client{Host: host},
	}
}
func (httpsink *HTTPEventSink) headers() map[string]string {
	return map[string]string{
		"Content-Type": "application/json",
	}
}
func (httpsink *HTTPEventSink) constructMessage(keyvals ...interface{}) (map[string]interface{}, error) {
	n := (len(keyvals) + 1) / 2
	m := make(map[string]interface{}, n)
	for i := 0; i < len(keyvals); i += 2 {
		k := keyvals[i]
		key, ok := k.(string)
		if !ok {
			return m, errors.New("Not all keys are strings")
		}
		var v interface{}
		if i+1 < len(keyvals) {
			v = keyvals[i+1]
		}
		m[key] = v
	}
	return m, nil
}
func (httpsink *HTTPEventSink) Receive(keyvals ...interface{}) error {
	var err error
	var event HTTPEvent
	m, err := httpsink.constructMessage(keyvals...)
	if err != nil {
		return err
	}
	event.Message = m
	event.Timestamp = time.Now().UTC()
	var response interface{}
	return httpsink.client.Post(
		httpsink.method,
		httpsink.headers(),
		&event, &response)
}

================
File: log/logger.go
================
package log
import "github.com/go-kit/kit/log"
type Logger interface {
	Log(keyvals ...interface{}) error
	Event(keyvals ...interface{}) error
}
type logger struct {
	wrapped log.Logger
	sinks   []EventSink
}
func NewLogger(wrapped log.Logger, sinks []EventSink) Logger {
	return &logger{wrapped, sinks}
}
func (l *logger) Log(keyvals ...interface{}) error {
	return l.wrapped.Log(keyvals...)
}
func (l *logger) Event(keyvals ...interface{}) error {
	var err error
	if l.sinks != nil {
		for _, sink := range l.sinks {
			if err = sink.Receive(keyvals...); err != nil {
				_ = l.Log("message", "error sending event", "sink", sink, "error", err)
			}
		}
	}
	return err
}

================
File: queue/manager.go
================
package queue
import (
	"fmt"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/state"
)
type Manager interface {
	Name() string
	QurlFor(name string, prefixed bool) (string, error)
	Initialize(config.Config, string) error
	Enqueue(qURL string, run state.Run) error
	ReceiveRun(qURL string) (RunReceipt, error)
	ReceiveStatus(qURL string) (StatusReceipt, error)
	ReceiveCloudTrail(qURL string) (state.CloudTrailS3File, error)
	ReceiveKubernetesEvent(qURL string) (state.KubernetesEvent, error)
	ReceiveEMREvent(qURL string) (state.EmrEvent, error)
	ReceiveKubernetesRun(queue string) (string, error)
	List() ([]string, error)
}
type RunReceipt struct {
	Run  *state.Run
	Done func() error
}
type StatusReceipt struct {
	StatusUpdate *string
	Done         func() error
}
func NewQueueManager(conf config.Config, name string) (Manager, error) {
	switch name {
	case state.EKSEngine:
		sqsEKS := &SQSManager{}
		if err := sqsEKS.Initialize(conf, state.EKSEngine); err != nil {
			return nil, errors.Wrap(err, "problem initializing SQSManager")
		}
		return sqsEKS, nil
	case state.EKSSparkEngine:
		sqsEKSSpark := &SQSManager{}
		if err := sqsEKSSpark.Initialize(conf, state.EKSSparkEngine); err != nil {
			return nil, errors.Wrap(err, "problem initializing SQSManager")
		}
		return sqsEKSSpark, nil
	default:
		return nil, fmt.Errorf("no QueueManager named [%s] was found", name)
	}
}

================
File: queue/sqs_manager.go
================
package queue
import (
	"encoding/json"
	"fmt"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/aws/aws-sdk-go/aws/session"
	"github.com/aws/aws-sdk-go/service/sqs"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/state"
	awstrace "gopkg.in/DataDog/dd-trace-go.v1/contrib/aws/aws-sdk-go/aws"
)
type SQSManager struct {
	namespace         string
	retentionSeconds  string
	visibilityTimeout string
	qc                sqsClient
	qurlCache         map[string]string
}
type sqsClient interface {
	GetQueueUrl(input *sqs.GetQueueUrlInput) (*sqs.GetQueueUrlOutput, error)
	CreateQueue(input *sqs.CreateQueueInput) (*sqs.CreateQueueOutput, error)
	ListQueues(input *sqs.ListQueuesInput) (*sqs.ListQueuesOutput, error)
	SendMessage(input *sqs.SendMessageInput) (*sqs.SendMessageOutput, error)
	ReceiveMessage(input *sqs.ReceiveMessageInput) (*sqs.ReceiveMessageOutput, error)
	DeleteMessage(input *sqs.DeleteMessageInput) (*sqs.DeleteMessageOutput, error)
}
func (qm *SQSManager) Name() string {
	return "sqs"
}
func (qm *SQSManager) Initialize(conf config.Config, engine string) error {
	if !conf.IsSet("aws_default_region") {
		return errors.Errorf("SQSManager needs [aws_default_region] set in config")
	}
	qm.retentionSeconds = "604800"
	if conf.IsSet("queue_retention_seconds") {
		qm.retentionSeconds = conf.GetString("queue_retention_seconds")
	}
	qm.visibilityTimeout = "45"
	if conf.IsSet("queue_process_time") {
		qm.visibilityTimeout = conf.GetString("queue_process_time")
	}
	if !conf.IsSet("queue_namespace") {
		return errors.Errorf("SQSManager needs [queue_namespace] set in config")
	}
	qm.namespace = conf.GetString("queue_namespace")
	flotillaMode := conf.GetString("flotilla_mode")
	if flotillaMode != "test" {
		sess := awstrace.WrapSession(session.Must(session.NewSession(&aws.Config{
			Region: aws.String(conf.GetString("aws_default_region"))})))
		qm.qc = sqs.New(sess)
	}
	qm.qurlCache = make(map[string]string)
	return nil
}
func (qm *SQSManager) QurlFor(name string, prefixed bool) (string, error) {
	key := fmt.Sprintf("%s-%t", name, prefixed)
	val, ok := qm.qurlCache[key]
	if ok {
		return val, nil
	}
	val, err := qm.getOrCreateQueue(name, prefixed)
	if err == nil {
		qm.qurlCache[key] = val
	}
	return val, err
}
func (qm *SQSManager) getOrCreateQueue(name string, prefixed bool) (string, error) {
	qname := name
	if prefixed {
		qname = fmt.Sprintf("%s-%s", qm.namespace, name)
	}
	res, err := qm.qc.GetQueueUrl(&sqs.GetQueueUrlInput{
		QueueName: &qname,
	})
	if err != nil || res.QueueUrl == nil {
		cqi := sqs.CreateQueueInput{
			Attributes: map[string]*string{
				"MessageRetentionPeriod": &qm.retentionSeconds,
				"VisibilityTimeout":      &qm.visibilityTimeout,
			},
			QueueName: &qname,
		}
		createQueueResponse, err := qm.qc.CreateQueue(&cqi)
		if err != nil {
			return "", errors.Wrapf(err, "problem trying to create sqs queue with name [%s]", qname)
		}
		return *createQueueResponse.QueueUrl, nil
	}
	return *res.QueueUrl, nil
}
func (qm *SQSManager) messageFromRun(run state.Run) (*string, error) {
	jsonized, err := json.Marshal(run)
	if err != nil {
		return nil, errors.Wrapf(err, "problem trying to serialize run with id [%s] as json", run.RunID)
	}
	asString := string(jsonized)
	return &asString, nil
}
func (qm *SQSManager) runFromMessage(message *sqs.Message) (state.Run, error) {
	var run state.Run
	if message == nil {
		return run, errors.Errorf("can't generate Run from nil message")
	}
	body := message.Body
	if body == nil {
		return run, errors.Errorf("can't generate Run from empty message")
	}
	if err := json.Unmarshal([]byte(*body), &run); err != nil {
		errors.Wrapf(err, "problem trying to deserialize run from json [%s]", *body)
	}
	return run, nil
}
func (qm *SQSManager) statusFromMessage(message *sqs.Message) (string, error) {
	var statusUpdate string
	if message == nil {
		return statusUpdate, errors.Errorf("can't generate StatusUpdate from nil message")
	}
	body := message.Body
	if body == nil {
		return statusUpdate, errors.Errorf("can't generate StatusUpdate from empty message")
	}
	return *body, nil
}
func (qm *SQSManager) Enqueue(qURL string, run state.Run) error {
	if len(qURL) == 0 {
		return errors.Errorf("no queue url specified, can't enqueue")
	}
	message, err := qm.messageFromRun(run)
	if err != nil {
		return errors.WithStack(err)
	}
	sme := sqs.SendMessageInput{
		QueueUrl:    &qURL,
		MessageBody: message,
	}
	_, err = qm.qc.SendMessage(&sme)
	if err != nil {
		return errors.Wrap(err, "problem sending sqs message")
	}
	return nil
}
func (qm *SQSManager) ReceiveRun(qURL string) (RunReceipt, error) {
	var receipt RunReceipt
	if len(qURL) == 0 {
		return receipt, errors.Errorf("no queue url specified, can't dequeue")
	}
	maxMessages := int64(1)
	visibilityTimeout := int64(45)
	rmi := sqs.ReceiveMessageInput{
		QueueUrl:            &qURL,
		MaxNumberOfMessages: &maxMessages,
		VisibilityTimeout:   &visibilityTimeout,
	}
	var err error
	response, err := qm.qc.ReceiveMessage(&rmi)
	if err != nil {
		return receipt, errors.Wrapf(err, "problem receiving sqs message from queue url [%s]", qURL)
	}
	if len(response.Messages) == 0 {
		return receipt, nil
	}
	run, err := qm.runFromMessage(response.Messages[0])
	if err != nil {
		return receipt, errors.WithStack(err)
	}
	receipt.Run = &run
	receipt.Done = func() error {
		return qm.ack(qURL, response.Messages[0].ReceiptHandle)
	}
	return receipt, nil
}
func (qm *SQSManager) ReceiveStatus(qURL string) (StatusReceipt, error) {
	var receipt StatusReceipt
	if len(qURL) == 0 {
		return receipt, errors.Errorf("no queue url specified, can't dequeue")
	}
	maxMessages := int64(1)
	visibilityTimeout := int64(45)
	rmi := sqs.ReceiveMessageInput{
		QueueUrl:            &qURL,
		MaxNumberOfMessages: &maxMessages,
		VisibilityTimeout:   &visibilityTimeout,
	}
	var err error
	response, err := qm.qc.ReceiveMessage(&rmi)
	if err != nil {
		return receipt, errors.Wrapf(err, "problem receiving sqs message from queue url [%s]", qURL)
	}
	if len(response.Messages) == 0 {
		return receipt, nil
	}
	statusUpdate, err := qm.statusFromMessage(response.Messages[0])
	if err != nil {
		return receipt, errors.WithStack(err)
	}
	receipt.StatusUpdate = &statusUpdate
	receipt.Done = func() error {
		return qm.ack(qURL, response.Messages[0].ReceiptHandle)
	}
	return receipt, nil
}
func (qm *SQSManager) ReceiveCloudTrail(qURL string) (state.CloudTrailS3File, error) {
	var receipt state.CloudTrailS3File
	if len(qURL) == 0 {
		return receipt, errors.Errorf("no queue url specified, can't dequeue")
	}
	maxMessages := int64(1)
	visibilityTimeout := int64(45)
	rmi := sqs.ReceiveMessageInput{
		QueueUrl:            &qURL,
		MaxNumberOfMessages: &maxMessages,
		VisibilityTimeout:   &visibilityTimeout,
	}
	var err error
	response, err := qm.qc.ReceiveMessage(&rmi)
	if err != nil {
		return receipt, errors.Wrapf(err, "problem receiving sqs message from queue url [%s]", qURL)
	}
	if response != nil && response.Messages != nil && len(response.Messages) > 0 && response.Messages[0].Body != nil {
		body := response.Messages[0].Body
		err = json.Unmarshal([]byte(*body), &receipt)
		_ = qm.ack(qURL, response.Messages[0].ReceiptHandle)
	}
	return receipt, nil
}
func (qm *SQSManager) ReceiveEMREvent(qURL string) (state.EmrEvent, error) {
	var emrEvent state.EmrEvent
	if len(qURL) == 0 {
		return emrEvent, errors.Errorf("no queue url specified, can't dequeue")
	}
	maxMessages := int64(1)
	visibilityTimeout := int64(45)
	rmi := sqs.ReceiveMessageInput{
		QueueUrl:            &qURL,
		MaxNumberOfMessages: &maxMessages,
		VisibilityTimeout:   &visibilityTimeout,
	}
	var err error
	response, err := qm.qc.ReceiveMessage(&rmi)
	if err != nil {
		return emrEvent, errors.Wrapf(err, "problem receiving sqs message from queue url [%s]", qURL)
	}
	if response != nil && response.Messages != nil && len(response.Messages) > 0 && response.Messages[0].Body != nil {
		body := response.Messages[0].Body
		err = json.Unmarshal([]byte(*body), &emrEvent)
		emrEvent.Done = func() error {
			return qm.ack(qURL, response.Messages[0].ReceiptHandle)
		}
	}
	return emrEvent, nil
}
func (qm *SQSManager) ReceiveKubernetesEvent(qURL string) (state.KubernetesEvent, error) {
	var kubernetesEvent state.KubernetesEvent
	if len(qURL) == 0 {
		return kubernetesEvent, errors.Errorf("no queue url specified, can't dequeue")
	}
	maxMessages := int64(1)
	visibilityTimeout := int64(45)
	rmi := sqs.ReceiveMessageInput{
		QueueUrl:            &qURL,
		MaxNumberOfMessages: &maxMessages,
		VisibilityTimeout:   &visibilityTimeout,
	}
	var err error
	response, err := qm.qc.ReceiveMessage(&rmi)
	if err != nil {
		return kubernetesEvent, errors.Wrapf(err, "problem receiving sqs message from queue url [%s]", qURL)
	}
	if response != nil && response.Messages != nil && len(response.Messages) > 0 && response.Messages[0].Body != nil {
		body := response.Messages[0].Body
		err = json.Unmarshal([]byte(*body), &kubernetesEvent)
		kubernetesEvent.Done = func() error {
			return qm.ack(qURL, response.Messages[0].ReceiptHandle)
		}
	}
	return kubernetesEvent, nil
}
func (qm *SQSManager) ReceiveKubernetesRun(queue string) (string, error) {
	var runId string
	qURL, err := qm.QurlFor(queue, false)
	if len(qURL) == 0 || err != nil {
		return runId, errors.Errorf("no queue url specified, can't dequeue")
	}
	maxMessages := int64(1)
	visibilityTimeout := int64(45)
	rmi := sqs.ReceiveMessageInput{
		QueueUrl:            &qURL,
		MaxNumberOfMessages: &maxMessages,
		VisibilityTimeout:   &visibilityTimeout,
	}
	response, err := qm.qc.ReceiveMessage(&rmi)
	if err != nil {
		return runId, errors.Wrapf(err, "problem receiving sqs message from queue url [%s]", qURL)
	}
	if response != nil && response.Messages != nil && len(response.Messages) > 0 && response.Messages[0].Body != nil {
		_ = qm.ack(qURL, response.Messages[0].ReceiptHandle)
		return *response.Messages[0].Body, nil
	}
	return runId, errors.Wrapf(err, "no message")
}
func (qm *SQSManager) ack(qURL string, handle *string) error {
	if handle == nil {
		return errors.Errorf("cannot acknowledge message with nil receipt")
	}
	if len(*handle) == 0 {
		return errors.Errorf("cannot acknowledge message with empty receipt")
	}
	dmi := sqs.DeleteMessageInput{
		QueueUrl:      &qURL,
		ReceiptHandle: handle,
	}
	if _, err := qm.qc.DeleteMessage(&dmi); err != nil {
		return errors.Wrapf(
			err, "problem deleting sqs message with handle [%s] from queue url [%s]", *handle, qURL)
	}
	return nil
}
func (qm *SQSManager) List() ([]string, error) {
	response, err := qm.qc.ListQueues(
		&sqs.ListQueuesInput{QueueNamePrefix: &qm.namespace})
	if err != nil {
		return nil, errors.Wrap(err, "problem listing sqs queues")
	}
	listed := make([]string, len(response.QueueUrls))
	for i, qurl := range response.QueueUrls {
		listed[i] = *qurl
	}
	return listed, nil
}

================
File: services/definition.go
================
package services
import (
	"fmt"
	"github.com/stitchfix/flotilla-os/exceptions"
	"github.com/stitchfix/flotilla-os/state"
	"strings"
)
type DefinitionService interface {
	Create(definition *state.Definition) (state.Definition, error)
	Get(definitionID string) (state.Definition, error)
	GetByAlias(alias string) (state.Definition, error)
	List(limit int, offset int, sortBy string,
		order string, filters map[string][]string,
		envFilters map[string]string) (state.DefinitionList, error)
	Update(definitionID string, updates state.Definition) (state.Definition, error)
	Delete(definitionID string) error
	ListGroups(limit int, offset int, name *string) (state.GroupsList, error)
	ListTags(limit int, offset int, name *string) (state.TagsList, error)
}
type definitionService struct {
	sm state.Manager
}
func NewDefinitionService(stateManager state.Manager) (DefinitionService, error) {
	ds := definitionService{sm: stateManager}
	return &ds, nil
}
func (ds *definitionService) Create(definition *state.Definition) (state.Definition, error) {
	if valid, reasons := definition.IsValid(); !valid {
		return state.Definition{}, exceptions.MalformedInput{strings.Join(reasons, "\n")}
	}
	exists, err := ds.aliasExists(definition.Alias)
	if err != nil {
		return state.Definition{}, err
	}
	if exists {
		return state.Definition{}, exceptions.ConflictingResource{
			fmt.Sprintf("definition with alias [%s] aleady exists", definition.Alias)}
	}
	definitionID, err := state.NewDefinitionID(*definition)
	if err != nil {
		return state.Definition{}, err
	}
	definition.DefinitionID = definitionID
	return *definition, ds.sm.CreateDefinition(*definition)
}
func (ds *definitionService) aliasExists(alias string) (bool, error) {
	dl, err := ds.sm.ListDefinitions(
		1024, 0, "alias", "asc", map[string][]string{"alias": {alias}}, nil)
	if err != nil {
		return false, err
	}
	for _, def := range dl.Definitions {
		if def.Alias == alias {
			return true, nil
		}
	}
	return false, nil
}
func (ds *definitionService) Get(definitionID string) (state.Definition, error) {
	return ds.sm.GetDefinition(definitionID)
}
func (ds *definitionService) GetByAlias(alias string) (state.Definition, error) {
	return ds.sm.GetDefinitionByAlias(alias)
}
func (ds *definitionService) List(limit int, offset int, sortBy string,
	order string, filters map[string][]string,
	envFilters map[string]string) (state.DefinitionList, error) {
	return ds.sm.ListDefinitions(limit, offset, sortBy, order, filters, envFilters)
}
func (ds *definitionService) Update(definitionID string, updates state.Definition) (state.Definition, error) {
	definition, err := ds.sm.GetDefinition(definitionID)
	if err != nil {
		return definition, err
	}
	definition.UpdateWith(updates)
	return ds.sm.UpdateDefinition(definitionID, definition)
}
func (ds *definitionService) Delete(definitionID string) error {
	return ds.sm.DeleteDefinition(definitionID)
}
func (ds *definitionService) ListGroups(limit int, offset int, name *string) (state.GroupsList, error) {
	return ds.sm.ListGroups(limit, offset, name)
}
func (ds *definitionService) ListTags(limit int, offset int, name *string) (state.TagsList, error) {
	return ds.sm.ListTags(limit, offset, name)
}

================
File: services/execution.go
================
package services
import (
	"encoding/json"
	"errors"
	"fmt"
	"math/rand"
	"regexp"
	"slices"
	"strconv"
	"strings"
	"time"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/stitchfix/flotilla-os/clients/cluster"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/exceptions"
	"github.com/stitchfix/flotilla-os/execution/engine"
	"github.com/stitchfix/flotilla-os/state"
)
type ExecutionService interface {
	CreateDefinitionRunByDefinitionID(definitionID string, req *state.DefinitionExecutionRequest) (state.Run, error)
	CreateDefinitionRunByAlias(alias string, req *state.DefinitionExecutionRequest) (state.Run, error)
	List(
		limit int,
		offset int,
		sortOrder string,
		sortField string,
		filters map[string][]string,
		envFilters map[string]string) (state.RunList, error)
	Get(runID string) (state.Run, error)
	UpdateStatus(runID string, status string, exitCode *int64, runExceptions *state.RunExceptions, exitReason *string) error
	Terminate(runID string, userInfo state.UserInfo) error
	ReservedVariables() []string
	ListClusters() ([]state.ClusterMetadata, error)
	GetDefaultCluster() string
	GetEvents(run state.Run) (state.PodEventList, error)
	CreateTemplateRunByTemplateID(templateID string, req *state.TemplateExecutionRequest) (state.Run, error)
	CreateTemplateRunByTemplateName(templateName string, templateVersion string, req *state.TemplateExecutionRequest) (state.Run, error)
	UpdateClusterMetadata(cluster state.ClusterMetadata) error
}
type executionService struct {
	stateManager          state.Manager
	eksClusterClient      cluster.Client
	eksExecutionEngine    engine.Engine
	emrExecutionEngine    engine.Engine
	reservedEnv           map[string]func(run state.Run) string
	eksClusterOverride    string
	eksClusterDefault     string
	eksGPUClusterOverride string
	eksGPUClusterDefault  string
	checkImageValidity    bool
	baseUri               string
	spotReAttemptOverride float32
	eksSpotOverride       bool
	spotThresholdMinutes  float64
	terminateJobChannel   chan state.TerminateJob
	validEksClusters      []string
}
func (es *executionService) GetEvents(run state.Run) (state.PodEventList, error) {
	return es.eksExecutionEngine.GetEvents(run)
}
func NewExecutionService(conf config.Config, eksExecutionEngine engine.Engine, sm state.Manager, eksClusterClient cluster.Client, emrExecutionEngine engine.Engine) (ExecutionService, error) {
	es := executionService{
		stateManager:       sm,
		eksClusterClient:   eksClusterClient,
		eksExecutionEngine: eksExecutionEngine,
		emrExecutionEngine: emrExecutionEngine,
	}
	ownerKey := conf.GetString("owner_id_var")
	if len(ownerKey) == 0 {
		ownerKey = "FLOTILLA_RUN_OWNER_ID"
	}
	es.validEksClusters = strings.Split(conf.GetString("eks_clusters"), ",")
	for k, _ := range es.validEksClusters {
		es.validEksClusters[k] = strings.TrimSpace(es.validEksClusters[k])
	}
	dbClusters, _ := es.stateManager.ListClusterStates()
	for _, cluster := range dbClusters {
		es.validEksClusters = append(es.validEksClusters, cluster.Name)
	}
	es.eksClusterOverride = conf.GetString("eks_cluster_override")
	es.eksGPUClusterOverride = conf.GetString("eks_gpu_cluster_override")
	es.eksClusterDefault = conf.GetString("eks_cluster_default")
	es.eksGPUClusterDefault = conf.GetString("eks_gpu_cluster_default")
	if !slices.Contains(es.validEksClusters, es.eksClusterDefault) || !slices.Contains(es.validEksClusters, es.eksGPUClusterDefault) {
		return nil, fmt.Errorf("an invalid cluster has been set as a default\nvalid_clusters:%s\neks_cluster_default:%s\neks_gpu_cluster_default:%s", es.validEksClusters, es.eksClusterDefault, es.eksGPUClusterDefault)
	}
	if conf.IsSet("check_image_validity") {
		es.checkImageValidity = conf.GetBool("check_image_validity")
	} else {
		es.checkImageValidity = true
	}
	if conf.IsSet("base_uri") {
		es.baseUri = conf.GetString("base_uri")
	}
	if conf.IsSet("eks_spot_reattempt_override") {
		es.spotReAttemptOverride = float32(conf.GetFloat64("eks_spot_reattempt_override"))
	} else {
		es.spotReAttemptOverride = float32(0.05)
	}
	if conf.IsSet("eks_spot_override") {
		es.eksSpotOverride = conf.GetBool("eks_spot_override")
	} else {
		es.eksSpotOverride = false
	}
	if conf.IsSet("eks_spot_threshold_minutes") {
		es.spotThresholdMinutes = conf.GetFloat64("eks_spot_threshold_minutes")
	} else {
		es.spotThresholdMinutes = 30.0
	}
	es.reservedEnv = map[string]func(run state.Run) string{
		"FLOTILLA_SERVER_MODE": func(run state.Run) string {
			return conf.GetString("flotilla_mode")
		},
		"FLOTILLA_RUN_ID": func(run state.Run) string {
			return run.RunID
		},
		"AWS_ROLE_SESSION_NAME": func(run state.Run) string {
			return run.RunID
		},
		ownerKey: func(run state.Run) string {
			return run.User
		},
	}
	es.terminateJobChannel = make(chan state.TerminateJob, 100)
	return &es, nil
}
func (es *executionService) ReservedVariables() []string {
	var keys []string
	for k := range es.reservedEnv {
		keys = append(keys, k)
	}
	return keys
}
func (es *executionService) CreateDefinitionRunByDefinitionID(definitionID string, req *state.DefinitionExecutionRequest) (state.Run, error) {
	definition, err := es.stateManager.GetDefinition(definitionID)
	if err != nil {
		return state.Run{}, err
	}
	return es.createFromDefinition(definition, req)
}
func (es *executionService) CreateDefinitionRunByAlias(alias string, req *state.DefinitionExecutionRequest) (state.Run, error) {
	definition, err := es.stateManager.GetDefinitionByAlias(alias)
	if err != nil {
		return state.Run{}, err
	}
	return es.createFromDefinition(definition, req)
}
func (es *executionService) createFromDefinition(definition state.Definition, req *state.DefinitionExecutionRequest) (state.Run, error) {
	var (
		run state.Run
		err error
	)
	fields := req.GetExecutionRequestCommon()
	rand.Seed(time.Now().Unix())
	if definition.TargetCluster != "" {
		fields.ClusterName = definition.TargetCluster
	}
	if req.ClusterName != "" {
		if es.isClusterValid(req.ClusterName) {
			fields.ClusterName = req.ClusterName
		} else {
			return run, fmt.Errorf("%s was not found in the list of valid clusters: %s", fields.ClusterName, es.validEksClusters)
		}
	}
	run.User = req.OwnerID
	es.sanitizeExecutionRequestCommonFields(fields)
	run, err = es.constructRunFromDefinition(definition, req)
	if err != nil {
		return run, err
	}
	return es.createAndEnqueueRun(run)
}
func (es *executionService) constructRunFromDefinition(definition state.Definition, req *state.DefinitionExecutionRequest) (state.Run, error) {
	run, err := es.constructBaseRunFromExecutable(definition, req)
	if err != nil {
		return run, err
	}
	run.DefinitionID = definition.DefinitionID
	run.Alias = definition.Alias
	queuedAt := time.Now()
	run.QueuedAt = &queuedAt
	run.GroupName = definition.GroupName
	run.RequiresDocker = definition.RequiresDocker
	if req.Description != nil {
		run.Description = req.Description
	}
	if req.IdempotenceKey != nil {
		run.IdempotenceKey = req.IdempotenceKey
	}
	if req.Arch != nil {
		run.Arch = req.Arch
	}
	if req.Labels != nil {
		run.Labels = *req.Labels
	}
	return run, nil
}
func (es *executionService) constructBaseRunFromExecutable(executable state.Executable, req state.ExecutionRequest) (state.Run, error) {
	resources := executable.GetExecutableResources()
	fields := req.GetExecutionRequestCommon()
	var (
		run state.Run
		err error
	)
	fields.Engine = req.GetExecutionRequestCommon().Engine
	runID, err := state.NewRunID(fields.Engine)
	if err != nil {
		return run, err
	}
	if *fields.Engine == state.EKSEngine {
		executableCmd, err := executable.GetExecutableCommand(req)
		if err != nil {
			return run, err
		}
		if (fields.Command == nil || len(*fields.Command) == 0) && (len(executableCmd) > 0) {
			fields.Command = aws.String(executableCmd)
		}
		executableID := executable.GetExecutableID()
		taskExecutionMinutes, _ := es.stateManager.GetTaskHistoricalRuntime(*executableID, runID)
		reAttemptRate, _ := es.stateManager.GetPodReAttemptRate()
		if reAttemptRate >= es.spotReAttemptOverride &&
			fields.Engine != nil &&
			fields.NodeLifecycle != nil &&
			*fields.Engine == state.EKSEngine &&
			*fields.NodeLifecycle == state.SpotLifecycle {
			fields.NodeLifecycle = &state.OndemandLifecycle
		}
		if taskExecutionMinutes > float32(es.spotThresholdMinutes) {
			fields.NodeLifecycle = &state.OndemandLifecycle
		}
	}
	if *fields.Engine == state.EKSSparkEngine {
		if req.GetExecutionRequestCommon().SparkExtension == nil {
			return run, errors.New("spark_extension can't be nil, when using eks-spark engine type")
		}
		fields.SparkExtension = req.GetExecutionRequestCommon().SparkExtension
		reAttemptRate, _ := es.stateManager.GetPodReAttemptRate()
		if reAttemptRate >= es.spotReAttemptOverride {
			fields.NodeLifecycle = &state.OndemandLifecycle
		}
	}
	if fields.NodeLifecycle == nil {
		fields.NodeLifecycle = &state.SpotLifecycle
	}
	run = state.Run{
		RunID:                 runID,
		ClusterName:           fields.ClusterName,
		Image:                 resources.Image,
		Status:                state.StatusQueued,
		User:                  fields.OwnerID,
		Command:               fields.Command,
		Memory:                fields.Memory,
		Cpu:                   fields.Cpu,
		Gpu:                   fields.Gpu,
		Engine:                fields.Engine,
		NodeLifecycle:         fields.NodeLifecycle,
		EphemeralStorage:      fields.EphemeralStorage,
		ExecutableID:          executable.GetExecutableID(),
		ExecutableType:        executable.GetExecutableType(),
		ActiveDeadlineSeconds: fields.ActiveDeadlineSeconds,
		TaskType:              state.DefaultTaskType,
		SparkExtension:        fields.SparkExtension,
		CommandHash:           fields.CommandHash,
		ServiceAccount:        fields.ServiceAccount,
	}
	if fields.Labels != nil {
		run.Labels = *fields.Labels
	}
	runEnv := es.constructEnviron(run, fields.Env)
	run.Env = &runEnv
	return run, nil
}
func (es *executionService) constructEnviron(run state.Run, env *state.EnvList) state.EnvList {
	size := len(es.reservedEnv)
	if env != nil {
		size += len(*env)
	}
	runEnv := make([]state.EnvVar, size)
	i := 0
	for k, f := range es.reservedEnv {
		runEnv[i] = state.EnvVar{
			Name:  k,
			Value: f(run),
		}
		i++
	}
	if env != nil {
		for j, e := range *env {
			runEnv[i+j] = e
		}
	}
	return state.EnvList(runEnv)
}
func (es *executionService) List(
	limit int,
	offset int,
	sortOrder string,
	sortField string,
	filters map[string][]string,
	envFilters map[string]string) (state.RunList, error) {
	definitionID, ok := filters["definition_id"]
	if ok {
		_, err := es.stateManager.GetDefinition(definitionID[0])
		if err != nil {
			return state.RunList{}, err
		}
	}
	if statusFilters, ok := filters["status"]; ok {
		for _, status := range statusFilters {
			if !state.IsValidStatus(status) {
				err := exceptions.MalformedInput{
					ErrorString: fmt.Sprintf("invalid status [%s]", status)}
				return state.RunList{}, err
			}
		}
	}
	return es.stateManager.ListRuns(limit, offset, sortField, sortOrder, filters, envFilters, []string{state.EKSEngine, state.EKSSparkEngine})
}
func (es *executionService) Get(runID string) (state.Run, error) {
	return es.stateManager.GetRun(runID)
}
func (es *executionService) UpdateStatus(runID string, status string, exitCode *int64, runExceptions *state.RunExceptions, exitReason *string) error {
	if !state.IsValidStatus(status) {
		return exceptions.MalformedInput{ErrorString: fmt.Sprintf("status %s is invalid", status)}
	}
	run, err := es.stateManager.GetRun(runID)
	if err != nil {
		return err
	}
	var startedAt *time.Time
	if run.StartedAt == nil {
		startedAt = run.QueuedAt
	} else {
		startedAt = run.StartedAt
	}
	finishedAt := time.Now()
	if exitReason == nil {
		extractedExitReason := es.extractExitReason(runExceptions)
		exitReason = &extractedExitReason
	}
	_, err = es.stateManager.UpdateRun(runID, state.Run{Status: status, ExitCode: exitCode, ExitReason: exitReason, RunExceptions: runExceptions, FinishedAt: &finishedAt, StartedAt: startedAt})
	return err
}
func (es *executionService) extractExitReason(runExceptions *state.RunExceptions) string {
	connectionError := regexp.MustCompile(`(?i).*(timeout|gatewayerror|socketerror|\s503\s|\s502\s|\s500\s|\s504\s|connectionerror).*`)
	pipError := regexp.MustCompile(`(?i).*(could\snot\sfind\sa\sversion|package\snot\sfound|ModuleNotFoundError|No\smatching\sdistribution\sfound).*`)
	yumError := regexp.MustCompile(`(?i).*(Nothing\sto\sdo).*`)
	gitError := regexp.MustCompile(`(?i).*(Could\snot\sread\sfrom\sremote\srepository|correct\saccess\srights|Repository\snot\sfound).*`)
	argumentError := regexp.MustCompile(`(?i).*(404|400|keyerror|column\smissing|RuntimeError).*`)
	syntaxError := regexp.MustCompile(`(?i).*(syntaxerror|typeerror|).*`)
	value, _ := json.Marshal(runExceptions)
	if value != nil {
		errorMsg := string(value)
		switch {
		case connectionError.MatchString(errorMsg):
			return "Connection error to downstream uri"
		case pipError.MatchString(errorMsg):
			return "Python pip package installation error"
		case yumError.MatchString(errorMsg):
			return "Yum installation error"
		case gitError.MatchString(errorMsg):
			return "Git clone error"
		case argumentError.MatchString(errorMsg):
			return "Data or argument error"
		case syntaxError.MatchString(errorMsg):
			return "Code or syntax error"
		default:
			return "Runtime exception encountered"
		}
	}
	return "Runtime exception encountered"
}
func (es *executionService) terminateWorker(jobChan <-chan state.TerminateJob) {
	for job := range jobChan {
		runID := job.RunID
		userInfo := job.UserInfo
		run, err := es.stateManager.GetRun(runID)
		if err != nil {
			break
		}
		subRuns, err := es.stateManager.ListRuns(1000, 0, "status", "desc", nil, map[string]string{"PARENT_FLOTILLA_RUN_ID": run.RunID}, state.Engines)
		if err == nil && subRuns.Total > 0 {
			for _, subRun := range subRuns.Runs {
				es.terminateJobChannel <- state.TerminateJob{
					RunID:    subRun.RunID,
					UserInfo: job.UserInfo,
				}
			}
		}
		if run.Engine == nil {
			run.Engine = &state.EKSEngine
		}
		if run.Status != state.StatusStopped {
			if *run.Engine == state.EKSSparkEngine {
				err = es.emrExecutionEngine.Terminate(run)
			} else {
				err = es.eksExecutionEngine.Terminate(run)
			}
			exitReason := "Task terminated by user"
			if len(userInfo.Email) > 0 {
				exitReason = fmt.Sprintf("Task terminated by - %s", userInfo.Email)
			}
			exitCode := int64(1)
			finishedAt := time.Now()
			_, err = es.stateManager.UpdateRun(run.RunID, state.Run{
				Status:     state.StatusStopped,
				ExitReason: &exitReason,
				ExitCode:   &exitCode,
				FinishedAt: &finishedAt,
			})
			break
		}
		break
	}
}
func (es *executionService) Terminate(runID string, userInfo state.UserInfo) error {
	es.terminateJobChannel <- state.TerminateJob{RunID: runID, UserInfo: userInfo}
	go es.terminateWorker(es.terminateJobChannel)
	return nil
}
func (es *executionService) ListClusters() ([]state.ClusterMetadata, error) {
	clusters, err := es.stateManager.ListClusterStates()
	if err != nil {
		return nil, err
	}
	return clusters, nil
}
func (es *executionService) GetDefaultCluster() string {
	return es.eksClusterDefault
}
func (es *executionService) sanitizeExecutionRequestCommonFields(fields *state.ExecutionRequestCommon) {
	if fields.Engine == nil {
		fields.Engine = &state.EKSEngine
	}
	if es.eksSpotOverride {
		fields.NodeLifecycle = &state.OndemandLifecycle
	}
	if fields.ActiveDeadlineSeconds == nil {
		if fields.NodeLifecycle == &state.OndemandLifecycle {
			fields.ActiveDeadlineSeconds = &state.OndemandActiveDeadlineSeconds
		} else {
			fields.ActiveDeadlineSeconds = &state.SpotActiveDeadlineSeconds
		}
	}
}
func (es *executionService) createAndEnqueueRun(run state.Run) (state.Run, error) {
	var err error
	if run.IdempotenceKey != nil {
		priorRunId, err := es.stateManager.CheckIdempotenceKey(*run.IdempotenceKey)
		if err == nil && len(priorRunId) > 0 {
			priorRun, err := es.Get(priorRunId)
			if err == nil {
				return priorRun, nil
			}
		}
	}
	if err = es.stateManager.CreateRun(run); err != nil {
		return run, err
	}
	if *run.Engine == state.EKSEngine {
		err = es.eksExecutionEngine.Enqueue(run)
	} else {
		err = es.emrExecutionEngine.Enqueue(run)
	}
	queuedAt := time.Now()
	if err != nil {
		return run, err
	}
	if run, err = es.stateManager.UpdateRun(run.RunID, state.Run{QueuedAt: &queuedAt}); err != nil {
		return run, err
	}
	return run, nil
}
func (es *executionService) CreateTemplateRunByTemplateName(templateName string, templateVersion string, req *state.TemplateExecutionRequest) (state.Run, error) {
	version, err := strconv.Atoi(templateVersion)
	if err != nil {
		fetch, template, err := es.stateManager.GetLatestTemplateByTemplateName(templateName)
		if fetch && err == nil {
			return es.CreateTemplateRunByTemplateID(template.TemplateID, req)
		}
	} else {
		fetch, template, err := es.stateManager.GetTemplateByVersion(templateName, int64(version))
		if fetch && err == nil {
			return es.CreateTemplateRunByTemplateID(template.TemplateID, req)
		}
	}
	return state.Run{},
		errors.New(fmt.Sprintf("invalid template name or version, template_name: %s, template_version: %s", templateName, templateVersion))
}
func (es *executionService) CreateTemplateRunByTemplateID(templateID string, req *state.TemplateExecutionRequest) (state.Run, error) {
	template, err := es.stateManager.GetTemplateByID(templateID)
	if err != nil {
		return state.Run{}, err
	}
	return es.createFromTemplate(template, req)
}
func (es *executionService) createFromTemplate(template state.Template, req *state.TemplateExecutionRequest) (state.Run, error) {
	var (
		run state.Run
		err error
	)
	fields := req.GetExecutionRequestCommon()
	es.sanitizeExecutionRequestCommonFields(fields)
	run, err = es.constructRunFromTemplate(template, req)
	if err != nil {
		return run, err
	}
	if !req.DryRun {
		return es.createAndEnqueueRun(run)
	}
	return run, nil
}
func (es *executionService) constructRunFromTemplate(template state.Template, req *state.TemplateExecutionRequest) (state.Run, error) {
	run, err := es.constructBaseRunFromExecutable(template, req)
	if err != nil {
		return run, err
	}
	run.DefinitionID = template.TemplateID
	run.Alias = template.TemplateID
	run.GroupName = "template_group_name"
	run.ExecutionRequestCustom = req.GetExecutionRequestCustom()
	return run, nil
}
func (es *executionService) isClusterValid(clusterName string) bool {
	return slices.Contains(es.validEksClusters, clusterName)
}
func (es *executionService) UpdateClusterMetadata(cluster state.ClusterMetadata) error {
	if !es.isClusterValid(cluster.Name) {
		return fmt.Errorf("cluster %s not found in the list of valid clusters", cluster.Name)
	}
	return es.stateManager.UpdateClusterMetadata(cluster)
}

================
File: services/logs.go
================
package services
import (
	"github.com/aws/aws-sdk-go/aws"
	"github.com/stitchfix/flotilla-os/clients/logs"
	"github.com/stitchfix/flotilla-os/state"
	"net/http"
)
type LogService interface {
	Logs(runID string, lastSeen *string, role *string, facility *string) (string, *string, error)
	LogsText(runID string, w http.ResponseWriter) error
}
type logService struct {
	sm state.Manager
	lc logs.Client
}
func NewLogService(sm state.Manager, lc logs.Client) (LogService, error) {
	return &logService{sm: sm, lc: lc}, nil
}
func (ls *logService) Logs(runID string, lastSeen *string, role *string, facility *string) (string, *string, error) {
	run, err := ls.sm.GetRun(runID)
	if err != nil {
		return "", nil, err
	}
	if run.Status != state.StatusRunning && run.Status != state.StatusStopped {
		// Won't have logs yet
		return "", aws.String(""), nil
	}
	if run.ExecutableType == nil {
		defaultExecutableType := state.ExecutableTypeDefinition
		run.ExecutableType = &defaultExecutableType
	}
	if run.ExecutableID == nil {
		run.ExecutableID = &run.DefinitionID
	}
	executable, err := ls.sm.GetExecutableByTypeAndID(*run.ExecutableType, *run.ExecutableID)
	return ls.lc.Logs(executable, run, lastSeen, role, facility)
}
func (ls *logService) LogsText(runID string, w http.ResponseWriter) error {
	run, err := ls.sm.GetRun(runID)
	if err != nil {
		return err
	}
	if run.Status != state.StatusRunning && run.Status != state.StatusStopped {
		return nil
	}
	if run.ExecutableType == nil {
		defaultExecutableType := state.ExecutableTypeDefinition
		run.ExecutableType = &defaultExecutableType
	}
	if run.ExecutableID == nil {
		run.ExecutableID = &run.DefinitionID
	}
	executable, err := ls.sm.GetExecutableByTypeAndID(*run.ExecutableType, *run.ExecutableID)
	return ls.lc.LogsText(executable, run, w)
}

================
File: services/template.go
================
package services
import (
	"reflect"
	"strings"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/exceptions"
	"github.com/stitchfix/flotilla-os/state"
)
type TemplateService interface {
	GetByID(id string) (state.Template, error)
	GetLatestByName(templateName string) (bool, state.Template, error)
	List(limit int, offset int, sortBy string, order string) (state.TemplateList, error)
	ListLatestOnly(limit int, offset int, sortBy string, order string) (state.TemplateList, error)
	Create(tpl *state.CreateTemplateRequest) (state.CreateTemplateResponse, error)
}
type templateService struct {
	sm state.Manager
}
func NewTemplateService(conf config.Config, sm state.Manager) (TemplateService, error) {
	ts := templateService{sm: sm}
	return &ts, nil
}
func (ts *templateService) Create(req *state.CreateTemplateRequest) (state.CreateTemplateResponse, error) {
	res := state.CreateTemplateResponse{
		DidCreate: false,
		Template:  state.Template{},
	}
	curr, err := ts.constructTemplateFromCreateTemplateRequest(req)
	if valid, reasons := curr.IsValid(); !valid {
		return res, exceptions.MalformedInput{ErrorString: strings.Join(reasons, "\n")}
	}
	templateID, err := state.NewTemplateID(curr)
	if err != nil {
		return res, err
	}
	curr.TemplateID = templateID
	doesExist, prev, err := ts.sm.GetLatestTemplateByTemplateName(curr.TemplateName)
	if err != nil {
		return res, err
	}
	if doesExist == false {
		curr.Version = 1
		res.Template = curr
		res.DidCreate = true
		return res, ts.sm.CreateTemplate(curr)
	}
	if ts.diff(prev, curr) == true {
		curr.Version = prev.Version + 1
		res.Template = curr
		res.DidCreate = true
		return res, ts.sm.CreateTemplate(curr)
	}
	res.Template = prev
	return res, nil
}
func (ts *templateService) GetByID(id string) (state.Template, error) {
	return ts.sm.GetTemplateByID(id)
}
func (ts *templateService) GetLatestByName(templateName string) (bool, state.Template, error) {
	return ts.sm.GetLatestTemplateByTemplateName(templateName)
}
func (ts *templateService) List(limit int, offset int, sortBy string, order string) (state.TemplateList, error) {
	return ts.sm.ListTemplates(limit, offset, sortBy, order)
}
func (ts *templateService) ListLatestOnly(limit int, offset int, sortBy string, order string) (state.TemplateList, error) {
	return ts.sm.ListTemplatesLatestOnly(limit, offset, sortBy, order)
}
func (ts *templateService) diff(prev state.Template, curr state.Template) bool {
	if prev.TemplateName != curr.TemplateName {
		return true
	}
	if prev.CommandTemplate != curr.CommandTemplate {
		return true
	}
	if prev.Image != curr.Image {
		return true
	}
	if *prev.Memory != *curr.Memory {
		return true
	}
	if *prev.Gpu != *curr.Gpu {
		return true
	}
	if *prev.Cpu != *curr.Cpu {
		return true
	}
	if prev.Env != nil && curr.Env != nil {
		prevEnv := *prev.Env
		currEnv := *curr.Env
		if len(prevEnv) != len(currEnv) {
			return true
		}
		for i, e := range prevEnv {
			if e != currEnv[i] {
				return true
			}
		}
	}
	if *prev.AdaptiveResourceAllocation != *curr.AdaptiveResourceAllocation {
		return true
	}
	if reflect.DeepEqual(prev.Defaults, curr.Defaults) == false {
		return true
	}
	if prev.AvatarURI != curr.AvatarURI {
		return true
	}
	if prev.Ports != nil && curr.Ports != nil {
		prevPorts := *prev.Ports
		currPorts := *curr.Ports
		if len(prevPorts) != len(currPorts) {
			return true
		}
		for i, e := range prevPorts {
			if e != currPorts[i] {
				return true
			}
		}
	}
	if prev.Tags != nil && curr.Tags != nil {
		prevTags := *prev.Tags
		currTags := *curr.Tags
		if len(prevTags) != len(currTags) {
			return true
		}
		for i, e := range prevTags {
			if e != currTags[i] {
				return true
			}
		}
	}
	if reflect.DeepEqual(prev.Schema, curr.Schema) == false {
		return true
	}
	return false
}
func (ts *templateService) constructTemplateFromCreateTemplateRequest(req *state.CreateTemplateRequest) (state.Template, error) {
	tpl := state.Template{}
	if len(req.TemplateName) > 0 {
		tpl.TemplateName = req.TemplateName
	}
	if req.Schema != nil {
		tpl.Schema = req.Schema
	}
	if len(req.CommandTemplate) > 0 {
		tpl.CommandTemplate = req.CommandTemplate
	}
	if len(req.Image) > 0 {
		tpl.Image = req.Image
	}
	if req.Memory != nil {
		tpl.Memory = req.Memory
	} else {
		tpl.Memory = &state.MinMem
	}
	if req.Gpu != nil {
		tpl.Gpu = req.Gpu
	}
	if req.Cpu != nil {
		tpl.Cpu = req.Cpu
	} else {
		tpl.Cpu = &state.MinCPU
	}
	if req.Env != nil {
		tpl.Env = req.Env
	}
	if req.AdaptiveResourceAllocation != nil {
		tpl.AdaptiveResourceAllocation = req.AdaptiveResourceAllocation
	} else {
		*tpl.AdaptiveResourceAllocation = true
	}
	if req.Ports != nil {
		tpl.Ports = req.Ports
	}
	if req.Tags != nil {
		tpl.Tags = req.Tags
	}
	if req.Defaults != nil {
		tpl.Defaults = req.Defaults
	} else {
		tpl.Defaults = state.TemplatePayload{}
	}
	if len(req.AvatarURI) > 0 {
		tpl.AvatarURI = req.AvatarURI
	} else {
		tpl.AvatarURI = ""
	}
	return tpl, nil
}

================
File: services/worker.go
================
package services
import (
	"fmt"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/exceptions"
	"github.com/stitchfix/flotilla-os/state"
)
type WorkerService interface {
	List(engine string) (state.WorkersList, error)
	Get(workerType string, engine string) (state.Worker, error)
	Update(workerType string, updates state.Worker) (state.Worker, error)
	BatchUpdate(updates []state.Worker) (state.WorkersList, error)
}
type workerService struct {
	sm state.Manager
}
func NewWorkerService(conf config.Config, sm state.Manager) (WorkerService, error) {
	ws := workerService{sm: sm}
	return &ws, nil
}
func (ws *workerService) List(engine string) (state.WorkersList, error) {
	return ws.sm.ListWorkers(engine)
}
func (ws *workerService) Get(workerType string, engine string) (state.Worker, error) {
	var w state.Worker
	if err := ws.validate(workerType); err != nil {
		return w, err
	}
	return ws.sm.GetWorker(workerType, engine)
}
func (ws *workerService) Update(workerType string, updates state.Worker) (state.Worker, error) {
	var w state.Worker
	if err := ws.validate(workerType); err != nil {
		return w, err
	}
	return ws.sm.UpdateWorker(workerType, updates)
}
func (ws *workerService) BatchUpdate(updates []state.Worker) (state.WorkersList, error) {
	var wl state.WorkersList
	for _, update := range updates {
		if err := ws.validate(update.WorkerType); err != nil {
			return wl, err
		}
	}
	return ws.sm.BatchUpdateWorkers(updates)
}
func (ws *workerService) validate(workerType string) error {
	if !state.IsValidWorkerType(workerType) {
		var validTypesList []string
		for validType := range state.WorkerTypes {
			validTypesList = append(validTypesList, validType)
		}
		return exceptions.MalformedInput{
			ErrorString: fmt.Sprintf(
				"Worker type: [%s] is not a valid worker type; valid types: %s",
				workerType, validTypesList)}
	}
	return nil
}

================
File: state/manager.go
================
package state
import (
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/log"
)
type Manager interface {
	Name() string
	Initialize(conf config.Config) error
	Cleanup() error
	ListDefinitions(
		limit int, offset int, sortBy string,
		order string, filters map[string][]string,
		envFilters map[string]string) (DefinitionList, error)
	GetDefinition(definitionID string) (Definition, error)
	GetDefinitionByAlias(alias string) (Definition, error)
	UpdateDefinition(definitionID string, updates Definition) (Definition, error)
	CreateDefinition(d Definition) error
	DeleteDefinition(definitionID string) error
	ListRuns(limit int, offset int, sortBy string, order string, filters map[string][]string, envFilters map[string]string, engines []string) (RunList, error)
	EstimateRunResources(executableID string, commandHash string) (TaskResources, error)
	EstimateExecutorCount(executableID string, commandHash string) (int64, error)
	ExecutorOOM(executableID string, commandHash string) (bool, error)
	DriverOOM(executableID string, commandHash string) (bool, error)
	GetRun(runID string) (Run, error)
	CreateRun(r Run) error
	UpdateRun(runID string, updates Run) (Run, error)
	ListGroups(limit int, offset int, name *string) (GroupsList, error)
	ListTags(limit int, offset int, name *string) (TagsList, error)
	ListWorkers(engine string) (WorkersList, error)
	BatchUpdateWorkers(updates []Worker) (WorkersList, error)
	GetWorker(workerType string, engine string) (Worker, error)
	UpdateWorker(workerType string, updates Worker) (Worker, error)
	GetExecutableByTypeAndID(executableType ExecutableType, executableID string) (Executable, error)
	GetTemplateByID(templateID string) (Template, error)
	GetLatestTemplateByTemplateName(templateName string) (bool, Template, error)
	GetTemplateByVersion(templateName string, templateVersion int64) (bool, Template, error)
	ListTemplates(limit int, offset int, sortBy string, order string) (TemplateList, error)
	ListTemplatesLatestOnly(limit int, offset int, sortBy string, order string) (TemplateList, error)
	CreateTemplate(t Template) error
	ListFailingNodes() (NodeList, error)
	GetPodReAttemptRate() (float32, error)
	GetNodeLifecycle(executableID string, commandHash string) (string, error)
	GetTaskHistoricalRuntime(executableID string, runId string) (float32, error)
	CheckIdempotenceKey(idempotenceKey string) (string, error)
	GetRunByEMRJobId(string) (Run, error)
	ListClusterStates() ([]ClusterMetadata, error)
	UpdateClusterMetadata(cluster ClusterMetadata) error
}
func NewStateManager(conf config.Config, logger log.Logger) (Manager, error) {
	name := "postgres"
	if conf.IsSet("state_manager") {
		name = conf.GetString("state_manager")
	}
	switch name {
	case "postgres":
		pgm := &SQLStateManager{log: logger}
		err := pgm.Initialize(conf)
		if err != nil {
			return nil, errors.Wrap(err, "problem initializing SQLStateManager")
		}
		return pgm, nil
	default:
		return nil, errors.Errorf("state.Manager named [%s] not found", name)
	}
}

================
File: state/models.go
================
package state
import (
	"bytes"
	"encoding/json"
	"fmt"
	"regexp"
	"sort"
	"strconv"
	"strings"
	"text/template"
	"time"
	"github.com/Masterminds/sprig"
	"github.com/aws/aws-sdk-go/aws"
	uuid "github.com/nu7hatch/gouuid"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/utils"
	"github.com/xeipuuv/gojsonschema"
)
var EKSEngine = "eks"
var EKSSparkEngine = "eks-spark"
var DefaultEngine = EKSEngine
var DefaultTaskType = "task"
var MinCPU = int64(256)
var MaxCPU = int64(60000)
var MaxGPUCPU = int64(94000)
var MinMem = int64(512)
var MaxMem = int64(350000)
var MaxGPUMem = int64(376000)
var MaxEphemeralStorage = int64(5000)
var TTLSecondsAfterFinished = int32(3600)
var SpotActiveDeadlineSeconds = int64(172800)
var OndemandActiveDeadlineSeconds = int64(604800)
var SpotLifecycle = "spot"
var OndemandLifecycle = "ondemand"
var DefaultLifecycle = SpotLifecycle
var NodeLifeCycles = []string{OndemandLifecycle, SpotLifecycle}
var Engines = []string{EKSEngine, EKSSparkEngine}
var StatusRunning = "RUNNING"
var StatusQueued = "QUEUED"
var StatusNeedsRetry = "NEEDS_RETRY"
var StatusPending = "PENDING"
var StatusStopped = "STOPPED"
var MaxLogLines = int64(256)
var EKSBackoffLimit = int32(0)
var GPUNodeTypes = []string{"p3.2xlarge", "p3.8xlarge", "p3.16xlarge", "g5.xlarge", "g5.2xlarge", "g5.4xlarge", "g5.8xlarge", "g5.12xlarge", "g5.16xlarge", "g5.24xlarge", "g5.48xlarge"}
var WorkerTypes = map[string]bool{
	"retry":  true,
	"submit": true,
	"status": true,
}
func IsValidWorkerType(workerType string) bool {
	return WorkerTypes[workerType]
}
func IsValidStatus(status string) bool {
	return status == StatusRunning ||
		status == StatusQueued ||
		status == StatusNeedsRetry ||
		status == StatusPending ||
		status == StatusStopped
}
func NewRunID(engine *string) (string, error) {
	s, err := newUUIDv4()
	return fmt.Sprintf("%s-%s", *engine, s[len(*engine)+1:]), err
}
func NewDefinitionID(definition Definition) (string, error) {
	uuid4, err := newUUIDv4()
	if err != nil {
		return "", err
	}
	return fmt.Sprintf("%s-%s", definition.GroupName, uuid4), nil
}
func newUUIDv4() (string, error) {
	u, err := uuid.NewV4()
	if err != nil {
		return "", err
	}
	return u.String(), nil
}
// EnvList wraps a list of EnvVar
//   - abstraction to make it easier to read
//     and write to db
type EnvList []EnvVar
// PortsList wraps a list of int
//   - abstraction to make it easier to read
//     and write to db
type PortsList []int
// EnvVar represents a single environment variable
// for either a definition or a run
type EnvVar struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}
type NodeList []string
// Tags wraps a list of strings
//   - abstraction to make it easier to read
//     and write to db
type Tags []string
// ExecutableResources define the resources and flags required to run an
// executable.
type ExecutableResources struct {
	Image                      string     `json:"image"`
	Memory                     *int64     `json:"memory,omitempty"`
	Gpu                        *int64     `json:"gpu,omitempty"`
	Cpu                        *int64     `json:"cpu,omitempty"`
	EphemeralStorage           *int64     `json:"ephemeral_storage,omitempty" db:"ephemeral_storage"`
	Env                        *EnvList   `json:"env"`
	AdaptiveResourceAllocation *bool      `json:"adaptive_resource_allocation,omitempty"`
	Ports                      *PortsList `json:"ports,omitempty"`
	Tags                       *Tags      `json:"tags,omitempty"`
}
type ExecutableType string
const (
	ExecutableTypeDefinition ExecutableType = "task_definition"
	ExecutableTypeTemplate   ExecutableType = "template"
)
type Executable interface {
	GetExecutableID() *string
	GetExecutableType() *ExecutableType
	GetExecutableResources() *ExecutableResources
	GetExecutableCommand(req ExecutionRequest) (string, error)
	GetExecutableResourceName() string // This will typically be an ARN.
}
func UnmarshalSparkExtension(data []byte) (SparkExtension, error) {
	var r SparkExtension
	err := json.Unmarshal(data, &r)
	return r, err
}
func (r *SparkExtension) Marshal() ([]byte, error) {
	return json.Marshal(r)
}
type SparkExtension struct {
	SparkSubmitJobDriver *SparkSubmitJobDriver `json:"spark_submit_job_driver,omitempty"`
	ApplicationConf      []Conf                `json:"application_conf,omitempty"`
	HiveConf             []Conf                `json:"hive_conf,omitempty"`
	EMRJobId             *string               `json:"emr_job_id,omitempty"`
	SparkAppId           *string               `json:"spark_app_id,omitempty"`
	EMRJobManifest       *string               `json:"emr_job_manifest,omitempty"`
	HistoryUri           *string               `json:"history_uri,omitempty"`
	MetricsUri           *string               `json:"metrics_uri,omitempty"`
	VirtualClusterId     *string               `json:"virtual_cluster_id,omitempty"`
	EMRReleaseLabel      *string               `json:"emr_release_label,omitempty"`
	ExecutorInitCommand  *string               `json:"executor_init_command,omitempty"`
	DriverInitCommand    *string               `json:"driver_init_command,omitempty"`
	AppUri               *string               `json:"app_uri,omitempty"`
	Executors            []string              `json:"executors,omitempty"`
	ExecutorOOM          *bool                 `json:"executor_oom,omitempty"`
	DriverOOM            *bool                 `json:"driver_oom,omitempty"`
}
type Conf struct {
	Name  *string `json:"name,omitempty"`
	Value *string `json:"value,omitempty"`
}
type SparkSubmitJobDriver struct {
	EntryPoint          *string   `json:"entry_point,omitempty"`
	EntryPointArguments []*string `json:"entry_point_arguments,omitempty"`
	SparkSubmitConf     []Conf    `json:"spark_submit_conf,omitempty"`
	Files               []string  `json:"files,omitempty"`
	PyFiles             []string  `json:"py_files,omitempty"`
	Jars                []string  `json:"jars,omitempty"`
	Class               *string   `json:"class,omitempty"`
	WorkingDir          *string   `json:"working_dir,omitempty"`
	NumExecutors        *int64    `json:"num_executors,omitempty"`
	ExecutorMemory      *int64    `json:"executor_memory,omitempty"`
}
type Labels map[string]string
// Common fields required to execute any Executable.
type ExecutionRequestCommon struct {
	ClusterName           string          `json:"cluster_name"`
	Tier                  Tier            `json:"tier"`
	Env                   *EnvList        `json:"env"`
	OwnerID               string          `json:"owner_id"`
	Command               *string         `json:"command"`
	Memory                *int64          `json:"memory"`
	Cpu                   *int64          `json:"cpu"`
	Gpu                   *int64          `json:"gpu"`
	Engine                *string         `json:"engine"`
	EphemeralStorage      *int64          `json:"ephemeral_storage"`
	NodeLifecycle         *string         `json:"node_lifecycle"`
	ActiveDeadlineSeconds *int64          `json:"active_deadline_seconds,omitempty"`
	SparkExtension        *SparkExtension `json:"spark_extension,omitempty"`
	Description           *string         `json:"description,omitempty"`
	CommandHash           *string         `json:"command_hash,omitempty"`
	IdempotenceKey        *string         `json:"idempotence_key,omitempty"`
	Arch                  *string         `json:"arch,omitempty"`
	Labels                *Labels         `json:"labels,omitempty"`
	ServiceAccount        *string         `json:"service_account,omitempty"`
}
type ExecutionRequestCustom map[string]interface{}
type ExecutionRequest interface {
	GetExecutionRequestCommon() *ExecutionRequestCommon
	GetExecutionRequestCustom() *ExecutionRequestCustom
}
type DefinitionExecutionRequest struct {
	*ExecutionRequestCommon
}
// Returns ExecutionRequestCommon, common between Template and Definition types
func (d *DefinitionExecutionRequest) GetExecutionRequestCommon() *ExecutionRequestCommon {
	return d.ExecutionRequestCommon
}
// Only relevant to the template type
func (d *DefinitionExecutionRequest) GetExecutionRequestCustom() *ExecutionRequestCustom {
	return nil
}
type TerminateJob struct {
	RunID    string
	UserInfo UserInfo
}
// task definition. It implements the `Executable` interface.
type Definition struct {
	DefinitionID   string `json:"definition_id"`
	GroupName      string `json:"group_name,omitempty"`
	Alias          string `json:"alias"`
	Command        string `json:"command,omitempty"`
	TaskType       string `json:"task_type,omitempty"`
	RequiresDocker bool   `json:"requires_docker,omitempty" db:"requires_docker"`
	TargetCluster  string `json:"target_cluster,omitempty" db:"target_cluster"`
	ExecutableResources
}
func (d Definition) GetExecutableID() *string {
	return &d.DefinitionID
}
func (d Definition) GetExecutableType() *ExecutableType {
	t := ExecutableTypeDefinition
	return &t
}
func (d Definition) GetExecutableResources() *ExecutableResources {
	return &d.ExecutableResources
}
func (d Definition) GetExecutableCommand(req ExecutionRequest) (string, error) {
	return d.Command, nil
}
func (d Definition) GetExecutableResourceName() string {
	return d.DefinitionID
}
var commandWrapper = `
set -e
set -x
{{.Command}}
`
var CommandTemplate, _ = template.New("command").Parse(commandWrapper)
func (d *Definition) WrappedCommand() (string, error) {
	var result bytes.Buffer
	if err := CommandTemplate.Execute(&result, d); err != nil {
		return "", err
	}
	return result.String(), nil
}
type validationCondition struct {
	condition bool
	reason    string
}
// IsValid returns true only if this is a valid definition with all
// required information
func (d *Definition) IsValid() (bool, []string) {
	conditions := []validationCondition{
		{len(d.Image) == 0, "string [image] must be specified"},
		{len(d.Alias) == 0, "string [alias] must be specified"},
	}
	valid := true
	var reasons []string
	for _, cond := range conditions {
		if cond.condition {
			valid = false
			reasons = append(reasons, cond.reason)
		}
	}
	return valid, reasons
}
func (d *Definition) UpdateWith(other Definition) {
	if len(other.DefinitionID) > 0 {
		d.DefinitionID = other.DefinitionID
	}
	if len(other.Image) > 0 {
		d.Image = other.Image
	}
	if len(other.GroupName) > 0 {
		d.GroupName = other.GroupName
	}
	if len(other.Alias) > 0 {
		d.Alias = other.Alias
	}
	if other.Memory != nil {
		d.Memory = other.Memory
	}
	if other.Gpu != nil {
		d.Gpu = other.Gpu
	}
	if other.Cpu != nil {
		d.Cpu = other.Cpu
	}
	if other.EphemeralStorage != nil {
		d.EphemeralStorage = other.EphemeralStorage
	}
	if other.AdaptiveResourceAllocation != nil {
		d.AdaptiveResourceAllocation = other.AdaptiveResourceAllocation
	}
	if len(other.Command) > 0 {
		d.Command = other.Command
	}
	if len(other.TaskType) > 0 {
		d.TaskType = other.TaskType
	}
	if other.Env != nil {
		d.Env = other.Env
	}
	if other.Ports != nil {
		d.Ports = other.Ports
	}
	if other.Tags != nil {
		d.Tags = other.Tags
	}
}
func (d Definition) MarshalJSON() ([]byte, error) {
	type Alias Definition
	env := d.Env
	if env == nil {
		env = &EnvList{}
	}
	return json.Marshal(&struct {
		Env *EnvList `json:"env"`
		Alias
	}{
		Env:   env,
		Alias: (Alias)(d),
	})
}
type DefinitionList struct {
	Total       int          `json:"total"`
	Definitions []Definition `json:"definitions"`
}
func (dl *DefinitionList) MarshalJSON() ([]byte, error) {
	type Alias DefinitionList
	l := dl.Definitions
	if l == nil {
		l = []Definition{}
	}
	return json.Marshal(&struct {
		Definitions []Definition `json:"definitions"`
		*Alias
	}{
		Definitions: l,
		Alias:       (*Alias)(dl),
	})
}
type Run struct {
	RunID                   string                   `json:"run_id"`
	DefinitionID            string                   `json:"definition_id"`
	Alias                   string                   `json:"alias"`
	Image                   string                   `json:"image"`
	ClusterName             string                   `json:"cluster"`
	ExitCode                *int64                   `json:"exit_code,omitempty"`
	Status                  string                   `json:"status"`
	QueuedAt                *time.Time               `json:"queued_at,omitempty"`
	StartedAt               *time.Time               `json:"started_at,omitempty"`
	FinishedAt              *time.Time               `json:"finished_at,omitempty"`
	InstanceID              string                   `json:"-"`
	InstanceDNSName         string                   `json:"-"`
	GroupName               string                   `json:"group_name"`
	User                    string                   `json:"user,omitempty"`
	TaskType                string                   `json:"task_type,omitempty"`
	Env                     *EnvList                 `json:"env,omitempty"`
	Command                 *string                  `json:"command,omitempty"`
	CommandHash             *string                  `json:"command_hash,omitempty"`
	Memory                  *int64                   `json:"memory,omitempty"`
	MemoryLimit             *int64                   `json:"memory_limit,omitempty"`
	Cpu                     *int64                   `json:"cpu,omitempty"`
	CpuLimit                *int64                   `json:"cpu_limit,omitempty"`
	Gpu                     *int64                   `json:"gpu,omitempty"`
	ExitReason              *string                  `json:"exit_reason,omitempty"`
	Engine                  *string                  `json:"engine,omitempty"`
	NodeLifecycle           *string                  `json:"node_lifecycle,omitempty"`
	EphemeralStorage        *int64                   `json:"ephemeral_storage,omitempty" db:"ephemeral_storage"`
	PodName                 *string                  `json:"pod_name,omitempty"`
	Namespace               *string                  `json:"namespace,omitempty"`
	MaxMemoryUsed           *int64                   `json:"max_memory_used,omitempty"`
	MaxCpuUsed              *int64                   `json:"max_cpu_used,omitempty"`
	PodEvents               *PodEvents               `json:"pod_events,omitempty"`
	CloudTrailNotifications *CloudTrailNotifications `json:"cloudtrail_notifications,omitempty"`
	ExecutableID            *string                  `json:"executable_id,omitempty"`
	ExecutableType          *ExecutableType          `json:"executable_type,omitempty"`
	ExecutionRequestCustom  *ExecutionRequestCustom  `json:"execution_request_custom,omitempty"`
	AttemptCount            *int64                   `json:"attempt_count,omitempty"`
	SpawnedRuns             *SpawnedRuns             `json:"spawned_runs,omitempty"`
	RunExceptions           *RunExceptions           `json:"run_exceptions,omitempty"`
	ActiveDeadlineSeconds   *int64                   `json:"active_deadline_seconds,omitempty"`
	SparkExtension          *SparkExtension          `json:"spark_extension,omitempty"`
	MetricsUri              *string                  `json:"metrics_uri,omitempty"`
	Description             *string                  `json:"description,omitempty"`
	IdempotenceKey          *string                  `json:"idempotence_key,omitempty"`
	Arch                    *string                  `json:"arch,omitempty"`
	Labels                  Labels                   `json:"labels,omitempty"`
	RequiresDocker          bool                     `json:"requires_docker,omitempty" db:"requires_docker"`
	ServiceAccount          *string                  `json:"service_account,omitempty" db:"service_account"`
	Tier                    Tier                     `json:"tier,omitempty"`
}
func (d *Run) UpdateWith(other Run) {
	if len(other.RunID) > 0 {
		d.RunID = other.RunID
	}
	if len(other.DefinitionID) > 0 {
		d.DefinitionID = other.DefinitionID
	}
	if other.Tier != "" {
		d.Tier = other.Tier
	}
	if len(other.Alias) > 0 {
		d.Alias = other.Alias
	}
	if len(other.Image) > 0 {
		d.Image = other.Image
	}
	if len(other.ClusterName) > 0 {
		d.ClusterName = other.ClusterName
	}
	if other.ExitCode != nil {
		d.ExitCode = other.ExitCode
	}
	if other.QueuedAt != nil {
		d.QueuedAt = other.QueuedAt
	}
	if other.StartedAt != nil {
		d.StartedAt = other.StartedAt
	}
	if other.FinishedAt != nil {
		d.FinishedAt = other.FinishedAt
	}
	if len(other.InstanceID) > 0 {
		d.InstanceID = other.InstanceID
	}
	if len(other.InstanceDNSName) > 0 {
		d.InstanceDNSName = other.InstanceDNSName
	}
	if len(other.GroupName) > 0 {
		d.GroupName = other.GroupName
	}
	if len(other.User) > 0 {
		d.User = other.User
	}
	if len(other.TaskType) > 0 {
		d.TaskType = other.TaskType
	}
	if other.Env != nil {
		d.Env = other.Env
	}
	if other.ExitReason != nil {
		d.ExitReason = other.ExitReason
	}
	if other.Command != nil && len(*other.Command) > 0 {
		d.Command = other.Command
	}
	if other.CommandHash != nil && len(*other.CommandHash) > 0 {
		d.CommandHash = other.CommandHash
	}
	if other.Memory != nil {
		d.Memory = other.Memory
	}
	if other.Cpu != nil {
		d.Cpu = other.Cpu
	}
	if other.Gpu != nil {
		d.Gpu = other.Gpu
	}
	if other.MaxMemoryUsed != nil {
		d.MaxMemoryUsed = other.MaxMemoryUsed
	}
	if other.MaxCpuUsed != nil {
		d.MaxCpuUsed = other.MaxCpuUsed
	}
	if other.Engine != nil {
		d.Engine = other.Engine
	}
	if other.EphemeralStorage != nil {
		d.EphemeralStorage = other.EphemeralStorage
	}
	if other.NodeLifecycle != nil {
		d.NodeLifecycle = other.NodeLifecycle
	}
	if other.PodName != nil {
		d.PodName = other.PodName
	}
	if other.Namespace != nil {
		d.Namespace = other.Namespace
	}
	if other.PodEvents != nil {
		d.PodEvents = other.PodEvents
	}
	if other.SpawnedRuns != nil {
		d.SpawnedRuns = other.SpawnedRuns
	}
	if other.RunExceptions != nil {
		d.RunExceptions = other.RunExceptions
	}
	if other.ExecutableID != nil {
		d.ExecutableID = other.ExecutableID
	}
	if other.ExecutableType != nil {
		d.ExecutableType = other.ExecutableType
	}
	if other.SparkExtension != nil {
		d.SparkExtension = other.SparkExtension
	}
	if other.CloudTrailNotifications != nil && len((*other.CloudTrailNotifications).Records) > 0 {
		d.CloudTrailNotifications = other.CloudTrailNotifications
	}
	if other.ExecutionRequestCustom != nil {
		d.ExecutionRequestCustom = other.ExecutionRequestCustom
	}
	if other.CpuLimit != nil {
		d.CpuLimit = other.CpuLimit
	}
	if other.MetricsUri != nil {
		d.MetricsUri = other.MetricsUri
	}
	if other.Description != nil {
		d.Description = other.Description
	}
	if other.IdempotenceKey != nil {
		d.IdempotenceKey = other.IdempotenceKey
	}
	if other.Arch != nil {
		d.Arch = other.Arch
	}
	if other.MemoryLimit != nil {
		d.MemoryLimit = other.MemoryLimit
	}
	if other.AttemptCount != nil {
		d.AttemptCount = other.AttemptCount
	}
	if other.Labels != nil {
		d.Labels = other.Labels
	}
	//
	// Runs have a deterministic lifecycle
	//
	// QUEUED --> PENDING --> RUNNING --> STOPPED
	// QUEUED --> PENDING --> NEEDS_RETRY --> QUEUED ...
	// QUEUED --> PENDING --> STOPPED ...
	//
	statusPrecedence := map[string]int{
		StatusNeedsRetry: -1,
		StatusQueued:     0,
		StatusPending:    1,
		StatusRunning:    2,
		StatusStopped:    3,
	}
	if other.Status == StatusNeedsRetry {
		d.Status = StatusNeedsRetry
	} else {
		if runStatus, ok := statusPrecedence[d.Status]; ok {
			if newStatus, ok := statusPrecedence[other.Status]; ok {
				if newStatus > runStatus {
					d.Status = other.Status
				}
			}
		}
	}
}
func removeDuplicateStr(strSlice []string) []string {
	allKeys := make(map[string]bool)
	var list []string
	for _, item := range strSlice {
		if _, value := allKeys[item]; !value {
			allKeys[item] = true
			list = append(list, item)
		}
	}
	return list
}
type byExecutorName []string
func (s byExecutorName) Len() int {
	return len(s)
}
func (s byExecutorName) Key(i int) int {
	r, _ := regexp.Compile("-exec-(\\d+)")
	matches := r.FindStringSubmatch(s[i])
	if matches == nil || len(matches) < 2 {
		return 0
	}
	key, err := strconv.Atoi(matches[1])
	if err != nil {
		return 0
	}
	return key
}
func (s byExecutorName) Swap(i, j int) {
	s[i], s[j] = s[j], s[i]
}
func (s byExecutorName) Less(i, j int) bool {
	return s.Key(i) < s.Key(j)
}
func (r Run) MarshalJSON() ([]byte, error) {
	type Alias Run
	instance := map[string]string{
		"instance_id": r.InstanceID,
		"dns_name":    r.InstanceDNSName,
	}
	podEvents := r.PodEvents
	if podEvents == nil {
		podEvents = &PodEvents{}
	}
	var executors []string
	for _, podEvent := range *podEvents {
		if strings.Contains(podEvent.SourceObject, "-exec-") {
			executors = append(executors, podEvent.SourceObject)
		}
	}
	if executors != nil && len(executors) > 0 && *r.Engine != EKSEngine {
		executors = removeDuplicateStr(executors)
		sort.Sort(byExecutorName(executors))
		r.SparkExtension.Executors = executors
	}
	cloudTrailNotifications := r.CloudTrailNotifications
	if cloudTrailNotifications == nil {
		cloudTrailNotifications = &CloudTrailNotifications{}
	}
	executionRequestCustom := r.ExecutionRequestCustom
	if executionRequestCustom == nil {
		executionRequestCustom = &ExecutionRequestCustom{}
	}
	if r.Description == nil {
		r.Description = aws.String(r.Alias)
	}
	sparkExtension := r.SparkExtension
	if sparkExtension == nil {
		sparkExtension = &SparkExtension{}
	} else {
		if sparkExtension.HiveConf != nil {
			for _, conf := range sparkExtension.HiveConf {
				if conf.Name != nil && strings.Contains(*conf.Name, "ConnectionPassword") {
					conf.Value = aws.String("****")
				}
			}
		}
		if r.Status != StatusStopped && r.SparkExtension.AppUri != nil {
			r.SparkExtension.HistoryUri = r.SparkExtension.AppUri
		}
	}
	return json.Marshal(&struct {
		Instance                map[string]string        `json:"instance"`
		PodEvents               *PodEvents               `json:"pod_events"`
		CloudTrailNotifications *CloudTrailNotifications `json:"cloudtrail_notifications"`
		SparkExtension          *SparkExtension          `json:"spark_extension"`
		Alias
	}{
		Instance:                instance,
		PodEvents:               podEvents,
		CloudTrailNotifications: cloudTrailNotifications,
		SparkExtension:          sparkExtension,
		Alias:                   (Alias)(r),
	})
}
type RunList struct {
	Total int   `json:"total"`
	Runs  []Run `json:"history"`
}
type PodEvents []PodEvent
type PodEventList struct {
	Total     int       `json:"total"`
	PodEvents PodEvents `json:"pod_events"`
}
type SpawnedRun struct {
	RunID string `json:"run_id"`
}
type SpawnedRuns []SpawnedRun
type RunExceptions []string
func (w *PodEvent) Equal(other PodEvent) bool {
	return w.Reason == other.Reason &&
		other.Timestamp != nil &&
		w.Timestamp.Equal(*other.Timestamp) &&
		w.SourceObject == other.SourceObject &&
		w.Message == other.Message &&
		w.EventType == other.EventType
}
type PodEvent struct {
	Timestamp    *time.Time `json:"timestamp,omitempty"`
	EventType    string     `json:"event_type"`
	Reason       string     `json:"reason"`
	SourceObject string     `json:"source_object"`
	Message      string     `json:"message"`
}
type GroupsList struct {
	Groups []string
	Total  int
}
type TagsList struct {
	Tags  []string
	Total int
}
type Worker struct {
	WorkerType       string `json:"worker_type"`
	CountPerInstance int    `json:"count_per_instance"`
	Engine           string `json:"engine"`
}
func (w *Worker) UpdateWith(other Worker) {
	if other.CountPerInstance >= 0 {
		w.CountPerInstance = other.CountPerInstance
	}
}
type WorkersList struct {
	Total   int      `json:"total"`
	Workers []Worker `json:"workers"`
}
type UserInfo struct {
	Name  string `json:"name"`
	Email string `json:"email"`
}
type TaskResources struct {
	Cpu    int64 `json:"cpu"`
	Memory int64 `json:"memory"`
}
type CloudTrailS3File struct {
	S3Bucket    string   `json:"s3Bucket"`
	S3ObjectKey []string `json:"s3ObjectKey"`
	Done        func() error
}
func (e *CloudTrailNotifications) Marshal() ([]byte, error) {
	return json.Marshal(e)
}
type CloudTrailNotifications struct {
	Records []Record `json:"Records"`
}
type Record struct {
	UserIdentity UserIdentity `json:"userIdentity"`
	EventSource  string       `json:"eventSource"`
	EventName    string       `json:"eventName"`
}
type UserIdentity struct {
	Arn string `json:"arn"`
}
func (w *Record) Equal(other Record) bool {
	return w.EventName == other.EventName && w.EventSource == other.EventSource
}
func (w *Record) String() string {
	return fmt.Sprintf("%s-%s", w.EventSource, w.EventName)
}
const TemplatePayloadKey = "template_payload"
type TemplatePayload map[string]interface{}
type TemplateExecutionRequest struct {
	*ExecutionRequestCommon
	TemplatePayload TemplatePayload `json:"template_payload"`
	DryRun          bool            `json:"dry_run,omitempty"`
}
func (t TemplateExecutionRequest) GetExecutionRequestCommon() *ExecutionRequestCommon {
	return t.ExecutionRequestCommon
}
func (t TemplateExecutionRequest) GetExecutionRequestCustom() *ExecutionRequestCustom {
	return &ExecutionRequestCustom{
		TemplatePayloadKey: t.TemplatePayload,
	}
}
type TemplateJSONSchema map[string]interface{}
type Template struct {
	TemplateID      string             `json:"template_id"`
	TemplateName    string             `json:"template_name"`
	Version         int64              `json:"version"`
	Schema          TemplateJSONSchema `json:"schema"`
	CommandTemplate string             `json:"command_template"`
	Defaults        TemplatePayload    `json:"defaults"`
	AvatarURI       string             `json:"avatar_uri"`
	ExecutableResources
}
type CreateTemplateRequest struct {
	TemplateName    string             `json:"template_name"`
	Schema          TemplateJSONSchema `json:"schema"`
	CommandTemplate string             `json:"command_template"`
	Defaults        TemplatePayload    `json:"defaults"`
	AvatarURI       string             `json:"avatar_uri"`
	ExecutableResources
}
type CreateTemplateResponse struct {
	DidCreate bool     `json:"did_create"`
	Template  Template `json:"template,omitempty"`
}
func (t Template) GetExecutableID() *string {
	return &t.TemplateID
}
func (t Template) GetExecutableType() *ExecutableType {
	et := ExecutableTypeTemplate
	return &et
}
func (t Template) GetExecutableResources() *ExecutableResources {
	return &t.ExecutableResources
}
func (t Template) GetExecutableCommand(req ExecutionRequest) (string, error) {
	var (
		err    error
		result bytes.Buffer
	)
	customFields := *req.GetExecutionRequestCustom()
	executionPayload, ok := customFields[TemplatePayloadKey]
	if !ok || executionPayload == nil {
		return "", err
	}
	executionPayload, err = t.compositeUserAndDefaults(executionPayload)
	schemaLoader := gojsonschema.NewGoLoader(t.Schema)
	documentLoader := gojsonschema.NewGoLoader(executionPayload)
	// Perform JSON schema validation to ensure that the request's template
	// payload conforms to the template's JSON schema.
	validationResult, err := gojsonschema.Validate(schemaLoader, documentLoader)
	if err != nil {
		return "", err
	}
	if validationResult != nil && validationResult.Valid() != true {
		var res []string
		for _, resultError := range validationResult.Errors() {
			res = append(res, resultError.String())
		}
		return "", errors.New(strings.Join(res, "\n"))
	}
	textTemplate, err := template.New("command").Funcs(sprig.TxtFuncMap()).Parse(t.CommandTemplate)
	if err != nil {
		return "", err
	}
	// Dump payload into the template string.
	if err = textTemplate.Execute(&result, executionPayload); err != nil {
		return "", err
	}
	return result.String(), nil
}
// Returns the Template Id.
func (t Template) GetExecutableResourceName() string {
	return t.TemplateID
}
func (t Template) compositeUserAndDefaults(userPayload interface{}) (TemplatePayload, error) {
	var (
		final map[string]interface{}
		ok    bool
	)
	final, ok = userPayload.(TemplatePayload)
	if !ok {
		return final, errors.New("unable to cast request payload to TemplatePayload struct")
	}
	err := utils.MergeMaps(&final, t.Defaults)
	if err != nil {
		return final, err
	}
	return final, nil
}
func NewTemplateID(t Template) (string, error) {
	uuid4, err := newUUIDv4()
	if err != nil {
		return "", err
	}
	return fmt.Sprintf("tpl-%s", uuid4[4:]), nil
}
func (t *Template) IsValid() (bool, []string) {
	conditions := []validationCondition{
		{len(t.TemplateName) == 0, "string [template_name] must be specified"},
		{len(t.Schema) == 0, "schema must be specified"},
		{len(t.CommandTemplate) == 0, "string [command_template] must be specified"},
		{len(t.Image) == 0, "string [image] must be specified"},
		{t.Memory == nil, "int [memory] must be specified"},
	}
	valid := true
	var reasons []string
	for _, cond := range conditions {
		if cond.condition {
			valid = false
			reasons = append(reasons, cond.reason)
		}
	}
	return valid, reasons
}
type TemplateList struct {
	Total     int        `json:"total"`
	Templates []Template `json:"templates"`
}
func (tl *TemplateList) MarshalJSON() ([]byte, error) {
	type Alias TemplateList
	l := tl.Templates
	if l == nil {
		l = []Template{}
	}
	return json.Marshal(&struct {
		Templates []Template `json:"templates"`
		*Alias
	}{
		Templates: l,
		Alias:     (*Alias)(tl),
	})
}
func (r *KubernetesEvent) Marshal() ([]byte, error) {
	return json.Marshal(r)
}
type KubernetesEvent struct {
	Metadata           Metadata       `json:"metadata,omitempty"`
	Reason             string         `json:"reason,omitempty"`
	Message            string         `json:"message,omitempty"`
	Source             Source         `json:"source,omitempty"`
	FirstTimestamp     string         `json:"firstTimestamp,omitempty"`
	LastTimestamp      string         `json:"lastTimestamp,omitempty"`
	Count              int64          `json:"count,omitempty"`
	Type               string         `json:"type,omitempty"`
	EventTime          interface{}    `json:"eventTime,omitempty"`
	ReportingComponent string         `json:"reportingComponent,omitempty"`
	ReportingInstance  string         `json:"reportingInstance,omitempty"`
	InvolvedObject     InvolvedObject `json:"involvedObject,omitempty"`
	Done               func() error
}
type InvolvedObject struct {
	Kind            string      `json:"kind,omitempty"`
	Namespace       string      `json:"namespace,omitempty"`
	Name            string      `json:"name,omitempty"`
	Uid             string      `json:"uid,omitempty"`
	APIVersion      string      `json:"apiVersion,omitempty"`
	ResourceVersion string      `json:"resourceVersion,omitempty"`
	FieldPath       string      `json:"fieldPath,omitempty"`
	Labels          EventLabels `json:"labels,omitempty"`
}
type EventLabels struct {
	ControllerUid string `json:"controller-uid,omitempty"`
	JobName       string `json:"job-name,omitempty"`
	ClusterName   string `json:"cluster-name,omitempty"`
}
type Metadata struct {
	Name              string `json:"name,omitempty"`
	Namespace         string `json:"namespace,omitempty"`
	SelfLink          string `json:"selfLink,omitempty"`
	Uid               string `json:"uid,omitempty"`
	ResourceVersion   string `json:"resourceVersion,omitempty"`
	CreationTimestamp string `json:"creationTimestamp,omitempty"`
}
type Source struct {
	Component string `json:"component,omitempty"`
	Host      string `json:"host,omitempty"`
}
func UnmarshalEmrEvents(data []byte) (EmrEvent, error) {
	var r EmrEvent
	err := json.Unmarshal(data, &r)
	return r, err
}
func (r *EmrEvent) Marshal() ([]byte, error) {
	return json.Marshal(r)
}
type EmrEvent struct {
	Version    *string       `json:"version,omitempty"`
	ID         *string       `json:"id,omitempty"`
	DetailType *string       `json:"detail-type,omitempty"`
	Source     *string       `json:"source,omitempty"`
	Account    *string       `json:"account,omitempty"`
	Time       *string       `json:"time,omitempty"`
	Region     *string       `json:"region,omitempty"`
	Resources  []interface{} `json:"resources,omitempty"`
	Detail     *Detail       `json:"detail,omitempty"`
	Done       func() error
}
type Detail struct {
	Severity         *string `json:"severity,omitempty"`
	Name             *string `json:"name,omitempty"`
	ID               *string `json:"id,omitempty"`
	Arn              *string `json:"arn,omitempty"`
	VirtualClusterID *string `json:"virtualClusterId,omitempty"`
	State            *string `json:"state,omitempty"`
	CreatedBy        *string `json:"createdBy,omitempty"`
	ReleaseLabel     *string `json:"releaseLabel,omitempty"`
	ExecutionRoleArn *string `json:"executionRoleArn,omitempty"`
	FailureReason    *string `json:"failureReason,omitempty"`
	StateDetails     *string `json:"stateDetails,omitempty"`
	Message          *string `json:"message,omitempty"`
}
type LaunchRequest struct {
	ClusterName *string  `json:"cluster,omitempty"`
	Env         *EnvList `json:"env,omitempty"`
	Tier        Tier     `json:"tier"`
}
type LaunchRequestV2 struct {
	Tier                  Tier            `json:"tier"`
	RunTags               RunTags         `json:"run_tags"`
	Command               *string         `json:"command,omitempty"`
	Memory                *int64          `json:"memory,omitempty"`
	Cpu                   *int64          `json:"cpu,omitempty"`
	Gpu                   *int64          `json:"gpu,omitempty"`
	EphemeralStorage      *int64          `json:"ephemeral_storage,omitempty"`
	Engine                *string         `json:"engine,omitempty"`
	NodeLifecycle         *string         `json:"node_lifecycle,omitempty"`
	ActiveDeadlineSeconds *int64          `json:"active_deadline_seconds,omitempty"`
	SparkExtension        *SparkExtension `json:"spark_extension,omitempty"`
	ClusterName           *string         `json:"cluster,omitempty"`
	Env                   *EnvList        `json:"env,omitempty"`
	Description           *string         `json:"description,omitempty"`
	CommandHash           *string         `json:"command_hash,omitempty"`
	IdempotenceKey        *string         `json:"idempotence_key,omitempty"`
	Arch                  *string         `json:"arch,omitempty"`
	Labels                *Labels         `json:"labels,omitempty"`
	ServiceAccount        *string         `json:"service_account,omitempty"`
}
type RunTags struct {
	OwnerEmail string `json:"owner_email"`
	TeamName   string `json:"team_name"`
	OwnerID    string `json:"owner_id"`
}
type ClusterStatus string
type Tier string
type Tiers []string
type Capability string
type Capabilities []string
const (
	Tier1 Tier = "Tier1"
	Tier2 Tier = "Tier2"
	Tier3 Tier = "Tier3"
	Tier4 Tier = "Tier4"
)
const (
	StatusActive      ClusterStatus = "active"
	StatusMaintenance ClusterStatus = "maintenance"
	StatusOffline     ClusterStatus = "offline"
)
type ClusterMetadata struct {
	Name              string        `json:"name" db:"name"`
	Status            ClusterStatus `json:"status" db:"status"`
	StatusReason      string        `json:"status_reason" db:"status_reason"`
	StatusSince       time.Time     `json:"status_since" db:"status_since"`
	AllowedTiers      []Tier        `json:"allowed_tiers" db:"allowed_tiers"`
	Capabilities      []Capability  `json:"capabilities" db:"capabilities"`
	UpdatedAt         time.Time     `json:"updated_at" db:"updated_at"`
	Namespace         string        `json:"namespace" db:"namespace"`
	Region            string        `json:"region" db:"region"`
	EMRVirtualCluster string        `json:"emr_virtual_cluster" db:"emr_virtual_cluster"`
}

================
File: state/pg_queries.go
================
package state
const DefinitionSelect = `
select td.definition_id                    as definitionid,
       td.adaptive_resource_allocation     as adaptiveresourceallocation,
       td.image                            as image,
       td.group_name                       as groupname,
       td.alias                            as alias,
       td.memory                           as memory,
       coalesce(td.command, '')            as command,
       coalesce(td.task_type, '')          as tasktype,
       env::TEXT                           as env,
       td.cpu                              as cpu,
       td.gpu                              as gpu,
       td.ephemeral_storage 			   as ephemeral_storage,
       coalesce(td.requires_docker, false) as requires_docker,
       coalesce(td.target_cluster, '')     as target_cluster,
       array_to_json('{""}'::TEXT[])::TEXT as tags,
       array_to_json('{}'::INT[])::TEXT    as ports
from (select * from task_def) td
`
const ListDefinitionsSQL = DefinitionSelect + "\n%s %s limit $1 offset $2"
const (
	ListClusterStatesSQL = `
SELECT
	name,
	status,
	status_reason,
	status_since,
	allowed_tiers,
	region,
	updated_at,
	namespace,
	emr_virtual_cluster
FROM cluster_state
ORDER BY name ASC`
)
const GetDefinitionSQL = DefinitionSelect + "\nwhere definition_id = $1"
const GetDefinitionByAliasSQL = DefinitionSelect + "\nwhere alias = $1"
const TaskResourcesSelectCommandSQL = `
SELECT cast((percentile_disc(0.99) within GROUP (ORDER BY A.max_memory_used)) * 1.75 as int) as memory,
       cast((percentile_disc(0.99) within GROUP (ORDER BY A.max_cpu_used)) * 1.25  as int)  as cpu
FROM (SELECT memory as max_memory_used, cpu as max_cpu_used
      FROM TASK
      WHERE
           queued_at >= CURRENT_TIMESTAMP - INTERVAL '3 days'
           AND (exit_code = 137 or exit_reason = 'OOMKilled')
           AND engine = 'eks'
           AND definition_id = $1
           AND command_hash = (SELECT command_hash FROM task WHERE run_id = $2)
      LIMIT 30) A
`
const TaskResourcesExecutorCountSQL = `
SELECT least(coalesce(cast((percentile_disc(0.99) within GROUP (ORDER BY A.executor_count)) as int), 25), 100) as executor_count
FROM (SELECT CASE
                 WHEN (exit_reason like '%Exception%')
                     THEN (spark_extension -> 'spark_submit_job_driver' -> 'num_executors')::int * 1.75
                 ELSE (spark_extension -> 'spark_submit_job_driver' -> 'num_executors')::int * 1
                 END as executor_count
      FROM TASK
      WHERE
           queued_at >= CURRENT_TIMESTAMP - INTERVAL '24 hours'
           AND engine = 'eks-spark'
           AND definition_id = $1
           AND command_hash = $2
      LIMIT 30) A
`
const TaskResourcesDriverOOMSQL = `
SELECT (spark_extension -> 'driver_oom')::boolean AS driver_oom
FROM TASK
WHERE queued_at >= CURRENT_TIMESTAMP - INTERVAL '7 days'
  AND engine = 'eks-spark'
  AND definition_id = $1
  AND command_hash = $2
  AND exit_code = 137
  AND spark_extension ? 'driver_oom'
GROUP BY 1
`
const TaskIdempotenceKeyCheckSQL = `
WITH runs as (
    SELECT run_id
    FROM task
    WHERE idempotence_key = $1
      and (exit_code = 0 or exit_code is null)
      and queued_at >= CURRENT_TIMESTAMP - INTERVAL '7 days')
SELECT run_id
FROM runs
LIMIT 1;
`
const TaskResourcesExecutorOOMSQL = `
SELECT CASE WHEN A.c >= 1 THEN true::boolean ELSE false::boolean END
FROM (SELECT count(*) as c
      FROM TASK
      WHERE
           queued_at >= CURRENT_TIMESTAMP - INTERVAL '7 days'
           AND definition_id = $1
           AND command_hash = $2
		   AND engine = 'eks-spark'
           AND exit_code !=0
      LIMIT 30) A
`
const TaskResourcesExecutorNodeLifecycleSQL = `
SELECT CASE WHEN A.c >= 1 THEN 'ondemand' ELSE 'spot' END
FROM (SELECT count(*) as c
      FROM TASK
      WHERE
           queued_at >= CURRENT_TIMESTAMP - INTERVAL '12 hour'
           AND definition_id = $1
           AND command_hash = $2
           AND exit_code !=0
      LIMIT 30) A
`
const TaskExecutionRuntimeCommandSQL = `
SELECT percentile_disc(0.95) within GROUP (ORDER BY A.minutes) as minutes
FROM (SELECT EXTRACT(epoch from finished_at - started_at) / 60 as minutes
      FROM TASK
      WHERE definition_id = $1
        AND exit_code = 0
        AND engine = 'eks'
        AND queued_at >= CURRENT_TIMESTAMP - INTERVAL '7 days'
        AND command_hash = (SELECT command_hash FROM task WHERE run_id = $2)
      LIMIT 30) A
`
const ListFailingNodesSQL = `
SELECT instance_dns_name
FROM (
         SELECT instance_dns_name, count(*) as c
         FROM TASK
         WHERE (exit_code = 128 OR
                pod_events @> '[{"reason": "Failed"}]' OR
                pod_events @> '[{"reason": "FailedSync"}]' OR
                pod_events @> '[{"reason": "FailedCreatePodSandBox"}]' OR
                pod_events @> '[{"reason": "OutOfmemory"}]')
           AND engine = 'eks'
           AND queued_at >= NOW() - INTERVAL '1 HOURS'
           AND instance_dns_name like 'ip-%'
         GROUP BY 1
         order by 2 desc) AS all_nodes
WHERE c >= 5
`
const PodReAttemptRate = `
SELECT (multiple_attempts / (CASE WHEN single_attempts = 0 THEN 1 ELSE single_attempts END)) AS attempts
FROM (
      SELECT COUNT(CASE WHEN attempt_count <= 1 THEN 1 END) * 1.0 AS single_attempts,
             COUNT(CASE WHEN attempt_count > 1 THEN 1 END) * 1.0 AS multiple_attempts
      FROM task
      WHERE engine = 'eks' AND
            queued_at >= NOW() - INTERVAL '18 MINUTES' AND
            node_lifecycle = 'spot') A
`
const RunSelect = `
select t.run_id                          as runid,
       coalesce(t.definition_id, '')     as definitionid,
       coalesce(t.alias, '')             as alias,
       coalesce(t.image, '')             as image,
       coalesce(t.cluster_name, '')      as clustername,
       t.exit_code                       as exitcode,
       t.exit_reason                     as exitreason,
       coalesce(t.status, '')            as status,
       queued_at                         as queuedat,
       started_at                        as startedat,
       finished_at                       as finishedat,
       coalesce(t.instance_id, '')       as instanceid,
       coalesce(t.instance_dns_name, '') as instancednsname,
       coalesce(t.group_name, '')        as groupname,
       coalesce(t.task_type, '')         as tasktype,
       env::TEXT                         as env,
       command,
       memory,
       cpu,
       gpu,
       engine,
       ephemeral_storage                 as ephemeral_storage,
       node_lifecycle                    as nodelifecycle,
       pod_name                          as podname,
       namespace,
       max_cpu_used                      as maxcpuused,
       max_memory_used                   as maxmemoryused,
       pod_events::TEXT                  as podevents,
       command_hash                      as commandhash,
       cloudtrail_notifications::TEXT    as cloudtrailnotifications,
       coalesce(executable_id, '')       as executableid,
       coalesce(executable_type, '')     as executabletype,
       execution_request_custom::TEXT    as executionrequestcustom,
       cpu_limit                         as cpulimit,
       memory_limit                      as memorylimit,
       attempt_count                     as attemptcount,
       spawned_runs::TEXT                as spawnedruns,
       run_exceptions::TEXT              as runexceptions,
       active_deadline_seconds           as activedeadlineseconds,
       spark_extension::TEXT             as sparkextension,
       metrics_uri                       as metricsuri,
       description                       as description,
	   idempotence_key                   as idempotencekey,
       coalesce("user", '')              as user,
	   coalesce(arch, '')                as arch,
	   labels::TEXT                      as labels,
	   coalesce(requires_docker,false)   as requires_docker,
	   service_account 				 	 as service_account,
     coalesce(tier::text, 'Tier4')   as tier
from task t
`
const ListRunsSQL = RunSelect + "\n%s %s limit $1 offset $2"
const GetRunSQL = RunSelect + "\nwhere run_id = $1"
const GetRunSQLByEMRJobId = RunSelect + "\nwhere spark_extension->>'emr_job_id' = $1"
const GetRunSQLForUpdate = GetRunSQL + " for update"
const GroupsSelect = `
select distinct group_name from task_def
`
const TagsSelect = `
select distinct text from tags
`
const ListGroupsSQL = GroupsSelect + "\n%s order by group_name asc limit $1 offset $2"
const ListTagsSQL = TagsSelect + "\n%s order by text asc limit $1 offset $2"
const WorkerSelect = `
  select
    worker_type        as workertype,
    count_per_instance as countperinstance,
    engine
  from worker
`
const ListWorkersSQL = WorkerSelect
const GetWorkerEngine = WorkerSelect + "\nwhere engine = $1"
const GetWorkerSQL = WorkerSelect + "\nwhere worker_type = $1 and engine = $2"
const GetWorkerSQLForUpdate = GetWorkerSQL + " for update"
const TemplateSelect = `
SELECT
  template_id as templateid,
  template_name as templatename,
  version,
  schema,
  command_template as commandtemplate,
  adaptive_resource_allocation as adaptiveresourceallocation,
  image,
  memory,
  env::TEXT as env,
  privileged,
  cpu,
  gpu,
  defaults,
  coalesce(avatar_uri, '') as avataruri
FROM template
`
const ListTemplatesSQL = TemplateSelect + "\n%s limit $1 offset $2"
const GetTemplateByIDSQL = TemplateSelect + "\nwhere template_id = $1"
const ListTemplatesLatestOnlySQL = `
  SELECT DISTINCT ON (template_name)
    template_id as templateid,
    template_name as templatename,
    version,
    schema,
    command_template as commandtemplate,
    adaptive_resource_allocation as adaptiveresourceallocation,
    image,
    memory,
    env::TEXT as env,
    privileged,
    cpu,
    gpu,
    defaults,
    coalesce(avatar_uri, '') as avataruri
  FROM template
  ORDER BY template_name, version DESC, template_id
  LIMIT $1 OFFSET $2
`
const GetTemplateLatestOnlySQL = TemplateSelect + "\nWHERE template_name = $1 ORDER BY version DESC LIMIT 1;"
const GetTemplateByVersionSQL = TemplateSelect + "\nWHERE template_name = $1 AND version = $2 ORDER BY version DESC LIMIT 1;"

================
File: state/pg_state_manager.go
================
package state
import (
	"database/sql/driver"
	"encoding/json"
	"fmt"
	"time"
	"github.com/stitchfix/flotilla-os/clients/metrics"
	"github.com/stitchfix/flotilla-os/log"
	"github.com/jmoiron/sqlx"
	"database/sql"
	"math"
	"strings"
	"github.com/lib/pq"
	_ "github.com/lib/pq"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/exceptions"
	"go.uber.org/multierr"
	sqltrace "gopkg.in/DataDog/dd-trace-go.v1/contrib/database/sql"
	sqlxtrace "gopkg.in/DataDog/dd-trace-go.v1/contrib/jmoiron/sqlx"
)
type SQLStateManager struct {
	db         *sqlx.DB
	readonlyDB *sqlx.DB
	log        log.Logger
}
func (sm *SQLStateManager) ListFailingNodes() (NodeList, error) {
	var err error
	var nodeList NodeList
	err = sm.readonlyDB.Select(&nodeList, ListFailingNodesSQL)
	if err != nil {
		if err == sql.ErrNoRows {
			return nodeList, exceptions.MissingResource{
				ErrorString: fmt.Sprintf("Error fetching node list")}
		} else {
			return nodeList, errors.Wrapf(err, "Error fetching node list")
		}
	}
	return nodeList, err
}
func (sm *SQLStateManager) GetPodReAttemptRate() (float32, error) {
	var err error
	attemptRate := float32(1.0)
	err = sm.readonlyDB.Get(&attemptRate, PodReAttemptRate)
	if err != nil {
		if err == sql.ErrNoRows {
			return attemptRate, exceptions.MissingResource{
				ErrorString: fmt.Sprintf("Error fetching attempt rate")}
		} else {
			return attemptRate, errors.Wrapf(err, "Error fetching attempt rate")
		}
	}
	return attemptRate, err
}
func (sm *SQLStateManager) GetNodeLifecycle(executableID string, commandHash string) (string, error) {
	var err error
	nodeType := "spot"
	err = sm.readonlyDB.Get(&nodeType, TaskResourcesExecutorNodeLifecycleSQL, executableID, commandHash)
	if err != nil {
		if err == sql.ErrNoRows {
			return nodeType, exceptions.MissingResource{
				ErrorString: fmt.Sprintf("Error fetching node type")}
		} else {
			return nodeType, errors.Wrapf(err, "Error fetching node type")
		}
	}
	return nodeType, err
}
func (sm *SQLStateManager) GetTaskHistoricalRuntime(executableID string, runID string) (float32, error) {
	var err error
	minutes := float32(1.0)
	err = sm.readonlyDB.Get(&minutes, TaskExecutionRuntimeCommandSQL, executableID, runID)
	if err != nil {
		if err == sql.ErrNoRows {
			return minutes, exceptions.MissingResource{
				ErrorString: fmt.Sprintf("Error fetching TaskRuntime rate")}
		} else {
			return minutes, errors.Wrapf(err, "Error fetching attempt rate")
		}
	}
	return minutes, err
}
func (sm *SQLStateManager) EstimateRunResources(executableID string, runID string) (TaskResources, error) {
	var err error
	var taskResources TaskResources
	err = sm.readonlyDB.Get(&taskResources, TaskResourcesSelectCommandSQL, executableID, runID)
	if err != nil {
		if err == sql.ErrNoRows {
			return taskResources, exceptions.MissingResource{
				ErrorString: fmt.Sprintf("Resource usage with executable %s not found", executableID)}
		} else {
			return taskResources, errors.Wrapf(err, "issue getting resources with executable [%s]", executableID)
		}
	}
	return taskResources, err
}
func (sm *SQLStateManager) EstimateExecutorCount(executableID string, commandHash string) (int64, error) {
	var err error
	executorCount := int64(25)
	err = sm.readonlyDB.Get(&executorCount, TaskResourcesExecutorCountSQL, executableID, commandHash)
	if err != nil {
		if err == sql.ErrNoRows {
			return executorCount, exceptions.MissingResource{
				ErrorString: fmt.Sprintf("Resource usage with executable %s not found", executableID)}
		} else {
			return executorCount, errors.Wrapf(err, "issue getting resources with executable [%s]", executableID)
		}
	}
	return executorCount, err
}
func (sm *SQLStateManager) CheckIdempotenceKey(idempotenceKey string) (string, error) {
	var err error
	runId := ""
	err = sm.readonlyDB.Get(&runId, TaskIdempotenceKeyCheckSQL, idempotenceKey)
	if err != nil || len(runId) == 0 {
		err = errors.New("no run_id found for idempotence key")
	}
	return runId, err
}
func (sm *SQLStateManager) ExecutorOOM(executableID string, commandHash string) (bool, error) {
	var err error
	executorOOM := false
	err = sm.readonlyDB.Get(&executorOOM, TaskResourcesExecutorOOMSQL, executableID, commandHash)
	if err != nil {
		if err == sql.ErrNoRows {
			return executorOOM, exceptions.MissingResource{
				ErrorString: fmt.Sprintf("Resource oom for executable %s not found", executableID)}
		} else {
			return executorOOM, errors.Wrapf(err, "issue getting resources with executable [%s]", executableID)
		}
	}
	return executorOOM, err
}
func (sm *SQLStateManager) DriverOOM(executableID string, commandHash string) (bool, error) {
	var err error
	driverOOM := false
	err = sm.readonlyDB.Get(&driverOOM, TaskResourcesDriverOOMSQL, executableID, commandHash)
	if err != nil {
		if err == sql.ErrNoRows {
			return driverOOM, exceptions.MissingResource{
				ErrorString: fmt.Sprintf("Resource oom for driver %s not found", executableID)}
		} else {
			return driverOOM, errors.Wrapf(err, "issue getting resources with executable [%s]", executableID)
		}
	}
	return driverOOM, err
}
func (sm *SQLStateManager) Name() string {
	return "postgres"
}
var likeFields = map[string]bool{
	"image":       true,
	"alias":       true,
	"group_name":  true,
	"command":     true,
	"text":        true,
	"exit_reason": true,
}
func (sm *SQLStateManager) Initialize(conf config.Config) error {
	dburl := conf.GetString("database_url")
	readonlyDbUrl := conf.GetString("readonly_database_url")
	createSchema := conf.GetBool("create_database_schema")
	fmt.Printf("create_database_schema: %t\ncreating schema...\n", createSchema)
	sqltrace.Register("postgres", &pq.Driver{}, sqltrace.WithServiceName("flotilla"))
	var err error
	if sm.db, err = sqlxtrace.Open("postgres", dburl); err != nil {
		return errors.Wrap(err, "unable to open postgres db")
	}
	sqltrace.Register("postgres", &pq.Driver{}, sqltrace.WithServiceName("flotilla"))
	if sm.readonlyDB, err = sqlxtrace.Open("postgres", readonlyDbUrl); err != nil {
		return errors.Wrap(err, "unable to open readonly postgres db")
	}
	if conf.IsSet("database_max_idle_connections") {
		sm.db.SetMaxIdleConns(conf.GetInt("database_max_idle_connections"))
		sm.readonlyDB.SetMaxIdleConns(conf.GetInt("database_max_idle_connections"))
	}
	if createSchema {
		if err = sm.db.Ping(); err != nil {
			for i := 0; i < 3 && err != nil; i++ {
				time.Sleep(time.Duration(5*math.Pow(2, float64(i))) * time.Second)
				err = sm.db.Ping()
			}
			if err != nil {
				return errors.Wrap(err, "error trying to connect to postgres db, retries exhausted")
			}
		}
		if err = sm.initWorkerTable(conf); err != nil {
			return errors.Wrap(err, "problem populating worker table sql")
		}
	}
	return nil
}
func (sm *SQLStateManager) makeWhereClause(filters map[string][]string) []string {
	wc := []string{}
	for k, v := range filters {
		if len(v) > 1 {
			quoted := make([]string, len(v))
			for i, filterVal := range v {
				quoted[i] = fmt.Sprintf("'%s'", filterVal)
			}
			wc = append(wc, fmt.Sprintf("%s in (%s)", k, strings.Join(quoted, ",")))
		} else if len(v) == 1 {
			fmtString := "%s='%s'"
			fieldName := k
			if likeFields[k] {
				fmtString = "%s like '%%%s%%'"
			} else if strings.HasSuffix(k, "_since") {
				fieldName = strings.Replace(k, "_since", "", -1)
				fmtString = "%s > '%s'"
			} else if strings.HasSuffix(k, "_until") {
				fieldName = strings.Replace(k, "_until", "", -1)
				fmtString = "%s < '%s'"
			}
			wc = append(wc, fmt.Sprintf(fmtString, fieldName, v[0]))
		}
	}
	return wc
}
func (sm *SQLStateManager) makeEnvWhereClause(filters map[string]string) []string {
	wc := make([]string, len(filters))
	i := 0
	for k, v := range filters {
		fmtString := `env @> '[{"name":"%s","value":"%s"}]'`
		wc[i] = fmt.Sprintf(fmtString, k, v)
		i++
	}
	return wc
}
func (sm *SQLStateManager) orderBy(obj IOrderable, field string, order string) (string, error) {
	if order == "asc" || order == "desc" {
		if obj.ValidOrderField(field) {
			return fmt.Sprintf("order by %s %s NULLS LAST", field, order), nil
		}
		return "", errors.Errorf("Invalid field to order by [%s], must be one of [%s]",
			field,
			strings.Join(obj.ValidOrderFields(), ", "))
	}
	return "", errors.Errorf("Invalid order string, must be one of ('asc', 'desc'), was %s", order)
}
func (sm *SQLStateManager) ListDefinitions(
	limit int, offset int, sortBy string,
	order string, filters map[string][]string,
	envFilters map[string]string) (DefinitionList, error) {
	var err error
	var result DefinitionList
	var whereClause, orderQuery string
	where := append(sm.makeWhereClause(filters), sm.makeEnvWhereClause(envFilters)...)
	if len(where) > 0 {
		whereClause = fmt.Sprintf("where %s", strings.Join(where, " and "))
	}
	orderQuery, err = sm.orderBy(&Definition{}, sortBy, order)
	if err != nil {
		return result, errors.WithStack(err)
	}
	sql := fmt.Sprintf(ListDefinitionsSQL, whereClause, orderQuery)
	countSQL := fmt.Sprintf("select COUNT(*) from (%s) as sq", sql)
	err = sm.db.Select(&result.Definitions, sql, limit, offset)
	if err != nil {
		return result, errors.Wrap(err, "issue running list definitions sql")
	}
	err = sm.db.Get(&result.Total, countSQL, nil, 0)
	if err != nil {
		return result, errors.Wrap(err, "issue running list definitions count sql")
	}
	return result, nil
}
func (sm *SQLStateManager) GetDefinition(definitionID string) (Definition, error) {
	var err error
	var definition Definition
	err = sm.db.Get(&definition, GetDefinitionSQL, definitionID)
	if err != nil {
		if err == sql.ErrNoRows {
			return definition, exceptions.MissingResource{
				fmt.Sprintf("Definition with ID %s not found", definitionID)}
		} else {
			return definition, errors.Wrapf(err, "issue getting definition with id [%s]", definitionID)
		}
	}
	return definition, nil
}
func (sm *SQLStateManager) GetDefinitionByAlias(alias string) (Definition, error) {
	var err error
	var definition Definition
	err = sm.db.Get(&definition, GetDefinitionByAliasSQL, alias)
	if err != nil {
		if err == sql.ErrNoRows {
			return definition, exceptions.MissingResource{
				fmt.Sprintf("Definition with alias %s not found", alias)}
		} else {
			return definition, errors.Wrapf(err, "issue getting definition with alias [%s]", alias)
		}
	}
	return definition, err
}
func (sm *SQLStateManager) UpdateDefinition(definitionID string, updates Definition) (Definition, error) {
	var (
		err      error
		existing Definition
	)
	existing, err = sm.GetDefinition(definitionID)
	if err != nil {
		return existing, errors.WithStack(err)
	}
	existing.UpdateWith(updates)
	selectForUpdate := `SELECT * FROM task_def WHERE definition_id = $1 FOR UPDATE;`
	deletePorts := `DELETE FROM task_def_ports WHERE task_def_id = $1;`
	deleteTags := `DELETE FROM task_def_tags WHERE task_def_id = $1`
	insertPorts := `
    INSERT INTO task_def_ports(
      task_def_id, port
    ) VALUES ($1, $2);
    `
	insertDefTags := `
	INSERT INTO task_def_tags(
	  task_def_id, tag_id
	) VALUES ($1, $2);
	`
	insertTags := `
	INSERT INTO tags(text) SELECT $1 WHERE NOT EXISTS (SELECT text from tags where text = $2)
	`
	tx, err := sm.db.Begin()
	if err != nil {
		return existing, errors.WithStack(err)
	}
	if _, err = tx.Exec(selectForUpdate, definitionID); err != nil {
		return existing, errors.WithStack(err)
	}
	if _, err = tx.Exec(deletePorts, definitionID); err != nil {
		return existing, errors.WithStack(err)
	}
	if _, err = tx.Exec(deleteTags, definitionID); err != nil {
		return existing, errors.WithStack(err)
	}
	update := `
    UPDATE task_def SET
      image = $2,
      alias = $3,
      memory = $4,
      command = $5,
      env = $6,
      cpu = $7,
      gpu = $8,
      adaptive_resource_allocation = $9,
      ephemeral_storage = $10,
	  requires_docker = $11,
      target_cluster = $12
    WHERE definition_id = $1;
    `
	if _, err = tx.Exec(
		update,
		definitionID,
		existing.Image,
		existing.Alias,
		existing.Memory,
		existing.Command,
		existing.Env,
		existing.Cpu,
		existing.Gpu,
		existing.AdaptiveResourceAllocation,
		existing.EphemeralStorage,
		existing.RequiresDocker,
		existing.TargetCluster); err != nil {
		return existing, errors.Wrapf(err, "issue updating definition [%s]", definitionID)
	}
	if existing.Ports != nil {
		for _, p := range *existing.Ports {
			if _, err = tx.Exec(insertPorts, definitionID, p); err != nil {
				tx.Rollback()
				return existing, errors.WithStack(err)
			}
		}
	}
	if existing.Tags != nil {
		for _, t := range *existing.Tags {
			if _, err = tx.Exec(insertTags, t, t); err != nil {
				tx.Rollback()
				return existing, errors.WithStack(err)
			}
			if _, err = tx.Exec(insertDefTags, definitionID, t); err != nil {
				tx.Rollback()
				return existing, errors.WithStack(err)
			}
		}
	}
	err = tx.Commit()
	if err != nil {
		return existing, errors.WithStack(err)
	}
	return existing, nil
}
func (sm *SQLStateManager) CreateDefinition(d Definition) error {
	var err error
	insertPorts := `
    INSERT INTO task_def_ports(
      task_def_id, port
    ) VALUES ($1, $2);
    `
	insertDefTags := `
	INSERT INTO task_def_tags(
	  task_def_id, tag_id
	) VALUES ($1, $2);
	`
	insertTags := `
	INSERT INTO tags(text) SELECT $1 WHERE NOT EXISTS (SELECT text from tags where text = $2)
	`
	tx, err := sm.db.Begin()
	if err != nil {
		return errors.WithStack(err)
	}
	insert := `
    INSERT INTO task_def(
      definition_id,
      image,
      group_name,
      alias,
      memory,
      command,
      env,
      cpu,
      gpu,
      adaptive_resource_allocation,
      ephemeral_storage,
      requires_docker,
      target_cluster
    )
    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13);
    `
	if _, err = tx.Exec(insert,
		d.DefinitionID,
		d.Image,
		d.GroupName,
		d.Alias,
		d.Memory,
		d.Command,
		d.Env,
		d.Cpu,
		d.Gpu,
		d.AdaptiveResourceAllocation,
		d.EphemeralStorage,
		d.RequiresDocker,
		d.TargetCluster); err != nil {
		tx.Rollback()
		return errors.Wrapf(
			err, "issue creating new task definition with alias [%s] and id [%s]", d.DefinitionID, d.Alias)
	}
	if d.Ports != nil {
		for _, p := range *d.Ports {
			if _, err = tx.Exec(insertPorts, d.DefinitionID, p); err != nil {
				tx.Rollback()
				return errors.WithStack(err)
			}
		}
	}
	if d.Tags != nil {
		for _, t := range *d.Tags {
			if _, err = tx.Exec(insertTags, t, t); err != nil {
				tx.Rollback()
				return errors.WithStack(err)
			}
			if _, err = tx.Exec(insertDefTags, d.DefinitionID, t); err != nil {
				tx.Rollback()
				return errors.WithStack(err)
			}
		}
	}
	err = tx.Commit()
	if err != nil {
		return errors.WithStack(err)
	}
	return nil
}
func (sm *SQLStateManager) DeleteDefinition(definitionID string) error {
	var err error
	statements := []string{
		"DELETE FROM task_def_ports WHERE task_def_id = $1",
		"DELETE FROM task_def_tags WHERE task_def_id = $1",
		"DELETE FROM task WHERE definition_id = $1",
		"DELETE FROM task_def WHERE definition_id = $1",
	}
	tx, err := sm.db.Begin()
	if err != nil {
		return errors.WithStack(err)
	}
	for _, stmt := range statements {
		if _, err = tx.Exec(stmt, definitionID); err != nil {
			tx.Rollback()
			return errors.Wrapf(err, "issue deleting definition with id [%s]", definitionID)
		}
	}
	err = tx.Commit()
	if err != nil {
		return errors.WithStack(err)
	}
	return nil
}
func (sm *SQLStateManager) ListRuns(limit int, offset int, sortBy string, order string, filters map[string][]string, envFilters map[string]string, engines []string) (RunList, error) {
	var err error
	var result RunList
	var whereClause, orderQuery string
	if filters == nil {
		filters = make(map[string][]string)
	}
	if engines != nil {
		filters["engine"] = engines
	} else {
		filters["engine"] = []string{DefaultEngine}
	}
	where := append(sm.makeWhereClause(filters), sm.makeEnvWhereClause(envFilters)...)
	if len(where) > 0 {
		whereClause = fmt.Sprintf("where %s", strings.Join(where, " and "))
	}
	orderQuery, err = sm.orderBy(&Run{}, sortBy, order)
	if err != nil {
		return result, errors.WithStack(err)
	}
	sql := fmt.Sprintf(ListRunsSQL, whereClause, orderQuery)
	countSQL := fmt.Sprintf("select COUNT(*) from (%s) as sq", sql)
	err = sm.db.Select(&result.Runs, sql, limit, offset)
	if err != nil {
		return result, errors.Wrap(err, "issue running list runs sql")
	}
	err = sm.db.Get(&result.Total, countSQL, nil, 0)
	if err != nil {
		return result, errors.Wrap(err, "issue running list runs count sql")
	}
	return result, nil
}
func (sm *SQLStateManager) GetRun(runID string) (Run, error) {
	var err error
	var r Run
	err = sm.db.Get(&r, GetRunSQL, runID)
	if err != nil {
		if err == sql.ErrNoRows {
			return r, exceptions.MissingResource{
				fmt.Sprintf("Run with id %s not found", runID)}
		} else {
			return r, errors.Wrapf(err, "issue getting run with id [%s]", runID)
		}
	}
	return r, nil
}
func (sm *SQLStateManager) GetRunByEMRJobId(emrJobId string) (Run, error) {
	var err error
	var r Run
	err = sm.db.Get(&r, GetRunSQLByEMRJobId, emrJobId)
	if err != nil {
		if err == sql.ErrNoRows {
			return r, exceptions.MissingResource{
				fmt.Sprintf("Run with emrjobid %s not found", emrJobId)}
		} else {
			return r, errors.Wrapf(err, "issue getting run with emrjobid [%s]", emrJobId)
		}
	}
	return r, nil
}
func (sm *SQLStateManager) GetResources(runID string) (Run, error) {
	var err error
	var r Run
	err = sm.db.Get(&r, GetRunSQL, runID)
	if err != nil {
		if err == sql.ErrNoRows {
			return r, exceptions.MissingResource{
				fmt.Sprintf("Run with id %s not found", runID)}
		} else {
			return r, errors.Wrapf(err, "issue getting run with id [%s]", runID)
		}
	}
	return r, nil
}
func (sm *SQLStateManager) UpdateRun(runID string, updates Run) (Run, error) {
	start := time.Now()
	var (
		err      error
		existing Run
	)
	tx, err := sm.db.Begin()
	if err != nil {
		return existing, errors.WithStack(err)
	}
	rows, err := tx.Query(GetRunSQLForUpdate, runID)
	if err != nil {
		tx.Rollback()
		return existing, errors.WithStack(err)
	}
	for rows.Next() {
		err = rows.Scan(
			&existing.RunID,
			&existing.DefinitionID,
			&existing.Alias,
			&existing.Image,
			&existing.ClusterName,
			&existing.ExitCode,
			&existing.ExitReason,
			&existing.Status,
			&existing.QueuedAt,
			&existing.StartedAt,
			&existing.FinishedAt,
			&existing.InstanceID,
			&existing.InstanceDNSName,
			&existing.GroupName,
			&existing.TaskType,
			&existing.Env,
			&existing.Command,
			&existing.Memory,
			&existing.Cpu,
			&existing.Gpu,
			&existing.Engine,
			&existing.EphemeralStorage,
			&existing.NodeLifecycle,
			&existing.PodName,
			&existing.Namespace,
			&existing.MaxCpuUsed,
			&existing.MaxMemoryUsed,
			&existing.PodEvents,
			&existing.CommandHash,
			&existing.CloudTrailNotifications,
			&existing.ExecutableID,
			&existing.ExecutableType,
			&existing.ExecutionRequestCustom,
			&existing.CpuLimit,
			&existing.MemoryLimit,
			&existing.AttemptCount,
			&existing.SpawnedRuns,
			&existing.RunExceptions,
			&existing.ActiveDeadlineSeconds,
			&existing.SparkExtension,
			&existing.MetricsUri,
			&existing.Description,
			&existing.IdempotenceKey,
			&existing.User,
			&existing.Arch,
			&existing.Labels,
			&existing.RequiresDocker,
			&existing.ServiceAccount,
			&existing.Tier,
		)
	}
	if err != nil {
		return existing, errors.WithStack(err)
	}
	existing.UpdateWith(updates)
	update := `
    UPDATE task SET
        definition_id = $2,
		alias = $3,
		image = $4,
		cluster_name = $5,
		exit_code = $6,
		exit_reason = $7,
		status = $8,
		queued_at = $9,
		started_at = $10,
		finished_at = $11,
		instance_id = $12,
		instance_dns_name = $13,
		group_name = $14,
		env = $15,
		command = $16,
		memory = $17,
		cpu = $18,
		gpu = $19,
		engine = $20,
		ephemeral_storage = $21,
		node_lifecycle = $22,
		pod_name = $23,
		namespace = $24,
		max_cpu_used = $25,
		max_memory_used = $26,
		pod_events = $27,
		cloudtrail_notifications = $28,
		executable_id = $29,
		executable_type = $30,
		execution_request_custom = $31,
		cpu_limit = $32,
		memory_limit = $33,
		attempt_count = $34,
		spawned_runs = $35,
		run_exceptions = $36,
		active_deadline_seconds = $37,
		spark_extension = $38,
		metrics_uri = $39,
		description = $40,
		idempotence_key = $41,
		"user" = $42,
		arch = $43,
		labels = $44,
		requires_docker = $45,
		service_account = $46,
        tier = $47
    WHERE run_id = $1;
    `
	if _, err = tx.Exec(
		update,
		runID,
		existing.DefinitionID,
		existing.Alias,
		existing.Image,
		existing.ClusterName,
		existing.ExitCode,
		existing.ExitReason,
		existing.Status,
		existing.QueuedAt,
		existing.StartedAt,
		existing.FinishedAt,
		existing.InstanceID,
		existing.InstanceDNSName,
		existing.GroupName,
		existing.Env,
		existing.Command,
		existing.Memory,
		existing.Cpu,
		existing.Gpu,
		existing.Engine,
		existing.EphemeralStorage,
		existing.NodeLifecycle,
		existing.PodName,
		existing.Namespace,
		existing.MaxCpuUsed,
		existing.MaxMemoryUsed,
		existing.PodEvents,
		existing.CloudTrailNotifications,
		existing.ExecutableID,
		existing.ExecutableType,
		existing.ExecutionRequestCustom,
		existing.CpuLimit,
		existing.MemoryLimit,
		existing.AttemptCount,
		existing.SpawnedRuns,
		existing.RunExceptions,
		existing.ActiveDeadlineSeconds,
		existing.SparkExtension,
		existing.MetricsUri,
		existing.Description,
		existing.IdempotenceKey,
		existing.User,
		existing.Arch,
		existing.Labels,
		existing.RequiresDocker,
		existing.ServiceAccount,
		existing.Tier); err != nil {
		tx.Rollback()
		return existing, errors.WithStack(err)
	}
	if err = tx.Commit(); err != nil {
		return existing, errors.WithStack(err)
	}
	_ = metrics.Timing(metrics.EngineUpdateRun, time.Since(start), []string{existing.ClusterName}, 1)
	go sm.logStatusUpdate(existing)
	return existing, nil
}
func (sm *SQLStateManager) CreateRun(r Run) error {
	var err error
	insert := `
	INSERT INTO task (
      	run_id,
		definition_id,
		alias,
		image,
		cluster_name,
		exit_code,
		exit_reason,
		status,
		queued_at,
		started_at,
		finished_at,
		instance_id,
		instance_dns_name,
		group_name,
		env,
		command,
		memory,
		cpu,
		gpu,
		engine,
		node_lifecycle,
		ephemeral_storage,
		pod_name,
		namespace,
		max_cpu_used,
		max_memory_used,
		pod_events,
		executable_id,
		executable_type,
		execution_request_custom,
		cpu_limit,
		memory_limit,
		attempt_count,
		spawned_runs,
		run_exceptions,
		active_deadline_seconds,
		task_type,
		command_hash,
		spark_extension,
		metrics_uri,
		description,
	    idempotence_key,
	    "user",
	    arch,
	    labels,
		requires_docker,
		service_account,
		tier
    ) VALUES (
        $1,
		$2,
		$3,
		$4,
		$5,
		$6,
		$7,
		$8,
		$9,
		$10,
		$11,
		$12,
		$13,
		$14,
		$15,
		$16,
		$17,
		$18,
		$19,
		$20,
		$21,
		$22,
		$23,
		$24,
		$25,
		$26,
		$27,
		$28,
		$29,
		$30,
		$31,
		$32,
		$33,
		$34,
		$35,
		$36,
		$37,
		$38,
		$39,
		$40,
        $41,
        $42,
        $43,
        $44,
        $45,
    	$46,
    	$47,
    	$48
	);
    `
	tx, err := sm.db.Begin()
	if err != nil {
		return errors.WithStack(err)
	}
	if _, err = tx.Exec(insert,
		r.RunID,
		r.DefinitionID,
		r.Alias,
		r.Image,
		r.ClusterName,
		r.ExitCode,
		r.ExitReason,
		r.Status,
		r.QueuedAt,
		r.StartedAt,
		r.FinishedAt,
		r.InstanceID,
		r.InstanceDNSName,
		r.GroupName,
		r.Env,
		r.Command,
		r.Memory,
		r.Cpu,
		r.Gpu,
		r.Engine,
		r.NodeLifecycle,
		r.EphemeralStorage,
		r.PodName,
		r.Namespace,
		r.MaxCpuUsed,
		r.MaxMemoryUsed,
		r.PodEvents,
		r.ExecutableID,
		r.ExecutableType,
		r.ExecutionRequestCustom,
		r.CpuLimit,
		r.MemoryLimit,
		r.AttemptCount,
		r.SpawnedRuns,
		r.RunExceptions,
		r.ActiveDeadlineSeconds,
		r.TaskType,
		r.CommandHash,
		r.SparkExtension,
		r.MetricsUri,
		r.Description,
		r.IdempotenceKey,
		r.User,
		r.Arch,
		r.Labels,
		r.RequiresDocker,
		r.ServiceAccount,
		r.Tier); err != nil {
		tx.Rollback()
		return errors.Wrapf(err, "issue creating new task run with id [%s]", r.RunID)
	}
	if err = tx.Commit(); err != nil {
		return errors.WithStack(err)
	}
	go sm.logStatusUpdate(r)
	return nil
}
func (sm *SQLStateManager) ListGroups(limit int, offset int, name *string) (GroupsList, error) {
	var (
		err         error
		result      GroupsList
		whereClause string
	)
	if name != nil && len(*name) > 0 {
		whereClause = fmt.Sprintf("where %s", strings.Join(
			sm.makeWhereClause(map[string][]string{"group_name": {*name}}), " and "))
	}
	sql := fmt.Sprintf(ListGroupsSQL, whereClause)
	countSQL := fmt.Sprintf("select COUNT(*) from (%s) as sq", sql)
	err = sm.db.Select(&result.Groups, sql, limit, offset)
	if err != nil {
		return result, errors.Wrap(err, "issue running list groups sql")
	}
	err = sm.db.Get(&result.Total, countSQL, nil, 0)
	if err != nil {
		return result, errors.Wrap(err, "issue running list groups count sql")
	}
	return result, nil
}
func (sm *SQLStateManager) ListTags(limit int, offset int, name *string) (TagsList, error) {
	var (
		err         error
		result      TagsList
		whereClause string
	)
	if name != nil && len(*name) > 0 {
		whereClause = fmt.Sprintf("where %s", strings.Join(
			sm.makeWhereClause(map[string][]string{"text": {*name}}), " and "))
	}
	sql := fmt.Sprintf(ListTagsSQL, whereClause)
	countSQL := fmt.Sprintf("select COUNT(*) from (%s) as sq", sql)
	err = sm.db.Select(&result.Tags, sql, limit, offset)
	if err != nil {
		return result, errors.Wrap(err, "issue running list tags sql")
	}
	err = sm.db.Get(&result.Total, countSQL, nil, 0)
	if err != nil {
		return result, errors.Wrap(err, "issue running list tags count sql")
	}
	return result, nil
}
func (sm *SQLStateManager) initWorkerTable(c config.Config) error {
	for _, engine := range Engines {
		fmt.Printf("init worker table for %s engine", engine)
		retryCount := int64(1)
		if c.IsSet(fmt.Sprintf("worker.%s.retry_worker_count_per_instance", engine)) {
			retryCount = int64(c.GetInt("worker.ecs.retry_worker_count_per_instance"))
		}
		submitCount := int64(1)
		if c.IsSet(fmt.Sprintf("worker.%s.submit_worker_count_per_instance", engine)) {
			submitCount = int64(c.GetInt("worker.ecs.submit_worker_count_per_instance"))
		}
		statusCount := int64(1)
		if c.IsSet(fmt.Sprintf("worker.%s.status_worker_count_per_instance", engine)) {
			statusCount = int64(c.GetInt("worker.ecs.status_worker_count_per_instance"))
		}
		var err error
		insert := `
		INSERT INTO worker (worker_type, count_per_instance, engine)
		VALUES ('retry', $1, $4), ('submit', $2, $4), ('status', $3, $4);
	`
		tx, err := sm.db.Begin()
		if err != nil {
			return errors.WithStack(err)
		}
		if _, err = tx.Exec(insert, retryCount, submitCount, statusCount, engine); err != nil {
			tx.Rollback()
			return errors.Wrapf(err, "issue populating worker table")
		}
		err = tx.Commit()
		if err != nil {
			return errors.WithStack(err)
		}
	}
	return nil
}
func (sm *SQLStateManager) ListWorkers(engine string) (WorkersList, error) {
	var err error
	var result WorkersList
	countSQL := fmt.Sprintf("select COUNT(*) from (%s) as sq", ListWorkersSQL)
	err = sm.readonlyDB.Select(&result.Workers, GetWorkerEngine, engine)
	if err != nil {
		return result, errors.Wrap(err, "issue running list workers sql")
	}
	err = sm.readonlyDB.Get(&result.Total, countSQL)
	if err != nil {
		return result, errors.Wrap(err, "issue running list workers count sql")
	}
	return result, nil
}
func (sm *SQLStateManager) GetWorker(workerType string, engine string) (w Worker, err error) {
	if err := sm.readonlyDB.Get(&w, GetWorkerSQL, workerType, engine); err != nil {
		if err == sql.ErrNoRows {
			err = exceptions.MissingResource{
				ErrorString: fmt.Sprintf("Worker of type %s not found", workerType)}
		} else {
			err = errors.Wrapf(err, "issue getting worker of type [%s]", workerType)
		}
	}
	return
}
func (sm *SQLStateManager) UpdateWorker(workerType string, updates Worker) (Worker, error) {
	var (
		err      error
		existing Worker
	)
	engine := DefaultEngine
	tx, err := sm.db.Begin()
	if err != nil {
		return existing, errors.WithStack(err)
	}
	rows, err := tx.Query(GetWorkerSQLForUpdate, workerType, engine)
	if err != nil {
		tx.Rollback()
		return existing, errors.WithStack(err)
	}
	for rows.Next() {
		err = rows.Scan(&existing.WorkerType, &existing.CountPerInstance)
	}
	if err != nil {
		return existing, errors.WithStack(err)
	}
	existing.UpdateWith(updates)
	update := `
		UPDATE worker SET count_per_instance = $2
    WHERE worker_type = $1;
    `
	if _, err = tx.Exec(update, workerType, existing.CountPerInstance); err != nil {
		tx.Rollback()
		return existing, errors.WithStack(err)
	}
	if err = tx.Commit(); err != nil {
		return existing, errors.WithStack(err)
	}
	return existing, nil
}
func (sm *SQLStateManager) BatchUpdateWorkers(updates []Worker) (WorkersList, error) {
	var existing WorkersList
	for _, w := range updates {
		_, err := sm.UpdateWorker(w.WorkerType, w)
		if err != nil {
			return existing, err
		}
	}
	return sm.ListWorkers(DefaultEngine)
}
func (sm *SQLStateManager) Cleanup() error {
	return multierr.Combine(sm.db.Close(), sm.readonlyDB.Close())
}
type IOrderable interface {
	ValidOrderField(field string) bool
	ValidOrderFields() []string
	DefaultOrderField() string
}
func (d *Definition) ValidOrderField(field string) bool {
	for _, f := range d.ValidOrderFields() {
		if field == f {
			return true
		}
	}
	return false
}
func (d *Definition) ValidOrderFields() []string {
	return []string{"alias", "image", "group_name", "memory"}
}
func (d *Definition) DefaultOrderField() string {
	return "group_name"
}
func (r *Run) ValidOrderField(field string) bool {
	for _, f := range r.ValidOrderFields() {
		if field == f {
			return true
		}
	}
	return false
}
func (r *Run) ValidOrderFields() []string {
	return []string{"run_id", "cluster_name", "status", "started_at", "finished_at", "group_name"}
}
func (r *Run) DefaultOrderField() string {
	return "group_name"
}
func (t *Template) ValidOrderField(field string) bool {
	for _, f := range t.ValidOrderFields() {
		if field == f {
			return true
		}
	}
	return false
}
func (t *Template) ValidOrderFields() []string {
	return []string{"template_name", "version"}
}
func (t *Template) DefaultOrderField() string {
	return "template_name"
}
func (e *EnvList) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.(string))
		json.Unmarshal(s, &e)
	}
	return nil
}
func (e EnvList) Value() (driver.Value, error) {
	res, _ := json.Marshal(e)
	return res, nil
}
func (e *PodEvents) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.(string))
		json.Unmarshal(s, &e)
	}
	return nil
}
func (e SpawnedRuns) Value() (driver.Value, error) {
	res, _ := json.Marshal(e)
	return res, nil
}
func (e *SpawnedRuns) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.(string))
		json.Unmarshal(s, &e)
	}
	return nil
}
func (e SparkExtension) Value() (driver.Value, error) {
	res, _ := json.Marshal(e)
	return res, nil
}
func (e *SparkExtension) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.(string))
		json.Unmarshal(s, &e)
	}
	return nil
}
func (e RunExceptions) Value() (driver.Value, error) {
	res, _ := json.Marshal(e)
	return res, nil
}
func (e *RunExceptions) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.(string))
		json.Unmarshal(s, &e)
	}
	return nil
}
func (e PodEvents) Value() (driver.Value, error) {
	res, _ := json.Marshal(e)
	return res, nil
}
func (e *PortsList) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.(string))
		json.Unmarshal(s, &e)
	}
	return nil
}
func (e PortsList) Value() (driver.Value, error) {
	res, _ := json.Marshal(e)
	return res, nil
}
func (e *Tags) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.(string))
		json.Unmarshal(s, &e)
	}
	return nil
}
func (e Tags) Value() (driver.Value, error) {
	res, _ := json.Marshal(e)
	return res, nil
}
func (e *CloudTrailNotifications) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.(string))
		json.Unmarshal(s, &e)
	}
	return nil
}
func (e CloudTrailNotifications) Value() (driver.Value, error) {
	res, _ := json.Marshal(e)
	return res, nil
}
func (e *ExecutionRequestCustom) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.(string))
		json.Unmarshal(s, &e)
	}
	return nil
}
func (e ExecutionRequestCustom) Value() (driver.Value, error) {
	res, _ := json.Marshal(e)
	return res, nil
}
func (tjs *TemplateJSONSchema) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.([]uint8))
		json.Unmarshal(s, &tjs)
	}
	return nil
}
func (tjs TemplateJSONSchema) Value() (driver.Value, error) {
	res, _ := json.Marshal(tjs)
	return res, nil
}
func (tjs *TemplatePayload) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.([]uint8))
		json.Unmarshal(s, &tjs)
	}
	return nil
}
func (tjs TemplatePayload) Value() (driver.Value, error) {
	res, _ := json.Marshal(tjs)
	return res, nil
}
func (e Labels) Value() (driver.Value, error) {
	res, _ := json.Marshal(e)
	return res, nil
}
func (e *Labels) Scan(value interface{}) error {
	if value != nil {
		s := []byte(value.(string))
		json.Unmarshal(s, &e)
	}
	return nil
}
func (sm *SQLStateManager) GetTemplateByID(templateID string) (Template, error) {
	var err error
	var tpl Template
	err = sm.db.Get(&tpl, GetTemplateByIDSQL, templateID)
	if err != nil {
		if err == sql.ErrNoRows {
			return tpl, exceptions.MissingResource{
				ErrorString: fmt.Sprintf("Template with ID %s not found", templateID)}
		}
		return tpl, errors.Wrapf(err, "issue getting tpl with id [%s]", templateID)
	}
	return tpl, nil
}
func (sm *SQLStateManager) GetTemplateByVersion(templateName string, templateVersion int64) (bool, Template, error) {
	var err error
	var tpl Template
	err = sm.db.Get(&tpl, GetTemplateByVersionSQL, templateName, templateVersion)
	if err != nil {
		if err == sql.ErrNoRows {
			return false, tpl, nil
		}
		return false, tpl, errors.Wrapf(err, "issue getting tpl with id [%s]", templateName)
	}
	return true, tpl, nil
}
func (sm *SQLStateManager) GetLatestTemplateByTemplateName(templateName string) (bool, Template, error) {
	var err error
	var tpl Template
	err = sm.db.Get(&tpl, GetTemplateLatestOnlySQL, templateName)
	if err != nil {
		if err == sql.ErrNoRows {
			return false, tpl, nil
		}
		return false, tpl, errors.Wrapf(err, "issue getting tpl with id [%s]", templateName)
	}
	return true, tpl, nil
}
func (sm *SQLStateManager) ListTemplates(limit int, offset int, sortBy string, order string) (TemplateList, error) {
	var err error
	var result TemplateList
	var orderQuery string
	orderQuery, err = sm.orderBy(&Template{}, sortBy, order)
	if err != nil {
		return result, errors.WithStack(err)
	}
	sql := fmt.Sprintf(ListTemplatesSQL, orderQuery)
	countSQL := fmt.Sprintf("select COUNT(*) from (%s) as sq", sql)
	err = sm.db.Select(&result.Templates, sql, limit, offset)
	if err != nil {
		return result, errors.Wrap(err, "issue running list templates sql")
	}
	err = sm.db.Get(&result.Total, countSQL, nil, 0)
	if err != nil {
		return result, errors.Wrap(err, "issue running list templates count sql")
	}
	return result, nil
}
func (sm *SQLStateManager) ListTemplatesLatestOnly(limit int, offset int, sortBy string, order string) (TemplateList, error) {
	var err error
	var result TemplateList
	countSQL := fmt.Sprintf("select COUNT(*) from (%s) as sq", ListTemplatesLatestOnlySQL)
	err = sm.db.Select(&result.Templates, ListTemplatesLatestOnlySQL, limit, offset)
	if err != nil {
		return result, errors.Wrap(err, "issue running list templates sql")
	}
	err = sm.db.Get(&result.Total, countSQL, nil, 0)
	if err != nil {
		return result, errors.Wrap(err, "issue running list templates count sql")
	}
	return result, nil
}
func (sm *SQLStateManager) CreateTemplate(t Template) error {
	var err error
	insert := `
    INSERT INTO template(
			template_id, template_name, version, schema, command_template,
			adaptive_resource_allocation, image, memory, env, cpu, gpu, defaults, avatar_uri
    )
    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15);
    `
	tx, err := sm.db.Begin()
	if err != nil {
		return errors.WithStack(err)
	}
	if _, err = tx.Exec(insert,
		t.TemplateID, t.TemplateName, t.Version, t.Schema, t.CommandTemplate,
		t.AdaptiveResourceAllocation, t.Image, t.Memory, t.Env,
		t.Cpu, t.Gpu, t.Defaults, t.AvatarURI); err != nil {
		tx.Rollback()
		return errors.Wrapf(
			err, "issue creating new template with template_name [%s] and version [%d]", t.TemplateName, t.Version)
	}
	err = tx.Commit()
	if err != nil {
		return errors.WithStack(err)
	}
	return nil
}
func (sm *SQLStateManager) GetExecutableByTypeAndID(t ExecutableType, id string) (Executable, error) {
	switch t {
	case ExecutableTypeDefinition:
		return sm.GetDefinition(id)
	case ExecutableTypeTemplate:
		return sm.GetTemplateByID(id)
	default:
		return nil, exceptions.MalformedInput{
			ErrorString: fmt.Sprintf("executable type of [%s] not valid.", t),
		}
	}
}
func (sm *SQLStateManager) logStatusUpdate(update Run) {
	var err error
	var startedAt, finishedAt time.Time
	var duration float64
	var env EnvList
	var command string
	if update.StartedAt != nil {
		startedAt = *update.StartedAt
		duration = time.Now().Sub(startedAt).Seconds()
	}
	if update.FinishedAt != nil {
		finishedAt = *update.FinishedAt
		duration = finishedAt.Sub(startedAt).Seconds()
	}
	if update.Env != nil {
		env = *update.Env
	}
	if update.Command != nil {
		command = *update.Command
	}
	if update.ExitCode != nil {
		err = sm.log.Event("eventClassName", "FlotillaTaskStatus",
			"run_id", update.RunID,
			"definition_id", update.DefinitionID,
			"alias", update.Alias,
			"image", update.Image,
			"cluster_name", update.ClusterName,
			"command", command,
			"exit_code", *update.ExitCode,
			"status", update.Status,
			"started_at", startedAt,
			"finished_at", finishedAt,
			"duration", duration,
			"instance_id", update.InstanceID,
			"instance_dns_name", update.InstanceDNSName,
			"group_name", update.GroupName,
			"user", update.User,
			"task_type", update.TaskType,
			"env", env,
			"executable_id", update.ExecutableID,
			"executable_type", update.ExecutableType,
			"Tier", update.Tier)
	} else {
		err = sm.log.Event("eventClassName", "FlotillaTaskStatus",
			"run_id", update.RunID,
			"definition_id", update.DefinitionID,
			"alias", update.Alias,
			"image", update.Image,
			"cluster_name", update.ClusterName,
			"command", command,
			"status", update.Status,
			"started_at", startedAt,
			"finished_at", finishedAt,
			"duration", duration,
			"instance_id", update.InstanceID,
			"instance_dns_name", update.InstanceDNSName,
			"group_name", update.GroupName,
			"user", update.User,
			"task_type", update.TaskType,
			"env", env,
			"executable_id", update.ExecutableID,
			"executable_type", update.ExecutableType,
			"Tier", update.Tier)
	}
	if err != nil {
		sm.log.Log("message", "Failed to emit status event", "run_id", update.RunID, "error", err.Error())
	}
}
func (sm *SQLStateManager) ListClusterStates() ([]ClusterMetadata, error) {
	var clusters []ClusterMetadata
	err := sm.db.Select(&clusters, ListClusterStatesSQL)
	return clusters, err
}
func (sm *SQLStateManager) UpdateClusterMetadata(cluster ClusterMetadata) error {
	sql := `
        INSERT INTO cluster_state (name, status, status_reason, status_since, allowed_tiers, capabilities, updated_at, namespace, region, emr_virtual_cluster)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
        ON CONFLICT (name) DO UPDATE
        SET status = $2,
            status_reason = $3,
            status_since = CASE WHEN status != $2 THEN NOW() ELSE status_since END,
            allowed_tiers = $5,
            capabilities = $6,
            updated_at = NOW(),
            namespace = $8,
            region = $9,
            emr_virtual_cluster = $10;
    `
	result, err := sm.db.Exec(sql,
		cluster.Name,
		cluster.Status,
		cluster.StatusReason,
		cluster.StatusSince,
		cluster.AllowedTiers,
		cluster.Capabilities,
		cluster.UpdatedAt,
		cluster.Namespace,
		cluster.Region,
		cluster.EMRVirtualCluster)
	if err != nil {
		return err
	}
	rows, err := result.RowsAffected()
	if err != nil {
		return err
	}
	if rows == 0 {
		return exceptions.MissingResource{
			ErrorString: fmt.Sprintf("Cluster %s not found", cluster.Name),
		}
	}
	return nil
}
func scanStringArray(value interface{}) ([]string, error) {
	if value == nil {
		return []string{}, nil
	}
	switch v := value.(type) {
	case []byte:
		str := string(v)
		if len(str) < 2 {
			return []string{}, nil
		}
		elements := strings.Split(str[1:len(str)-1], ",")
		result := make([]string, 0, len(elements))
		for _, e := range elements {
			if e != "" {
				result = append(result, e)
			}
		}
		return result, nil
	default:
		return nil, fmt.Errorf("unexpected type for string array: %T", value)
	}
}
func stringArrayValue(arr []string) (driver.Value, error) {
	if len(arr) == 0 {
		return "{}", nil
	}
	return fmt.Sprintf("{%s}", strings.Join(arr, ",")), nil
}
func (arr *Tiers) Scan(value interface{}) error {
	result, err := scanStringArray(value)
	if err != nil {
		return err
	}
	*arr = Tiers(result)
	return nil
}
func (arr Tiers) Value() (driver.Value, error) {
	return stringArrayValue([]string(arr))
}
func (arr *Capabilities) Scan(value interface{}) error {
	result, err := scanStringArray(value)
	if err != nil {
		return err
	}
	*arr = Capabilities(result)
	return nil
}
func (arr Capabilities) Value() (driver.Value, error) {
	return stringArrayValue([]string(arr))
}

================
File: testutils/mocks.go
================
package testutils
import (
	"fmt"
	"math"
	"net/http"
	"testing"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/execution/engine"
	"github.com/stitchfix/flotilla-os/queue"
	"github.com/stitchfix/flotilla-os/state"
)
type ImplementsAllTheThings struct {
	T                       *testing.T
	Calls                   []string
	Definitions             map[string]state.Definition
	Runs                    map[string]state.Run
	Workers                 []state.Worker
	Qurls                   map[string]string
	Defined                 []string
	Queued                  []string
	StatusUpdates           []string
	StatusUpdatesAsRuns     []state.Run
	ExecuteError            error
	ExecuteErrorIsRetryable bool
	Groups                  []string
	Tags                    []string
	Templates               map[string]state.Template
	ClusterStates           []state.ClusterMetadata
}
func (iatt *ImplementsAllTheThings) ListClusters() ([]state.ClusterMetadata, error) {
	iatt.Calls = append(iatt.Calls, "ListClusters")
	return iatt.ClusterStates, nil
}
func (i *ImplementsAllTheThings) ListClusterStates() ([]state.ClusterMetadata, error) {
	i.Calls = append(i.Calls, "ListClusterStates")
	fmt.Printf("ListClusterStates called, returning %d clusters\n", len(i.ClusterStates))
	return i.ClusterStates, nil
}
func (i *ImplementsAllTheThings) UpdateClusterMetadata(cluster state.ClusterMetadata) error {
	i.Calls = append(i.Calls, "UpdateClusterMetadata")
	return nil
}
func (iatt *ImplementsAllTheThings) LogsText(executable state.Executable, run state.Run, w http.ResponseWriter) error {
	iatt.Calls = append(iatt.Calls, "LogsText")
	return nil
}
func (iatt *ImplementsAllTheThings) Log(keyvals ...interface{}) error {
	iatt.Calls = append(iatt.Calls, "Name")
	return nil
}
func (iatt *ImplementsAllTheThings) Event(keyvals ...interface{}) error {
	iatt.Calls = append(iatt.Calls, "Name")
	return nil
}
func (iatt *ImplementsAllTheThings) Name() string {
	iatt.Calls = append(iatt.Calls, "Name")
	return "implementer"
}
func (iatt *ImplementsAllTheThings) Initialize(conf config.Config) error {
	iatt.Calls = append(iatt.Calls, "Initialize")
	return nil
}
func (iatt *ImplementsAllTheThings) Cleanup() error {
	iatt.Calls = append(iatt.Calls, "Cleanup")
	return nil
}
func (iatt *ImplementsAllTheThings) ListFailingNodes() (state.NodeList, error) {
	var nodeList state.NodeList
	iatt.Calls = append(iatt.Calls, "ListFailingNodes")
	return nodeList, nil
}
func (iatt *ImplementsAllTheThings) GetPodReAttemptRate() (float32, error) {
	iatt.Calls = append(iatt.Calls, "GetPodReAttemptRate")
	return 1.0, nil
}
func (iatt *ImplementsAllTheThings) GetNodeLifecycle(executableID string, commandHash string) (string, error) {
	iatt.Calls = append(iatt.Calls, "GetNodeLifecycle")
	return "spot", nil
}
func (iatt *ImplementsAllTheThings) GetTaskHistoricalRuntime(executableID string, runId string) (float32, error) {
	iatt.Calls = append(iatt.Calls, "GetTaskHistoricalRuntime")
	return 1.0, nil
}
func (iatt *ImplementsAllTheThings) ListDefinitions(
	limit int, offset int, sortBy string,
	order string, filters map[string][]string,
	envFilters map[string]string) (state.DefinitionList, error) {
	iatt.Calls = append(iatt.Calls, "ListDefinitions")
	dl := state.DefinitionList{Total: len(iatt.Definitions)}
	for _, d := range iatt.Definitions {
		dl.Definitions = append(dl.Definitions, d)
	}
	return dl, nil
}
func (iatt *ImplementsAllTheThings) GetDefinition(definitionID string) (state.Definition, error) {
	iatt.Calls = append(iatt.Calls, "GetDefinition")
	var err error
	d, ok := iatt.Definitions[definitionID]
	if !ok {
		err = fmt.Errorf("No definition %s", definitionID)
	}
	return d, err
}
func (iatt *ImplementsAllTheThings) GetDefinitionByAlias(alias string) (state.Definition, error) {
	iatt.Calls = append(iatt.Calls, "GetDefinitionByAlias")
	for _, d := range iatt.Definitions {
		if d.Alias == alias {
			return d, nil
		}
	}
	return state.Definition{}, fmt.Errorf("No definition with alias %s", alias)
}
func (iatt *ImplementsAllTheThings) UpdateDefinition(definitionID string, updates state.Definition) (state.Definition, error) {
	iatt.Calls = append(iatt.Calls, "UpdateDefinition")
	defn := iatt.Definitions[definitionID]
	defn.UpdateWith(updates)
	iatt.Definitions[definitionID] = defn
	return defn, nil
}
func (iatt *ImplementsAllTheThings) CreateDefinition(d state.Definition) error {
	iatt.Calls = append(iatt.Calls, "CreateDefinition")
	iatt.Definitions[d.DefinitionID] = d
	return nil
}
func (iatt *ImplementsAllTheThings) DeleteDefinition(definitionID string) error {
	iatt.Calls = append(iatt.Calls, "DeleteDefinition")
	delete(iatt.Definitions, definitionID)
	return nil
}
func (iatt *ImplementsAllTheThings) ListRuns(limit int, offset int, sortBy string, order string, filters map[string][]string, envFilters map[string]string, engines []string) (state.RunList, error) {
	iatt.Calls = append(iatt.Calls, "ListRuns")
	rl := state.RunList{Total: len(iatt.Runs)}
	for _, r := range iatt.Runs {
		rl.Runs = append(rl.Runs, r)
	}
	return rl, nil
}
func (iatt *ImplementsAllTheThings) GetRun(runID string) (state.Run, error) {
	iatt.Calls = append(iatt.Calls, "GetRun")
	var err error
	r, ok := iatt.Runs[runID]
	if !ok {
		err = fmt.Errorf("No run %s", runID)
	}
	return r, err
}
func (iatt *ImplementsAllTheThings) GetRunByEMRJobId(emrJobId string) (state.Run, error) {
	iatt.Calls = append(iatt.Calls, "GetRunByEMRJobId")
	var err error
	r, ok := iatt.Runs[emrJobId]
	if !ok {
		err = fmt.Errorf("No run %s", emrJobId)
	}
	return r, err
}
func (iatt *ImplementsAllTheThings) CreateRun(r state.Run) error {
	iatt.Calls = append(iatt.Calls, "CreateRun")
	iatt.Runs[r.RunID] = r
	return nil
}
func (iatt *ImplementsAllTheThings) EstimateRunResources(executableID string, command string) (state.TaskResources, error) {
	iatt.Calls = append(iatt.Calls, "EstimateRunResources")
	return state.TaskResources{}, nil
}
func (iatt *ImplementsAllTheThings) EstimateExecutorCount(executableID string, commandHash string) (int64, error) {
	iatt.Calls = append(iatt.Calls, "EstimateExecutorCount")
	return 0, nil
}
func (iatt *ImplementsAllTheThings) ExecutorOOM(executableID string, commandHash string) (bool, error) {
	iatt.Calls = append(iatt.Calls, "ExecutorOOM")
	return false, nil
}
func (iatt *ImplementsAllTheThings) DriverOOM(executableID string, commandHash string) (bool, error) {
	iatt.Calls = append(iatt.Calls, "DriverOOM")
	return false, nil
}
func (iatt *ImplementsAllTheThings) UpdateRun(runID string, updates state.Run) (state.Run, error) {
	iatt.Calls = append(iatt.Calls, "UpdateRun")
	run := iatt.Runs[runID]
	run.UpdateWith(updates)
	iatt.Runs[runID] = run
	return run, nil
}
func (iatt *ImplementsAllTheThings) ListGroups(limit int, offset int, name *string) (state.GroupsList, error) {
	iatt.Calls = append(iatt.Calls, "ListGroups")
	return state.GroupsList{Total: len(iatt.Groups), Groups: iatt.Groups}, nil
}
func (iatt *ImplementsAllTheThings) ListTags(limit int, offset int, name *string) (state.TagsList, error) {
	iatt.Calls = append(iatt.Calls, "ListTags")
	return state.TagsList{Total: len(iatt.Tags), Tags: iatt.Tags}, nil
}
func (iatt *ImplementsAllTheThings) initWorkerTable(c config.Config) error {
	iatt.Calls = append(iatt.Calls, "initWorkerTable")
	return nil
}
func (iatt *ImplementsAllTheThings) ListWorkers(engine string) (state.WorkersList, error) {
	iatt.Calls = append(iatt.Calls, "ListWorkers")
	return state.WorkersList{Total: len(iatt.Workers), Workers: iatt.Workers}, nil
}
func (iatt *ImplementsAllTheThings) CheckIdempotenceKey(idempotenceKey string) (string, error) {
	iatt.Calls = append(iatt.Calls, "CheckIdempotenceKey")
	return "42", nil
}
func (iatt *ImplementsAllTheThings) GetWorker(workerType string, engine string) (state.Worker, error) {
	iatt.Calls = append(iatt.Calls, "GetWorker")
	return state.Worker{WorkerType: workerType, CountPerInstance: 2}, nil
}
func (iatt *ImplementsAllTheThings) UpdateWorker(workerType string, updates state.Worker) (state.Worker, error) {
	iatt.Calls = append(iatt.Calls, "UpdateWorker")
	return state.Worker{WorkerType: workerType, CountPerInstance: updates.CountPerInstance}, nil
}
func (iatt *ImplementsAllTheThings) BatchUpdateWorkers(updates []state.Worker) (state.WorkersList, error) {
	iatt.Calls = append(iatt.Calls, "BatchUpdateWorkers")
	return state.WorkersList{Total: len(iatt.Workers), Workers: iatt.Workers}, nil
}
func (iatt *ImplementsAllTheThings) QurlFor(name string, prefixed bool) (string, error) {
	iatt.Calls = append(iatt.Calls, "QurlFor")
	qurl, _ := iatt.Qurls[name]
	return qurl, nil
}
func (iatt *ImplementsAllTheThings) Enqueue(run state.Run) error {
	iatt.Calls = append(iatt.Calls, "Enqueue")
	iatt.Queued = append(iatt.Queued, run.RunID)
	return nil
}
func (iatt *ImplementsAllTheThings) ReceiveRun(qURL string) (queue.RunReceipt, error) {
	iatt.Calls = append(iatt.Calls, "ReceiveRun")
	if len(iatt.Queued) == 0 {
		return queue.RunReceipt{}, nil
	}
	popped := iatt.Queued[0]
	iatt.Queued = iatt.Queued[1:]
	receipt := queue.RunReceipt{
		Run: &state.Run{RunID: popped},
	}
	receipt.Done = func() error {
		iatt.Calls = append(iatt.Calls, "RunReceipt.Done")
		return nil
	}
	return receipt, nil
}
func (iatt *ImplementsAllTheThings) ReceiveStatus(qURL string) (queue.StatusReceipt, error) {
	iatt.Calls = append(iatt.Calls, "ReceiveStatus")
	if len(iatt.StatusUpdates) == 0 {
		return queue.StatusReceipt{}, nil
	}
	popped := iatt.StatusUpdates[0]
	iatt.StatusUpdates = iatt.StatusUpdates[1:]
	receipt := queue.StatusReceipt{
		StatusUpdate: &popped,
	}
	receipt.Done = func() error {
		iatt.Calls = append(iatt.Calls, "RunReceipt.Done")
		return nil
	}
	return receipt, nil
}
func (iatt *ImplementsAllTheThings) List() ([]string, error) {
	iatt.Calls = append(iatt.Calls, "List")
	res := make([]string, len(iatt.Qurls))
	i := 0
	for _, qurl := range iatt.Qurls {
		res[i] = qurl
		i++
	}
	return res, nil
}
func (iatt *ImplementsAllTheThings) GetEvents(run state.Run) (state.PodEventList, error) {
	iatt.Calls = append(iatt.Calls, "GetEvents")
	return state.PodEventList{
		Total:     0,
		PodEvents: nil,
	}, nil
}
func (iatt *ImplementsAllTheThings) FetchUpdateStatus(run state.Run) (state.Run, error) {
	iatt.Calls = append(iatt.Calls, "FetchUpdateStatus")
	return run, nil
}
func (iatt *ImplementsAllTheThings) FetchPodMetrics(run state.Run) (state.Run, error) {
	iatt.Calls = append(iatt.Calls, "FetchPodMetrics")
	return run, nil
}
func (iatt *ImplementsAllTheThings) CanBeRun(clusterName string, executableResources state.ExecutableResources) (bool, error) {
	iatt.Calls = append(iatt.Calls, "CanBeRun")
	if clusterName == "invalidcluster" {
		return false, nil
	}
	return true, nil
}
func (iatt *ImplementsAllTheThings) IsImageValid(imageRef string) (bool, error) {
	iatt.Calls = append(iatt.Calls, "IsImageValid")
	if imageRef == "invalidimage" {
		return false, nil
	}
	return true, nil
}
func (iatt *ImplementsAllTheThings) PollRunStatus() (state.Run, error) {
	iatt.Calls = append(iatt.Calls, "PollRunStatus")
	return state.Run{}, nil
}
func (iatt *ImplementsAllTheThings) PollRuns() ([]engine.RunReceipt, error) {
	iatt.Calls = append(iatt.Calls, "PollRuns")
	var r []engine.RunReceipt
	if len(iatt.Queued) == 0 {
		return r, nil
	}
	popped := iatt.Queued[0]
	iatt.Queued = iatt.Queued[1:]
	receipt := queue.RunReceipt{
		Run: &state.Run{RunID: popped},
	}
	receipt.Done = func() error {
		iatt.Calls = append(iatt.Calls, "RunReceipt.Done")
		return nil
	}
	r = append(r, engine.RunReceipt{receipt})
	return r, nil
}
func (iatt *ImplementsAllTheThings) PollStatus() (engine.RunReceipt, error) {
	iatt.Calls = append(iatt.Calls, "PollStatus")
	if len(iatt.StatusUpdatesAsRuns) == 0 {
		return engine.RunReceipt{}, nil
	}
	popped := iatt.StatusUpdatesAsRuns[0]
	iatt.StatusUpdatesAsRuns = iatt.StatusUpdatesAsRuns[1:]
	receipt := queue.RunReceipt{
		Run: &popped,
	}
	receipt.Done = func() error {
		iatt.Calls = append(iatt.Calls, "StatusReceipt.Done")
		return nil
	}
	return engine.RunReceipt{receipt}, nil
}
func (iatt *ImplementsAllTheThings) Execute(executable state.Executable, run state.Run, manager state.Manager) (state.Run, bool, error) {
	iatt.Calls = append(iatt.Calls, "Execute")
	return state.Run{}, iatt.ExecuteErrorIsRetryable, iatt.ExecuteError
}
func (iatt *ImplementsAllTheThings) Terminate(run state.Run) error {
	iatt.Calls = append(iatt.Calls, "Terminate")
	return nil
}
func (iatt *ImplementsAllTheThings) Define(definition state.Definition) (state.Definition, error) {
	iatt.Calls = append(iatt.Calls, "Define")
	iatt.Defined = append(iatt.Defined, definition.DefinitionID)
	return definition, nil
}
func (iatt *ImplementsAllTheThings) Deregister(definition state.Definition) error {
	iatt.Calls = append(iatt.Calls, "Deregister")
	return nil
}
func (iatt *ImplementsAllTheThings) Logs(executable state.Executable, run state.Run, lastSeen *string, role *string, facility *string) (string, *string, error) {
	iatt.Calls = append(iatt.Calls, "Logs")
	return "", aws.String(""), nil
}
// GetExecutableByTypeAndID - StateManager
func (iatt *ImplementsAllTheThings) GetExecutableByTypeAndID(t state.ExecutableType, id string) (state.Executable, error) {
	iatt.Calls = append(iatt.Calls, "GetExecutableByTypeAndID")
	switch t {
	case state.ExecutableTypeDefinition:
		return iatt.GetDefinition(id)
	case state.ExecutableTypeTemplate:
		return iatt.GetTemplateByID(id)
	default:
		return nil, fmt.Errorf("Invalid executable type %s", t)
	}
}
func (iatt *ImplementsAllTheThings) ListTemplates(limit int, offset int, sortBy string, order string) (state.TemplateList, error) {
	iatt.Calls = append(iatt.Calls, "ListTemplates")
	tl := state.TemplateList{Total: len(iatt.Templates)}
	for _, t := range iatt.Templates {
		tl.Templates = append(tl.Templates, t)
	}
	return tl, nil
}
func (iatt *ImplementsAllTheThings) ListTemplatesLatestOnly(limit int, offset int, sortBy string, order string) (state.TemplateList, error) {
	iatt.Calls = append(iatt.Calls, "ListTemplatesLatestOnly")
	tl := state.TemplateList{Total: len(iatt.Templates)}
	for _, t := range iatt.Templates {
		tl.Templates = append(tl.Templates, t)
	}
	return tl, nil
}
func (iatt *ImplementsAllTheThings) GetTemplateByVersion(templateName string, templateVersion int64) (bool, state.Template, error) {
	iatt.Calls = append(iatt.Calls, "GetTemplateByVersion")
	var err error
	var tpl *state.Template
	for _, t := range iatt.Templates {
		if t.TemplateName == templateName && t.Version == templateVersion {
			tpl = &t
		}
	}
	if tpl == nil {
		return false, *tpl, fmt.Errorf("No template with name: %s", templateName)
	}
	return true, *tpl, err
}
func (iatt *ImplementsAllTheThings) GetTemplateByID(id string) (state.Template, error) {
	iatt.Calls = append(iatt.Calls, "GetTemplateByID")
	var err error
	t, ok := iatt.Templates[id]
	if !ok {
		err = fmt.Errorf("No template %s", id)
	}
	return t, err
}
func (iatt *ImplementsAllTheThings) GetLatestTemplateByTemplateName(templateName string) (bool, state.Template, error) {
	iatt.Calls = append(iatt.Calls, "GetLatestTemplateByTemplateName")
	var err error
	var tpl *state.Template
	var maxVersion int64 = int64(math.Inf(-1))
	for _, t := range iatt.Templates {
		if t.TemplateName == templateName && t.Version > maxVersion {
			tpl = &t
			maxVersion = t.Version
		}
	}
	if tpl == nil {
		return false, *tpl, fmt.Errorf("No template with name: %s", templateName)
	}
	return true, *tpl, err
}
func (iatt *ImplementsAllTheThings) CreateTemplate(t state.Template) error {
	iatt.Calls = append(iatt.Calls, "CreateTemplate")
	iatt.Templates[t.TemplateID] = t
	return nil
}

================
File: utils/utils.go
================
package utils
import (
	"github.com/pkg/errors"
	"reflect"
)
func StringSliceContains(s []string, e string) bool {
	for _, a := range s {
		if a == e {
			return true
		}
	}
	return false
}
func MergeMaps(a *map[string]interface{}, b map[string]interface{}) error {
	return mergeMapsRecursive(a, b)
}
func mergeMapsRecursive(a *map[string]interface{}, b map[string]interface{}) error {
	for k, v := range b {
		if reflect.TypeOf(v).Kind() == reflect.Map {
			if _, ok := (*a)[k]; !ok {
				(*a)[k] = v
			} else {
				aVal, ok := (*a)[k].(map[string]interface{})
				bVal, ok := v.(map[string]interface{})
				if !ok {
					return errors.New("unable to cast interface{} to map[string]interface{}")
				}
				if err := mergeMapsRecursive(&aVal, bVal); err != nil {
					return err
				}
			}
		} else {
			if _, ok := (*a)[k]; !ok {
				(*a)[k] = v
			}
		}
	}
	return nil
}

================
File: worker/cloudtrail_worker.go
================
package worker
import (
	"encoding/json"
	"fmt"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/aws/aws-sdk-go/aws/session"
	"github.com/aws/aws-sdk-go/service/s3"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/execution/engine"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/queue"
	"github.com/stitchfix/flotilla-os/state"
	"gopkg.in/tomb.v2"
	"strings"
	"time"
	awstrace "gopkg.in/DataDog/dd-trace-go.v1/contrib/aws/aws-sdk-go/aws"
)
type cloudtrailWorker struct {
	sm           state.Manager
	qm           queue.Manager
	conf         config.Config
	log          flotillaLog.Logger
	pollInterval time.Duration
	t            tomb.Tomb
	queue        string
	engine       *string
	s3Client     *s3.S3
}
func (ctw *cloudtrailWorker) Initialize(conf config.Config, sm state.Manager, eksEngine engine.Engine, emrEngine engine.Engine, log flotillaLog.Logger, pollInterval time.Duration, qm queue.Manager) error {
	ctw.pollInterval = pollInterval
	ctw.conf = conf
	ctw.sm = sm
	ctw.qm = qm
	ctw.log = log
	ctw.queue = conf.GetString("cloudtrail_queue")
	_ = ctw.qm.Initialize(ctw.conf, "eks")
	awsRegion := conf.GetString("eks_manifest_storage_options_region")
	sess := awstrace.WrapSession(session.Must(session.NewSession(&aws.Config{Region: aws.String(awsRegion)})))
	ctw.s3Client = s3.New(sess, aws.NewConfig().WithRegion(awsRegion))
	return nil
}
func (ctw *cloudtrailWorker) GetTomb() *tomb.Tomb {
	return &ctw.t
}
func (ctw *cloudtrailWorker) Run() error {
	for {
		select {
		case <-ctw.t.Dying():
			_ = ctw.log.Log("message", "A CloudTrail worker was terminated")
			return nil
		default:
			ctw.runOnce()
			time.Sleep(ctw.pollInterval)
		}
	}
}
func (ctw *cloudtrailWorker) runOnce() {
	qurl, err := ctw.qm.QurlFor(ctw.queue, false)
	if err != nil {
		_ = ctw.log.Log("message", "Error receiving CloudTrail queue", "error", fmt.Sprintf("%+v", err))
		return
	}
	cloudTrailS3File, err := ctw.qm.ReceiveCloudTrail(qurl)
	if err != nil {
		_ = ctw.log.Log("message", "Error receiving CloudTrail file", "error", fmt.Sprintf("%+v", err))
		return
	}
	ctw.processS3Keys(cloudTrailS3File)
}
func (ctw *cloudtrailWorker) processS3Keys(cloudTrailS3File state.CloudTrailS3File) {
	var ctn state.CloudTrailNotifications
	defaultRegion := ctw.conf.GetString("aws_default_region")
	for _, keyName := range cloudTrailS3File.S3ObjectKey {
		if !strings.Contains(keyName, defaultRegion) {
			continue
		}
		getObjectOutput, err := ctw.s3Client.GetObject(&s3.GetObjectInput{
			Bucket: aws.String(cloudTrailS3File.S3Bucket),
			Key:    aws.String(keyName),
		})
		if err != nil {
			_ = ctw.log.Log("message", "Error receiving CloudTrail file - no object", "error", fmt.Sprintf("%+v", err))
			return
		}
		err = json.NewDecoder(getObjectOutput.Body).Decode(&ctn)
		ctw.processCloudTrailNotifications(ctn)
		getObjectOutput.Body.Close()
	}
}
func (ctw *cloudtrailWorker) processCloudTrailNotifications(ctn state.CloudTrailNotifications) {
	sa := ctw.conf.GetString("eks_default_service_account")
	runIdRecordMap := make(map[string][]state.Record)
	for _, record := range ctn.Records {
		if strings.Contains(record.UserIdentity.Arn, sa) && strings.Contains(record.UserIdentity.Arn, "eks-") {
			runId := ctw.getRunId(record)
			runIdRecordMap[runId] = append(runIdRecordMap[runId], record)
		}
	}
	for runId, records := range runIdRecordMap {
		run, err := ctw.sm.GetRun(runId)
		if err == nil {
			var rawRecords []state.Record
			if run.CloudTrailNotifications == nil || len((*run.CloudTrailNotifications).Records) == 0 {
				rawRecords = records
			} else {
				rawRecords = append((*run.CloudTrailNotifications).Records, records...)
			}
			run.CloudTrailNotifications = &state.CloudTrailNotifications{Records: ctw.makeSet(rawRecords)}
			_, err = ctw.sm.UpdateRun(runId, run)
			if err != nil {
				_ = ctw.log.Log("message", "Error updating run", "error", fmt.Sprintf("%+v", err))
			}
		}
	}
}
func (ctw *cloudtrailWorker) makeSet(records []state.Record) []state.Record {
	keys := make(map[string]bool)
	var set []state.Record
	for _, record := range records {
		if _, value := keys[record.String()]; !value {
			keys[record.String()] = true
			set = append(set, record)
		}
	}
	return set
}
func (ctw *cloudtrailWorker) getRunId(record state.Record) string {
	splits := strings.Split(record.UserIdentity.Arn, "/")
	runId := splits[len(splits)-1]
	return runId
}

================
File: worker/events_worker.go
================
package worker
import (
	"context"
	"fmt"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/aws/aws-sdk-go/service/s3"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/execution/engine"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/queue"
	"github.com/stitchfix/flotilla-os/state"
	kubernetestrace "gopkg.in/DataDog/dd-trace-go.v1/contrib/k8s.io/client-go/kubernetes"
	"gopkg.in/tomb.v2"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
	"regexp"
	"strings"
	"time"
)
type eventsWorker struct {
	sm                state.Manager
	qm                queue.Manager
	conf              config.Config
	log               flotillaLog.Logger
	pollInterval      time.Duration
	t                 tomb.Tomb
	queue             string
	emrJobStatusQueue string
	s3Client          *s3.S3
	kClientSet        map[string]kubernetes.Clientset
	emrHistoryServer  string
	emrAppServer      map[string]string
	emrMetricsServer  string
	eksMetricsServer  string
	emrMaxPodEvents   int
	eksEngine         engine.Engine
	emrEngine         engine.Engine
}
func (ew *eventsWorker) Initialize(conf config.Config, sm state.Manager, eksEngine engine.Engine, emrEngine engine.Engine, log flotillaLog.Logger, pollInterval time.Duration, qm queue.Manager) error {
	ew.pollInterval = pollInterval
	ew.conf = conf
	ew.sm = sm
	ew.qm = qm
	ew.log = log
	ew.eksEngine = eksEngine
	ew.emrEngine = emrEngine
	eventsQueue, err := ew.qm.QurlFor(conf.GetString("eks_events_queue"), false)
	emrJobStatusQueue, err := ew.qm.QurlFor(conf.GetString("emr_job_status_queue"), false)
	ew.emrHistoryServer = conf.GetString("emr_history_server_uri")
	ew.emrAppServer = conf.GetStringMapString("emr_app_server_uri")
	ew.emrMetricsServer = conf.GetString("emr_metrics_server_uri")
	ew.eksMetricsServer = conf.GetString("eks_metrics_server_uri")
	if conf.IsSet("emr_max_attempt_count") {
		ew.emrMaxPodEvents = conf.GetInt("emr_max_pod_events")
	} else {
		ew.emrMaxPodEvents = 20000
	}
	if err != nil {
		_ = ew.log.Log("message", "Error receiving Kubernetes Event queue", "error", fmt.Sprintf("%+v", err))
		return nil
	}
	ew.queue = eventsQueue
	ew.emrJobStatusQueue = emrJobStatusQueue
	_ = ew.qm.Initialize(ew.conf, "eks")
	ew.kClientSet = make(map[string]kubernetes.Clientset)
	clusters := strings.Split(conf.GetString("eks_clusters"), ",")
	for _, clusterName := range clusters {
		filename := fmt.Sprintf("%s/%s", conf.GetString("eks_kubeconfig_basepath"), clusterName)
		clientConf, err := clientcmd.BuildConfigFromFlags("", filename)
		clientConf.WrapTransport = kubernetestrace.WrapRoundTripper
		if err != nil {
			_ = ew.log.Log("message", "error initializing-eksEngine-clusters", "error", fmt.Sprintf("%+v", err))
			return err
		}
		kClient, err := kubernetes.NewForConfig(clientConf)
		if err != nil {
			_ = ew.log.Log("message", fmt.Sprintf("%+v", err))
			return err
		}
		ew.kClientSet[clusterName] = *kClient
	}
	return nil
}
func (ew *eventsWorker) GetTomb() *tomb.Tomb {
	return &ew.t
}
func (ew *eventsWorker) Run() error {
	for {
		select {
		case <-ew.t.Dying():
			_ = ew.log.Log("message", "A CloudTrail worker was terminated")
			return nil
		default:
			ew.runOnce()
			ew.runOnceEMR()
			time.Sleep(ew.pollInterval)
		}
	}
}
func (ew *eventsWorker) runOnceEMR() {
	emrEvent, err := ew.qm.ReceiveEMREvent(ew.emrJobStatusQueue)
	if err != nil {
		_ = ew.log.Log("message", "Error receiving EMR Events", "error", fmt.Sprintf("%+v", err))
		return
	}
	ew.processEventEMR(emrEvent)
}
func (ew *eventsWorker) processEventEMR(emrEvent state.EmrEvent) {
	if emrEvent.Detail == nil {
		return
	}
	emrJobId := emrEvent.Detail.ID
	run, err := ew.sm.GetRunByEMRJobId(*emrJobId)
	if err == nil {
		layout := "2020-08-31T17:27:50Z"
		timestamp, err := time.Parse(layout, *emrEvent.Time)
		if err != nil {
			timestamp = time.Now()
		}
		switch *emrEvent.Detail.State {
		case "COMPLETED":
			run.ExitCode = aws.Int64(0)
			run.Status = state.StatusStopped
			run.FinishedAt = &timestamp
			if run.StartedAt == nil || run.StartedAt.After(*run.FinishedAt) {
				run.StartedAt = run.QueuedAt
			}
			run.ExitReason = emrEvent.Detail.StateDetails
		case "RUNNING":
			run.Status = state.StatusRunning
			run.StartedAt = &timestamp
		case "FAILED":
			run.ExitCode = aws.Int64(-1)
			run.Status = state.StatusStopped
			run.FinishedAt = &timestamp
			if run.StartedAt == nil || run.StartedAt.After(*run.FinishedAt) {
				run.StartedAt = run.QueuedAt
			}
			run.ExitReason = aws.String("Job failed, please look at Driver Init and/or Driver Stdout logs.")
			if emrEvent.Detail != nil {
				if emrEvent.Detail.StateDetails != nil && !strings.Contains(*emrEvent.Detail.StateDetails, "JobRun failed. Please refer logs uploaded") {
					exitReason := strings.Replace(*emrEvent.Detail.StateDetails, "Please refer logs uploaded to S3/CloudWatch based on your monitoring configuration.", "", -1)
					run.ExitReason = aws.String(exitReason)
				} else {
					if emrEvent.Detail.FailureReason != nil && !strings.Contains(*emrEvent.Detail.FailureReason, "USER_ERROR") {
						exitReason := strings.Replace(*emrEvent.Detail.FailureReason, "Please refer logs uploaded to S3/CloudWatch based on your monitoring configuration.", "", -1)
						run.ExitReason = aws.String(exitReason)
					}
				}
			}
			if run.SparkExtension.DriverOOM != nil && *run.SparkExtension.DriverOOM == true {
				run.ExitReason = aws.String("Driver OOMKilled, retry with more driver memory.")
				run.ExitCode = aws.Int64(137)
			}
			if run.SparkExtension.ExecutorOOM != nil && *run.SparkExtension.ExecutorOOM == true {
				run.ExitReason = aws.String("Executor OOMKilled, retry with more executor memory.")
				run.ExitCode = aws.Int64(137)
			}
		case "SUBMITTED":
			run.Status = state.StatusPending
		}
		ew.setEMRMetricsUri(&run)
		_, err = ew.sm.UpdateRun(run.RunID, run)
		if err == nil {
			_ = emrEvent.Done()
		}
	}
}
func (ew *eventsWorker) runOnce() {
	kubernetesEvent, err := ew.qm.ReceiveKubernetesEvent(ew.queue)
	if err != nil {
		_ = ew.log.Log("message", "Error receiving Kubernetes Events", "error", fmt.Sprintf("%+v", err))
		return
	}
	ew.processEvent(kubernetesEvent)
}
func (ew *eventsWorker) processEMRPodEvents(kubernetesEvent state.KubernetesEvent) {
	ctx := context.Background()
	if kubernetesEvent.InvolvedObject.Kind == "Pod" {
		var emrJobId *string = nil
		var sparkAppId *string = nil
		var driverServiceName *string = nil
		var executorOOM *bool = nil
		var driverOOM *bool = nil
		kClient, ok := ew.kClientSet[kubernetesEvent.InvolvedObject.Labels.ClusterName]
		if ok {
			pod, err := kClient.CoreV1().Pods(kubernetesEvent.InvolvedObject.Namespace).Get(ctx, kubernetesEvent.InvolvedObject.Name, metav1.GetOptions{})
			if err == nil {
				for k, v := range pod.Labels {
					if emrJobId == nil && strings.Compare(k, "emr-containers.amazonaws.com/job.id") == 0 {
						emrJobId = aws.String(v)
					}
					if sparkAppId == nil && strings.Compare(k, "spark-app-selector") == 0 {
						sparkAppId = aws.String(v)
					}
					if sparkAppId != nil && emrJobId != nil {
						break
					}
				}
			}
			if pod != nil {
				for _, container := range pod.Spec.Containers {
					for _, v := range container.Env {
						if v.Name == "SPARK_DRIVER_URL" {
							pat := regexp.MustCompile(`.*@(.*-svc).*`)
							matches := pat.FindAllStringSubmatch(v.Value, -1)
							for _, match := range matches {
								if len(match) == 2 {
									driverServiceName = &match[1]
								}
							}
						}
					}
				}
				if pod.Status.ContainerStatuses != nil && len(pod.Status.ContainerStatuses) > 0 {
					for _, containerStatus := range pod.Status.ContainerStatuses {
						if containerStatus.State.Terminated != nil {
							if containerStatus.State.Terminated.ExitCode == 137 {
								if strings.Contains(containerStatus.Name, "driver") {
									driverOOM = aws.Bool(true)
								} else {
									executorOOM = aws.Bool(true)
								}
							}
						}
					}
				}
			}
		}
		if emrJobId != nil {
			run, err := ew.sm.GetRunByEMRJobId(*emrJobId)
			if err == nil {
				layout := "2020-08-31T17:27:50Z"
				timestamp, err := time.Parse(layout, kubernetesEvent.FirstTimestamp)
				if err != nil {
					timestamp = time.Now()
				}
				event := state.PodEvent{
					Timestamp:    &timestamp,
					EventType:    kubernetesEvent.Type,
					Reason:       kubernetesEvent.Reason,
					SourceObject: kubernetesEvent.InvolvedObject.Name,
					Message:      kubernetesEvent.Message,
				}
				var events state.PodEvents
				if run.PodEvents != nil {
					events = append(*run.PodEvents, event)
				} else {
					events = state.PodEvents{event}
				}
				run.PodEvents = &events
				if executorOOM != nil && *executorOOM == true {
					run.SparkExtension.ExecutorOOM = executorOOM
				}
				if driverOOM != nil && *driverOOM == true {
					run.SparkExtension.DriverOOM = driverOOM
				}
				if sparkAppId != nil {
					sparkHistoryUri := fmt.Sprintf("%s/%s/jobs/", ew.emrHistoryServer, *sparkAppId)
					run.SparkExtension.SparkAppId = sparkAppId
					run.SparkExtension.HistoryUri = &sparkHistoryUri
					if driverServiceName != nil {
						appUri := fmt.Sprintf("%s/job/%s", ew.emrAppServer[run.ClusterName], *driverServiceName)
						run.SparkExtension.AppUri = &appUri
					}
				}
				ew.setEMRMetricsUri(&run)
				run, err = ew.sm.UpdateRun(run.RunID, run)
				if err != nil {
					_ = ew.log.Log("message", "error saving kubernetes events", "emrJobId", emrJobId, "error", fmt.Sprintf("%+v", err))
				}
				if run.PodEvents != nil && len(*run.PodEvents) >= ew.emrMaxPodEvents {
					_ = ew.emrEngine.Terminate(run)
				}
			}
		}
		_ = kubernetesEvent.Done()
	}
}
func (ew *eventsWorker) setEMRMetricsUri(run *state.Run) {
	if run != nil && run.SparkExtension != nil && run.SparkExtension.SparkAppId != nil {
		to := "now"
		if run.FinishedAt != nil {
			to = fmt.Sprintf("%d", run.FinishedAt.Add(time.Minute*1).UnixNano()/1000000)
		}
		from := time.Now().Add(-1*time.Minute*1).UnixNano() / 1000000
		if run.StartedAt != nil {
			from = run.StartedAt.Add(-1*time.Minute*1).UnixNano() / 1000000
		}
		metricsUri :=
			fmt.Sprintf("%s&tpl_var_flotilla_run_id=%s&from_ts=%d&to_ts=%s&live=true",
				ew.emrMetricsServer,
				*run.SparkExtension.SparkAppId,
				from,
				to,
			)
		run.MetricsUri = &metricsUri
	}
}
func (ew *eventsWorker) setEKSMetricsUri(run *state.Run) {
	if run != nil {
		to := time.Now().Add(1*time.Minute*1).UnixNano() / 1000000
		if run.FinishedAt != nil {
			to = run.FinishedAt.Add(time.Minute*1).UnixNano() / 1000000
		}
		from := time.Now().Add(-1*time.Minute*1).UnixNano() / 1000000
		if run.StartedAt != nil {
			from = run.StartedAt.Add(-1*time.Minute*1).UnixNano() / 1000000
		}
		metricsUri :=
			fmt.Sprintf("%s&tpl_var_flotilla_run_id=%s&from_ts=%d&to_ts=%d&live=true",
				ew.eksMetricsServer,
				run.RunID,
				from,
				to,
			)
		run.MetricsUri = &metricsUri
	}
}
func (ew *eventsWorker) processEvent(kubernetesEvent state.KubernetesEvent) {
	runId := kubernetesEvent.InvolvedObject.Labels.JobName
	if strings.HasPrefix(runId, "eks-spark") || len(runId) == 0 {
		ew.processEMRPodEvents(kubernetesEvent)
		return
	}
	layout := "2020-08-31T17:27:50Z"
	timestamp, err := time.Parse(layout, kubernetesEvent.FirstTimestamp)
	if err != nil {
		timestamp = time.Now()
	}
	run, err := ew.sm.GetRun(runId)
	if err == nil {
		event := state.PodEvent{
			Timestamp:    &timestamp,
			EventType:    kubernetesEvent.Type,
			Reason:       kubernetesEvent.Reason,
			SourceObject: kubernetesEvent.InvolvedObject.Name,
			Message:      kubernetesEvent.Message,
		}
		var events state.PodEvents
		if run.PodEvents != nil {
			events = append(*run.PodEvents, event)
		} else {
			events = state.PodEvents{event}
		}
		run.PodEvents = &events
		if kubernetesEvent.Reason == "Scheduled" {
			podName, err := ew.parsePodName(kubernetesEvent)
			if err == nil {
				run.PodName = &podName
			}
		}
		if kubernetesEvent.Reason == "DeadlineExceeded" {
			run.ExitReason = &kubernetesEvent.Message
			exitCode := int64(124)
			run.ExitCode = &exitCode
			run.Status = state.StatusStopped
			run.StartedAt = run.QueuedAt
			run.FinishedAt = &timestamp
		}
		if kubernetesEvent.Reason == "Completed" {
			run.ExitReason = &kubernetesEvent.Message
			exitCode := int64(0)
			run.ExitCode = &exitCode
			run.Status = state.StatusStopped
			run.StartedAt = run.QueuedAt
			run.FinishedAt = &timestamp
		}
		ew.setEKSMetricsUri(&run)
		run, err = ew.sm.UpdateRun(runId, run)
		if err != nil {
			_ = ew.log.Log("message", "error saving kubernetes events", "run", runId, "error", fmt.Sprintf("%+v", err))
		} else {
			_ = kubernetesEvent.Done()
		}
	}
}
func (ew *eventsWorker) parsePodName(kubernetesEvent state.KubernetesEvent) (string, error) {
	expression := regexp.MustCompile(`(eks-\w+-\w+-\w+-\w+-\w+-\w+)`)
	matches := expression.FindStringSubmatch(kubernetesEvent.Message)
	if matches != nil && len(matches) >= 1 {
		return matches[0], nil
	}
	return "", errors.Errorf("no pod name found for [%s]", kubernetesEvent.Message)
}

================
File: worker/retry_worker.go
================
package worker
import (
	"fmt"
	"github.com/stitchfix/flotilla-os/queue"
	"time"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/execution/engine"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/state"
	"gopkg.in/tomb.v2"
)
type retryWorker struct {
	sm           state.Manager
	ee           engine.Engine
	conf         config.Config
	log          flotillaLog.Logger
	pollInterval time.Duration
	t            tomb.Tomb
}
func (rw *retryWorker) Initialize(conf config.Config, sm state.Manager, eksEngine engine.Engine, emrEngine engine.Engine, log flotillaLog.Logger, pollInterval time.Duration, qm queue.Manager) error {
	rw.pollInterval = pollInterval
	rw.conf = conf
	rw.sm = sm
	rw.ee = eksEngine
	rw.log = log
	rw.log.Log("message", "initialized a retry worker")
	return nil
}
func (rw *retryWorker) GetTomb() *tomb.Tomb {
	return &rw.t
}
func (rw *retryWorker) Run() error {
	for {
		select {
		case <-rw.t.Dying():
			rw.log.Log("message", "A retry worker was terminated")
			return nil
		default:
			rw.runOnce()
			time.Sleep(rw.pollInterval)
		}
	}
}
func (rw *retryWorker) runOnce() {
	runList, err := rw.sm.ListRuns(25, 0, "started_at", "asc", map[string][]string{"status": {state.StatusNeedsRetry}}, nil, []string{state.EKSEngine})
	if runList.Total > 0 {
		rw.log.Log("message", fmt.Sprintf("Got %v jobs to retry", runList.Total))
	}
	if err != nil {
		rw.log.Log("message", "Error listing runs for retry", "error", fmt.Sprintf("%+v", err))
		return
	}
	for _, run := range runList.Runs {
		if _, err = rw.sm.UpdateRun(run.RunID, state.Run{Status: state.StatusQueued}); err != nil {
			rw.log.Log("message", "Error updating run status to StatusQueued", "run_id", run.RunID, "error", fmt.Sprintf("%+v", err))
			return
		}
		if err = rw.ee.Enqueue(run); err != nil {
			rw.log.Log("message", "Error enqueuing run", "run_id", run.RunID, "error", fmt.Sprintf("%+v", err))
			return
		}
	}
	return
}

================
File: worker/status_worker.go
================
package worker
import (
	"encoding/json"
	"fmt"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/go-redis/redis"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/clients/metrics"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/execution/engine"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/queue"
	"github.com/stitchfix/flotilla-os/state"
	"gopkg.in/tomb.v2"
	"io/ioutil"
	"math/rand"
	"net/http"
	"strings"
	"time"
)
type statusWorker struct {
	sm                       state.Manager
	ee                       engine.Engine
	conf                     config.Config
	log                      flotillaLog.Logger
	pollInterval             time.Duration
	t                        tomb.Tomb
	engine                   *string
	redisClient              *redis.Client
	workerId                 string
	exceptionExtractorClient *http.Client
	exceptionExtractorUrl    string
	emrEngine                engine.Engine
}
func (sw *statusWorker) Initialize(conf config.Config, sm state.Manager, eksEngine engine.Engine, emrEngine engine.Engine, log flotillaLog.Logger, pollInterval time.Duration, qm queue.Manager) error {
	sw.pollInterval = pollInterval
	sw.conf = conf
	sw.sm = sm
	sw.ee = eksEngine
	sw.log = log
	sw.workerId = fmt.Sprintf("workerid:%d", rand.Int())
	sw.engine = &state.EKSEngine
	sw.emrEngine = emrEngine
	if sw.conf.IsSet("eks_exception_extractor_url") {
		sw.exceptionExtractorClient = &http.Client{
			Timeout: time.Second * 5,
		}
		sw.exceptionExtractorUrl = sw.conf.GetString("eks_exception_extractor_url")
	}
	sw.setupRedisClient(conf)
	_ = sw.log.Log("message", "initialized a status worker")
	return nil
}
func (sw *statusWorker) setupRedisClient(conf config.Config) {
	if *sw.engine == state.EKSEngine {
		sw.redisClient = redis.NewClient(&redis.Options{Addr: conf.GetString("redis_address"), DB: conf.GetInt("redis_db")})
	}
}
func (sw *statusWorker) GetTomb() *tomb.Tomb {
	return &sw.t
}
func (sw *statusWorker) Run() error {
	for {
		select {
		case <-sw.t.Dying():
			sw.log.Log("message", "A status worker was terminated")
			return nil
		default:
			if *sw.engine == state.EKSEngine {
				sw.runOnceEKS()
				sw.runTimeouts()
				time.Sleep(sw.pollInterval)
			}
		}
	}
}
func (sw *statusWorker) runTimeouts() {
	rl, err := sw.sm.ListRuns(1000, 0, "started_at", "asc", map[string][]string{
		"queued_at_since": {
			time.Now().AddDate(0, 0, -300).Format(time.RFC3339),
		},
		"task_type": {state.DefaultTaskType},
		"status":    {state.StatusNeedsRetry, state.StatusRunning, state.StatusQueued, state.StatusPending},
	}, nil, state.Engines)
	if err != nil {
		_ = sw.log.Log("message", "unable to receive runs", "error", fmt.Sprintf("%+v", err))
		return
	}
	runs := rl.Runs
	sw.processTimeouts(runs)
}
func (sw *statusWorker) processTimeouts(runs []state.Run) {
	for _, run := range runs {
		if run.StartedAt != nil && run.ActiveDeadlineSeconds != nil {
			runningDuration := time.Now().Sub(*run.StartedAt)
			if int64(runningDuration.Seconds()) > *run.ActiveDeadlineSeconds {
				if run.Engine != nil && *run.Engine == state.EKSSparkEngine {
					_ = sw.emrEngine.Terminate(run)
				} else {
					_ = sw.ee.Terminate(run)
				}
				exitCode := int64(1)
				finishedAt := time.Now()
				_, _ = sw.sm.UpdateRun(run.RunID, state.Run{
					Status:     state.StatusStopped,
					ExitReason: aws.String(fmt.Sprintf("JobRun exceeded specified timeout of %v seconds", *run.ActiveDeadlineSeconds)),
					ExitCode:   &exitCode,
					FinishedAt: &finishedAt,
				})
			}
		}
	}
}
func (sw *statusWorker) runOnceEKS() {
	rl, err := sw.sm.ListRuns(1000, 0, "started_at", "asc", map[string][]string{
		"queued_at_since": {
			time.Now().AddDate(0, 0, -300).Format(time.RFC3339),
		},
		"task_type": {state.DefaultTaskType},
		"status":    {state.StatusNeedsRetry, state.StatusRunning, state.StatusQueued, state.StatusPending},
	}, nil, []string{state.EKSEngine})
	if err != nil {
		_ = sw.log.Log("message", "unable to receive runs", "error", fmt.Sprintf("%+v", err))
		return
	}
	runs := rl.Runs
	sw.processEKSRuns(runs)
}
func (sw *statusWorker) processEKSRuns(runs []state.Run) {
	var lockedRuns []state.Run
	for _, run := range runs {
		duration := time.Duration(45) * time.Second
		lock := sw.acquireLock(run, "status", duration)
		if lock {
			lockedRuns = append(lockedRuns, run)
		}
	}
	_ = metrics.Increment(metrics.StatusWorkerLockedRuns, []string{sw.workerId}, float64(len(lockedRuns)))
	for _, run := range lockedRuns {
		start := time.Now()
		go sw.processEKSRun(run)
		_ = metrics.Timing(metrics.StatusWorkerProcessEKSRun, time.Since(start), []string{sw.workerId}, 1)
	}
}
func (sw *statusWorker) acquireLock(run state.Run, purpose string, expiration time.Duration) bool {
	start := time.Now()
	key := fmt.Sprintf("%s-%s", run.RunID, purpose)
	ttl, err := sw.redisClient.TTL(key).Result()
	if err == nil && ttl.Nanoseconds() < 0 {
		_, err = sw.redisClient.Del(key).Result()
	}
	set, err := sw.redisClient.SetNX(key, sw.workerId, expiration).Result()
	if err != nil {
		_ = sw.log.Log("message", "unable to set lock", "error", fmt.Sprintf("%+v", err))
		return true
	}
	_ = metrics.Timing(metrics.StatusWorkerAcquireLock, time.Since(start), []string{sw.workerId}, 1)
	return set
}
func (sw *statusWorker) processEKSRun(run state.Run) {
	reloadRun, err := sw.sm.GetRun(run.RunID)
	if err == nil && reloadRun.Status == state.StatusStopped {
		return
	}
	start := time.Now()
	start = time.Now()
	updatedRun, err := sw.ee.FetchUpdateStatus(reloadRun)
	if err != nil {
		_ = sw.log.Log("message", "fetch update status", "run", run.RunID, "error", fmt.Sprintf("%+v", err))
	}
	_ = metrics.Timing(metrics.StatusWorkerFetchUpdateStatus, time.Since(start), []string{sw.workerId}, 1)
	if err == nil {
		subRuns, err := sw.sm.ListRuns(1000, 0, "status", "desc", nil, map[string]string{"PARENT_FLOTILLA_RUN_ID": run.RunID}, state.Engines)
		if err == nil && subRuns.Total > 0 {
			var spawnedRuns state.SpawnedRuns
			for _, subRun := range subRuns.Runs {
				spawnedRuns = append(spawnedRuns, state.SpawnedRun{RunID: subRun.RunID})
			}
			updatedRun.SpawnedRuns = &spawnedRuns
		}
	}
	if err != nil {
		message := fmt.Sprintf("%+v", err)
		minutesInQueue := time.Now().Sub(*run.QueuedAt).Minutes()
		if strings.Contains(message, "not found") && minutesInQueue > float64(30) {
			stoppedAt := time.Now()
			reason := "Job either timed out or not found on the EKS cluster."
			updatedRun.Status = state.StatusStopped
			updatedRun.FinishedAt = &stoppedAt
			updatedRun.ExitReason = &reason
			_, err = sw.sm.UpdateRun(updatedRun.RunID, updatedRun)
		}
	} else {
		fullUpdate := false
		if run.PodName != nil {
			if *run.PodName == *updatedRun.PodName && run.Status != updatedRun.Status {
				fullUpdate = true
			}
		}
		if fullUpdate {
			sw.logStatusUpdate(updatedRun)
			if updatedRun.ExitCode != nil {
				go sw.cleanupRun(run.RunID)
			}
			_, err = sw.sm.UpdateRun(updatedRun.RunID, updatedRun)
			if err != nil {
				_ = sw.log.Log("message", "unable to save eks runs", "error", fmt.Sprintf("%+v", err))
			}
			if updatedRun.Status == state.StatusStopped {
			}
		} else {
			if updatedRun.MaxMemoryUsed != run.MaxMemoryUsed ||
				updatedRun.MaxCpuUsed != run.MaxCpuUsed ||
				updatedRun.Cpu != run.Cpu ||
				updatedRun.PodName != run.PodName ||
				updatedRun.Memory != run.Memory ||
				updatedRun.PodEvents != run.PodEvents ||
				updatedRun.SpawnedRuns != run.SpawnedRuns {
				_, err = sw.sm.UpdateRun(updatedRun.RunID, updatedRun)
			}
		}
	}
}
func (sw *statusWorker) cleanupRun(runID string) {
	time.Sleep(120 * time.Second)
	run, err := sw.sm.GetRun(runID)
	if err == nil {
		_ = sw.ee.Terminate(run)
	}
}
func (sw *statusWorker) extractExceptions(runID string) {
	time.Sleep(60 * time.Second)
	run, err := sw.sm.GetRun(runID)
	if err == nil {
		jobUrl := fmt.Sprintf("%s/extract/%s", sw.exceptionExtractorUrl, run.RunID)
		res, err := sw.exceptionExtractorClient.Get(jobUrl)
		if err == nil && res != nil && res.Body != nil {
			body, err := ioutil.ReadAll(res.Body)
			if body != nil {
				defer res.Body.Close()
				runExceptions := state.RunExceptions{}
				err = json.Unmarshal(body, &runExceptions)
				if err == nil {
					run.RunExceptions = &runExceptions
				}
			}
			_, _ = sw.sm.UpdateRun(run.RunID, run)
		}
	}
}
func (sw *statusWorker) processEKSRunMetrics(run state.Run) {
	updatedRun, err := sw.ee.FetchPodMetrics(run)
	if err == nil {
		if updatedRun.MaxMemoryUsed != run.MaxMemoryUsed ||
			updatedRun.MaxCpuUsed != run.MaxCpuUsed {
			_, err = sw.sm.UpdateRun(updatedRun.RunID, updatedRun)
		}
	}
}
func (sw *statusWorker) logStatusUpdate(update state.Run) {
	var err error
	var startedAt, finishedAt time.Time
	var duration float64
	var env state.EnvList
	var command string
	if update.StartedAt != nil {
		startedAt = *update.StartedAt
		duration = time.Now().Sub(startedAt).Seconds()
	}
	if update.FinishedAt != nil {
		finishedAt = *update.FinishedAt
		duration = finishedAt.Sub(startedAt).Seconds()
	}
	if update.Env != nil {
		env = *update.Env
	}
	if update.Command != nil {
		command = *update.Command
	}
	if update.ExitCode != nil {
		err = sw.log.Event("eventClassName", "FlotillaTaskStatus",
			"run_id", update.RunID,
			"definition_id", update.DefinitionID,
			"alias", update.Alias,
			"image", update.Image,
			"cluster_name", update.ClusterName,
			"command", command,
			"exit_code", *update.ExitCode,
			"status", update.Status,
			"started_at", startedAt,
			"finished_at", finishedAt,
			"duration", duration,
			"instance_id", update.InstanceID,
			"instance_dns_name", update.InstanceDNSName,
			"group_name", update.GroupName,
			"user", update.User,
			"task_type", update.TaskType,
			"env", env,
			"executable_id", update.ExecutableID,
			"executable_type", update.ExecutableType)
	} else {
		err = sw.log.Event("eventClassName", "FlotillaTaskStatus",
			"run_id", update.RunID,
			"definition_id", update.DefinitionID,
			"alias", update.Alias,
			"image", update.Image,
			"cluster_name", update.ClusterName,
			"command", command,
			"status", update.Status,
			"started_at", startedAt,
			"finished_at", finishedAt,
			"duration", duration,
			"instance_id", update.InstanceID,
			"instance_dns_name", update.InstanceDNSName,
			"group_name", update.GroupName,
			"user", update.User,
			"task_type", update.TaskType,
			"env", env,
			"executable_id", update.ExecutableID,
			"executable_type", update.ExecutableType)
	}
	if err != nil {
		sw.log.Log("message", "Failed to emit status event", "run_id", update.RunID, "error", err.Error())
	}
}
func (sw *statusWorker) findRun(taskArn string) (state.Run, error) {
	var engines []string
	if sw.engine != nil {
		engines = []string{*sw.engine}
	} else {
		engines = nil
	}
	runs, err := sw.sm.ListRuns(1, 0, "started_at", "asc", map[string][]string{
		"task_arn": {taskArn},
	}, nil, engines)
	if err != nil {
		return state.Run{}, errors.Wrapf(err, "problem finding run by task arn [%s]", taskArn)
	}
	if runs.Total > 0 && len(runs.Runs) > 0 {
		return runs.Runs[0], nil
	}
	return state.Run{}, errors.Errorf("no run found for [%s]", taskArn)
}

================
File: worker/submit_worker.go
================
package worker
import (
	"fmt"
	"github.com/go-redis/redis"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/execution/engine"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/queue"
	"github.com/stitchfix/flotilla-os/state"
	"gopkg.in/tomb.v2"
	"time"
)
type submitWorker struct {
	sm           state.Manager
	eksEngine    engine.Engine
	emrEngine    engine.Engine
	conf         config.Config
	log          flotillaLog.Logger
	pollInterval time.Duration
	t            tomb.Tomb
	redisClient  *redis.Client
}
func (sw *submitWorker) Initialize(conf config.Config, sm state.Manager, eksEngine engine.Engine, emrEngine engine.Engine, log flotillaLog.Logger, pollInterval time.Duration, qm queue.Manager) error {
	sw.pollInterval = pollInterval
	sw.conf = conf
	sw.sm = sm
	sw.eksEngine = eksEngine
	sw.emrEngine = emrEngine
	sw.log = log
	sw.redisClient = redis.NewClient(&redis.Options{Addr: conf.GetString("redis_address"), DB: conf.GetInt("redis_db")})
	_ = sw.log.Log("message", "initialized a submit worker")
	return nil
}
func (sw *submitWorker) GetTomb() *tomb.Tomb {
	return &sw.t
}
func (sw *submitWorker) Run() error {
	for {
		select {
		case <-sw.t.Dying():
			sw.log.Log("message", "A submit worker was terminated")
			return nil
		default:
			sw.runOnce()
			time.Sleep(sw.pollInterval)
		}
	}
}
func (sw *submitWorker) runOnce() {
	var receipts []engine.RunReceipt
	var run state.Run
	var err error
	receipts, err = sw.eksEngine.PollRuns()
	receiptsEMR, err := sw.emrEngine.PollRuns()
	receipts = append(receipts, receiptsEMR...)
	if err != nil {
		sw.log.Log("message", "Error receiving runs", "error", fmt.Sprintf("%+v", err))
	}
	for _, runReceipt := range receipts {
		if runReceipt.Run == nil {
			continue
		}
		run, err = sw.sm.GetRun(runReceipt.Run.RunID)
		if err != nil {
			sw.log.Log("message", "Error fetching run from state, acking", "run_id", runReceipt.Run.RunID, "error", fmt.Sprintf("%+v", err))
			if err = runReceipt.Done(); err != nil {
				sw.log.Log("message", "Acking run failed", "run_id", run.RunID, "error", fmt.Sprintf("%+v", err))
			}
			continue
		}
		if run.Status == state.StatusQueued {
			var (
				launched  state.Run
				retryable bool
			)
			if run.ExecutableType == nil {
				defaultExecutableType := state.ExecutableTypeDefinition
				run.ExecutableType = &defaultExecutableType
			}
			if run.ExecutableID == nil {
				defID := run.DefinitionID
				run.ExecutableID = &defID
			}
			switch *run.ExecutableType {
			case state.ExecutableTypeDefinition:
				var d state.Definition
				d, err = sw.sm.GetDefinition(*run.ExecutableID)
				if err != nil {
					sw.logFailedToGetExecutableMessage(run, err)
					if err = runReceipt.Done(); err != nil {
						sw.log.Log("message", "Acking run failed", "run_id", run.RunID, "error", fmt.Sprintf("%+v", err))
					}
					continue
				}
				if run.Engine == nil || *run.Engine == state.EKSEngine {
					launched, retryable, err = sw.eksEngine.Execute(d, run, sw.sm)
				} else {
					launched, retryable, err = sw.emrEngine.Execute(d, run, sw.sm)
				}
				break
			case state.ExecutableTypeTemplate:
				var tpl state.Template
				tpl, err = sw.sm.GetTemplateByID(*run.ExecutableID)
				if err != nil {
					sw.logFailedToGetExecutableMessage(run, err)
					if err = runReceipt.Done(); err != nil {
						sw.log.Log("message", "Acking run failed", "run_id", run.RunID, "error", fmt.Sprintf("%+v", err))
					}
					continue
				}
				sw.log.Log("message", "Submitting", "run_id", run.RunID)
				launched, retryable, err = sw.eksEngine.Execute(tpl, run, sw.sm)
				break
			default:
				sw.log.Log("message", "submit worker failed", "run_id", run.RunID, "error", "invalid executable type")
				continue
			}
			if err != nil {
				sw.log.Log("message", "Error executing run", "run_id", run.RunID, "error", fmt.Sprintf("%+v", err), "retryable", retryable)
				if !retryable {
					launched.Status = state.StatusStopped
				} else {
					continue
				}
			} else {
				sw.log.Log("message", "Task submitted from SQS to the cluster", "run_id", run.RunID)
			}
			err = sw.log.Event("eventClassName", "FlotillaSubmitTask", "executable_id", *run.ExecutableID, "run_id", run.RunID)
			if err != nil {
				sw.log.Log("message", "Failed to emit event", "run_id", run.RunID, "error", fmt.Sprintf("%+v", err))
			}
			if _, err = sw.sm.UpdateRun(run.RunID, launched); err != nil {
				sw.log.Log("message", "Failed to update run status", "run_id", run.RunID, "status", launched.Status, "error", fmt.Sprintf("%+v", err))
			}
		} else {
			sw.log.Log("message", "Received run that is not runnable", "run_id", run.RunID, "status", run.Status)
		}
		if err = runReceipt.Done(); err != nil {
			sw.log.Log("message", "Acking run failed", "run_id", run.RunID, "error", fmt.Sprintf("%+v", err))
		}
	}
}
func (sw *submitWorker) logFailedToGetExecutableMessage(run state.Run, err error) {
	sw.log.Log(
		"message", "Error fetching executable for run",
		"run_id", run.RunID,
		"executable_id", run.ExecutableID,
		"executable_type", run.ExecutableType,
		"error", err.Error())
}

================
File: worker/worker_manager.go
================
package worker
import (
	"fmt"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/queue"
	"gopkg.in/tomb.v2"
	"time"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/execution/engine"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/state"
)
type workerManager struct {
	sm           state.Manager
	eksEngine    engine.Engine
	emrEngine    engine.Engine
	conf         config.Config
	log          flotillaLog.Logger
	pollInterval time.Duration
	workers      map[string][]Worker
	t            tomb.Tomb
	engine       *string
	qm           queue.Manager
}
func (wm *workerManager) Initialize(conf config.Config, sm state.Manager, eksEngine engine.Engine, emrEngine engine.Engine, log flotillaLog.Logger, pollInterval time.Duration, qm queue.Manager) error {
	wm.conf = conf
	wm.log = log
	wm.eksEngine = eksEngine
	wm.emrEngine = emrEngine
	wm.sm = sm
	wm.qm = qm
	wm.pollInterval = pollInterval
	if err := wm.InitializeWorkers(); err != nil {
		return errors.Errorf("WorkerManager unable to initialize workers: %s", err.Error())
	}
	return nil
}
func (wm *workerManager) GetTomb() *tomb.Tomb {
	return &wm.t
}
func (wm *workerManager) InitializeWorkers() error {
	workerList, err := wm.sm.ListWorkers(state.EKSEngine)
	if err != nil {
		return err
	}
	wm.workers = make(map[string][]Worker)
	for _, w := range workerList.Workers {
		wm.workers[w.WorkerType] = make([]Worker, w.CountPerInstance)
		for i := 0; i < w.CountPerInstance; i++ {
			wk, err := NewWorker(w.WorkerType, wm.log, wm.conf, wm.eksEngine, wm.emrEngine, wm.sm, wm.qm)
			if err != nil {
				return err
			}
			wk.GetTomb().Go(wk.Run)
			wm.workers[w.WorkerType][i] = wk
		}
	}
	return nil
}
func (wm *workerManager) Run() error {
	for {
		select {
		case <-wm.t.Dying():
			wm.log.Log("message", "Worker manager was terminated")
			return nil
		default:
			wm.runOnce()
			time.Sleep(wm.pollInterval)
		}
	}
}
func (wm *workerManager) runOnce() error {
	workerList, err := wm.sm.ListWorkers(state.EKSEngine)
	if err != nil {
		return err
	}
	for _, w := range workerList.Workers {
		currentWorkerCount := len(wm.workers[w.WorkerType])
		if currentWorkerCount != w.CountPerInstance {
			if err := wm.updateWorkerCount(w.WorkerType, currentWorkerCount, w.CountPerInstance); err != nil {
				wm.log.Log(
					"message", "problem updating worker count",
					"error", err.Error())
			}
		}
	}
	return nil
}
func (wm *workerManager) updateWorkerCount(
	workerType string, currentWorkerCount int, desiredWorkerCount int) error {
	if currentWorkerCount > desiredWorkerCount {
		for i := desiredWorkerCount; i < currentWorkerCount; i++ {
			wm.log.Log("message", fmt.Sprintf(
				"Managing [%v] %s workers but %v are desired, scaling down",
				currentWorkerCount, workerType, desiredWorkerCount))
			if err := wm.removeWorker(workerType); err != nil {
				return err
			}
		}
	} else if currentWorkerCount < desiredWorkerCount {
		for i := currentWorkerCount; i < desiredWorkerCount; i++ {
			wm.log.Log("message", fmt.Sprintf(
				"Managing [%v] %s workers but %v are desired, scaling up",
				currentWorkerCount, workerType, desiredWorkerCount))
			if err := wm.addWorker(workerType); err != nil {
				return err
			}
		}
	}
	return nil
}
func (wm *workerManager) removeWorker(workerType string) error {
	if workers, ok := wm.workers[workerType]; ok {
		if len(workers) > 0 {
			toKill := workers[len(workers)-1]
			toKill.GetTomb().Kill(nil)
			wm.workers[workerType] = workers[:len(workers)-1]
		}
	} else {
		return fmt.Errorf("invalid worker type %s", workerType)
	}
	return nil
}
func (wm *workerManager) addWorker(workerType string) error {
	wk, err := NewWorker(workerType, wm.log, wm.conf, wm.eksEngine, wm.emrEngine, wm.sm, wm.qm)
	if err != nil {
		return err
	}
	wk.GetTomb().Go(wk.Run)
	if _, ok := wm.workers[workerType]; ok {
		wm.workers[workerType] = append(wm.workers[workerType], wk)
	} else {
		return fmt.Errorf("invalid worker type %s", workerType)
	}
	return nil
}

================
File: worker/worker.go
================
package worker
import (
	"fmt"
	"github.com/stitchfix/flotilla-os/queue"
	"time"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/execution/engine"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/state"
	"gopkg.in/tomb.v2"
)
type Worker interface {
	Initialize(conf config.Config, sm state.Manager, eksEngine engine.Engine, emrEngine engine.Engine, log flotillaLog.Logger, pollInterval time.Duration, qm queue.Manager) error
	Run() error
	GetTomb() *tomb.Tomb
}
func NewWorker(workerType string, log flotillaLog.Logger, conf config.Config, eksEngine engine.Engine, emrEngine engine.Engine, sm state.Manager, qm queue.Manager) (Worker, error) {
	var worker Worker
	switch workerType {
	case "submit":
		worker = &submitWorker{}
	case "retry":
		worker = &retryWorker{}
	case "status":
		worker = &statusWorker{}
	case "worker_manager":
		worker = &workerManager{}
	case "cloudtrail":
		worker = &cloudtrailWorker{}
	case "events":
		worker = &eventsWorker{}
	default:
		return nil, errors.Errorf("no workerType [%s] exists", workerType)
	}
	pollInterval, err := GetPollInterval(workerType, conf)
	if err = worker.Initialize(conf, sm, eksEngine, emrEngine, log, pollInterval, qm); err != nil {
		return worker, errors.Wrapf(err, "problem initializing worker [%s]", workerType)
	}
	return worker, nil
}
func GetPollInterval(workerType string, conf config.Config) (time.Duration, error) {
	var interval time.Duration
	pollIntervalString := conf.GetString(fmt.Sprintf("worker_%s_interval", workerType))
	if len(pollIntervalString) == 0 {
		return interval, errors.Errorf("worker type: [%s] needs worker_%s_interval set", workerType, workerType)
	}
	return time.ParseDuration(pollIntervalString)
}

================
File: .gitignore
================
# Binaries for programs and plugins
*.exe
*.dll
*.so
*.dylib

# Test binary, build with `go test -c`
*.test

# Output of the go coverage tool, specifically when used with LiteIDE
*.out

# Project-local glide cache, RE: https://github.com/Masterminds/glide/issues/736
.glide/

vendor/**
!vendor/vendor.json

.idea
*.iml

flotilla-os

# gh-pages and ui_branch
node_modules
.cache/
.DS_Store
yarn-error.log
ui/build/
.env

================
File: go.mod
================
module github.com/stitchfix/flotilla-os

go 1.21

require (
	github.com/DataDog/datadog-go/v5 v5.1.0
	github.com/Masterminds/sprig v2.22.0+incompatible
	github.com/aws/aws-sdk-go v1.40.18
	github.com/go-kit/kit v0.9.0
	github.com/go-redis/redis v6.15.9+incompatible
	github.com/gorilla/mux v1.7.4-0.20190701202633-d83b6ffe499a
	github.com/jmoiron/sqlx v1.2.1-0.20190426154859-38398a30ed85
	github.com/lib/pq v1.10.2
	github.com/nu7hatch/gouuid v0.0.0-20131221200532-179d4d0c4d8d
	github.com/pkg/errors v0.9.1
	github.com/rs/cors v1.6.1-0.20190613161432-33ffc0734c60
	github.com/spf13/viper v1.4.1-0.20190614151712-3349bd9cc288
	github.com/xeipuuv/gojsonschema v0.0.0-20180618132009-1d523034197f
	go.uber.org/multierr v1.5.0
	gopkg.in/DataDog/dd-trace-go.v1 v1.38.0
	gopkg.in/tomb.v2 v2.0.0-20161208151619-d5d1b5820637
	k8s.io/api v0.25.16
	k8s.io/apimachinery v0.25.16
	k8s.io/client-go v0.25.16
	k8s.io/metrics v0.25.16
)

require (
	github.com/DataDog/datadog-agent/pkg/obfuscate v0.0.0-20211129110424-6491aa3bf583 // indirect
	github.com/DataDog/datadog-go v4.8.3+incompatible // indirect
	github.com/DataDog/sketches-go v1.0.0 // indirect
	github.com/Masterminds/goutils v1.1.0 // indirect
	github.com/Masterminds/semver v1.5.0 // indirect
	github.com/Microsoft/go-winio v0.5.1 // indirect
	github.com/PuerkitoBio/purell v1.1.1 // indirect
	github.com/PuerkitoBio/urlesc v0.0.0-20170810143723-de5bf2ad4578 // indirect
	github.com/cespare/xxhash/v2 v2.1.2 // indirect
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/dgraph-io/ristretto v0.1.0 // indirect
	github.com/dustin/go-humanize v1.0.0 // indirect
	github.com/emicklei/go-restful/v3 v3.8.0 // indirect
	github.com/fsnotify/fsnotify v1.4.9 // indirect
	github.com/go-logfmt/logfmt v0.5.0 // indirect
	github.com/go-logr/logr v1.2.3 // indirect
	github.com/go-openapi/jsonpointer v0.19.5 // indirect
	github.com/go-openapi/jsonreference v0.19.5 // indirect
	github.com/go-openapi/swag v0.19.14 // indirect
	github.com/gogo/protobuf v1.3.2 // indirect
	github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b // indirect
	github.com/golang/protobuf v1.5.2 // indirect
	github.com/google/gnostic v0.5.7-v3refs // indirect
	github.com/google/gofuzz v1.2.0 // indirect
	github.com/google/uuid v1.3.0 // indirect
	github.com/hashicorp/hcl v1.0.0 // indirect
	github.com/huandu/xstrings v1.3.0 // indirect
	github.com/imdario/mergo v0.3.6 // indirect
	github.com/jmespath/go-jmespath v0.4.0 // indirect
	github.com/josharian/intern v1.0.0 // indirect
	github.com/json-iterator/go v1.1.12 // indirect
	github.com/magiconair/properties v1.8.1 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/mitchellh/copystructure v1.0.0 // indirect
	github.com/mitchellh/mapstructure v1.4.2 // indirect
	github.com/mitchellh/reflectwalk v1.0.0 // indirect
	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
	github.com/modern-go/reflect2 v1.0.2 // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/pelletier/go-toml v1.7.0 // indirect
	github.com/philhofer/fwd v1.1.1 // indirect
	github.com/spf13/afero v1.2.2 // indirect
	github.com/spf13/cast v1.3.0 // indirect
	github.com/spf13/jwalterweatherman v1.0.0 // indirect
	github.com/spf13/pflag v1.0.5 // indirect
	github.com/subosito/gotenv v1.2.0 // indirect
	github.com/tinylib/msgp v1.1.2 // indirect
	github.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f // indirect
	github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415 // indirect
	go.uber.org/atomic v1.6.0 // indirect
	golang.org/x/crypto v0.14.0 // indirect
	golang.org/x/net v0.17.0 // indirect
	golang.org/x/oauth2 v0.0.0-20211104180415-d3ed0bb246c8 // indirect
	golang.org/x/sys v0.13.0 // indirect
	golang.org/x/term v0.13.0 // indirect
	golang.org/x/text v0.13.0 // indirect
	golang.org/x/time v0.0.0-20220210224613-90d013bbcef8 // indirect
	golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1 // indirect
	google.golang.org/appengine v1.6.7 // indirect
	google.golang.org/protobuf v1.28.0 // indirect
	gopkg.in/inf.v0 v0.9.1 // indirect
	gopkg.in/yaml.v2 v2.4.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
	k8s.io/klog/v2 v2.70.1 // indirect
	k8s.io/kube-openapi v0.0.0-20220803162953-67bda5d908f1 // indirect
	k8s.io/utils v0.0.0-20220728103510-ee6ede2d64ed // indirect
	sigs.k8s.io/json v0.0.0-20220713155537-f223a00ba0e2 // indirect
	sigs.k8s.io/structured-merge-diff/v4 v4.2.3 // indirect
	sigs.k8s.io/yaml v1.2.0 // indirect
)

================
File: main.go
================
package main
import (
	"fmt"
	gklog "github.com/go-kit/kit/log"
	"github.com/pkg/errors"
	"github.com/stitchfix/flotilla-os/clients/cluster"
	"github.com/stitchfix/flotilla-os/clients/logs"
	"github.com/stitchfix/flotilla-os/clients/metrics"
	"github.com/stitchfix/flotilla-os/clients/middleware"
	"github.com/stitchfix/flotilla-os/config"
	"github.com/stitchfix/flotilla-os/execution/engine"
	"github.com/stitchfix/flotilla-os/flotilla"
	flotillaLog "github.com/stitchfix/flotilla-os/log"
	"github.com/stitchfix/flotilla-os/queue"
	"github.com/stitchfix/flotilla-os/state"
	"log"
	"os"
)
func main() {
	args := os.Args
	if len(args) < 2 {
		fmt.Println("Usage: flotilla-os <conf_dir>")
		os.Exit(1)
	}
	l := gklog.NewLogfmtLogger(gklog.NewSyncWriter(os.Stderr))
	l = gklog.With(l, "ts", gklog.DefaultTimestampUTC)
	eventSinks := []flotillaLog.EventSink{flotillaLog.NewLocalEventSink()}
	logger := flotillaLog.NewLogger(l, eventSinks)
	confDir := args[1]
	c, err := config.NewConfig(&confDir)
	if err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize config"))
		os.Exit(1)
	}
	if err = metrics.InstantiateClient(c); err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize metrics client"))
		os.Exit(1)
	}
	stateManager, err := state.NewStateManager(c, logger)
	if err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize state manager"))
		os.Exit(1)
	}
	if err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize registry client"))
		os.Exit(1)
	}
	eksClusterClient, err := cluster.NewClusterClient(c, state.EKSEngine)
	if err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize EKS cluster client"))
	}
	eksLogsClient, err := logs.NewLogsClient(c, logger, state.EKSEngine)
	if err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize EKS logs client"))
	}
	eksQueueManager, err := queue.NewQueueManager(c, state.EKSEngine)
	if err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize eks queue manager"))
		os.Exit(1)
	}
	emrQueueManager, err := queue.NewQueueManager(c, state.EKSSparkEngine)
	if err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize eks queue manager"))
		os.Exit(1)
	}
	eksExecutionEngine, err := engine.NewExecutionEngine(c, eksQueueManager, state.EKSEngine, logger)
	if err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize EKS execution engine"))
		os.Exit(1)
	}
	emrExecutionEngine, err := engine.NewExecutionEngine(c, eksQueueManager, state.EKSSparkEngine, logger)
	if err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize EMR execution engine"))
		os.Exit(1)
	}
	middlewareClient, err := middleware.NewClient()
	if err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize middleware client"))
		os.Exit(1)
	}
	app, err := flotilla.NewApp(c, logger, eksLogsClient, eksExecutionEngine, stateManager, eksClusterClient, eksQueueManager, emrExecutionEngine, emrQueueManager, middlewareClient)
	if err != nil {
		fmt.Printf("%+v\n", errors.Wrap(err, "unable to initialize app"))
		os.Exit(1)
	}
	log.Fatal(app.Run())
}



================================================================
End of Codebase
================================================================
